\relax 
\emailauthor{jkrijthe@gmail.com}{Jesse H. Krijthe\corref {cor1}}
\emailauthor{m.loog@tudelft.nl}{Marco Loog}
\Newlabel{cor1}{1}
\Newlabel{prlab}{a}
\Newlabel{molepi}{b}
\Newlabel{imagegroup}{c}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\citation{Cozman2006,Cozman2003}
\citation{Seeger2001,Singh2008}
\citation{Hastie2001,Poggio2003,Rifkin2003,Suykens1999,Tibshirani1996}
\citation{Rifkin2003}
\citation{Bottou2010}
\citation{Krijthe2015}
\citation{Chapelle2006,Zhu2009}
\citation{Nigam2000}
\citation{Kall2007}
\citation{Shi2011}
\citation{Cozman2006,Cozman2003}
\citation{Elworthy1994}
\citation{Goldberg2009,Wang2007a}
\citation{Goldberg2009}
\citation{Goldberg2009}
\citation{McLachlan1975}
\citation{Abney2004,Yarowsky1995}
\citation{Elworthy1994}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{3}}
\newlabel{section:relatedwork}{{2}{3}}
\citation{Nigam2000,Yarowsky1995}
\citation{Cozman2006,Cozman2003}
\citation{Abney2004}
\citation{Zhu2009}
\citation{Grandvalet2005}
\citation{Joachims1999}
\citation{Bennett1998,Sindhwani2006}
\citation{Collobert2006}
\citation{Sindhwani2006,Wang2007}
\citation{Wang2007a}
\citation{Loog2010,Loog2014b}
\citation{Loog2010,Loog2014b}
\citation{Loog2010,Loog2014b}
\citation{Li2011}
\citation{Bennett1998}
\citation{Hastie2001,Poggio2003,Suykens1999}
\citation{Little2002}
\citation{Healy1956}
\citation{Shaffer1991}
\citation{Fan2008}
\@writefile{toc}{\contentsline {section}{\numberline {3}Implicitly Constrained Least Squares Classification}{6}}
\newlabel{section:overview}{{3}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A visual representation of implicitly constrained semi-supervised learning. $\mathcal  {F}_{\beta }$ is the space of all linear models. $\mathaccentV {hat}05E{\beta }_{sup}$ denotes the solution given only a small amount of labeled data. $\mathcal  {C}_{\beta }$ is the subset of the space which contains all the solutions we get when applying all possible (soft) labelings to the unlabeled data. $\mathaccentV {hat}05E{\beta }_{semi}$ is a projection of $\mathaccentV {hat}05E{\beta }_{sup}$ onto $\mathcal  {C}_{\beta }$. $\mathaccentV {hat}05E{\beta }_{oracle}$ is the supervised solution if we would have the labels for all the objects.}}{6}}
\newlabel{fig:constrainedsubset}{{1}{6}}
\citation{Hastie2001,Rifkin2003}
\@writefile{toc}{\contentsline {section}{\numberline {4}Method}{7}}
\newlabel{section:method}{{4}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Supervised Multivariate Least Squares Classification}{7}}
\newlabel{section:leastsquares}{{4.1}{7}}
\newlabel{squaredloss}{{1}{7}}
\newlabel{olssolution}{{2}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Implicitly Constrained Least Squares Classification}{7}}
\newlabel{section:icls}{{4.2}{7}}
\newlabel{constrainedregion}{{3}{8}}
\newlabel{icls}{{5}{8}}
\citation{Byrd1995}
\citation{Sokolovska2008}
\@writefile{toc}{\contentsline {section}{\numberline {5}Theoretical Results}{9}}
\newlabel{section:theoreticalresults}{{5}{9}}
\newlabel{eq:trueloss}{{7}{9}}
\newlabel{eq:bayesoptimal}{{8}{9}}
\newlabel{theorem:1d}{{1}{10}}
\newlabel{eqn:sslsolution}{{11}{10}}
\newlabel{supervisedsolution}{{12}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces An example where implicitly constrained optimization improves performance. The supervised solution $\mathaccentV {hat}05E{\beta }_{sup}$ which minimizes the supervised loss (the solid curve), is not part of the interval of allowed solutions. The solution that minimizes this supervised loss within the allowed interval is $\mathaccentV {hat}05E{\beta }_{semi}$. This solution is closer to the optimal solution ${\beta }^{\ast }$ than the supervised solution $\mathaccentV {hat}05E{\beta }_{sup}$.}}{11}}
\newlabel{fig:constrainedproblem}{{2}{11}}
\newlabel{eq:condition}{{13}{11}}
\citation{McLachlan1975}
\citation{Shaffer1991}
\citation{Bache2013}
\citation{Chapelle2006}
\citation{Chapelle2006}
\newlabel{eq:condition2}{{14}{12}}
\newlabel{eq:condition3}{{15}{12}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Empirical Results}{12}}
\newlabel{section:empiricalresults}{{6}{12}}
\@writefile{toc}{\contentsline {paragraph}{Self-Learning}{12}}
\@writefile{toc}{\contentsline {paragraph}{Updated Second Moment Least Squares (USM)}{12}}
\citation{Bache2013}
\citation{Bache2013}
\citation{Bache2013}
\citation{Bache2013}
\citation{Bache2013}
\citation{Bache2013}
\citation{Bache2013}
\citation{Bache2013}
\citation{Bache2013}
\citation{Bache2013}
\citation{Chapelle2006}
\citation{Chapelle2006}
\citation{Chapelle2006}
\citation{Chapelle2006}
\citation{Chapelle2006}
\citation{Chapelle2006}
\citation{Opper1996,Raudys1998}
\citation{Skurichina1999}
\@writefile{toc}{\contentsline {paragraph}{Oracle}{13}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Description of the datasets used in the experiments. PCA99 refers to the number of principal components required to retain at least 99\% of the variance. Majority refers to the proportion of the number of objects from the largest class}}{13}}
\newlabel{table:datasets}{{1}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Peaking Behaviour in Semi-supervised Least Squares}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Peaking phenomenon in Semi-supervised Least Squares Classification. The lines indicate the Mean classification error for $L=\qopname  \relax m{max}(d+5,20)$ and $1000$ repeats. The shaded areas indicate $+/-$ the standard error of the mean.}}{14}}
\newlabel{fig:peaking}{{3}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Comparison of Learning Curves}{15}}
\citation{Chapelle2006}
\citation{Chapelle2006}
\citation{Chapelle2006}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Mean classification error for $L=\qopname  \relax m{max}(d+5,20)$ and $1000$ repeats. The shaded areas indicate $+/-$ the standard error of the mean.}}{16}}
\newlabel{fig:errorcurves}{{4}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Benchmark performance}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Mean squared loss on the test set for $L=\qopname  \relax m{max}(d+5,20)$ and $1000$ repeats. The shaded areas indicate $+/-$ the standard error of the mean.}}{17}}
\newlabel{fig:losscurves}{{5}{17}}
\citation{Cozman2006}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Average 10-fold cross-validation error and (between parentheses) number of times the error of the semi-supervised classifier is higher than the supervised error for $100$ repeats. Indicated in $\mathbf  {bold}$ is which semi-supervised classifier has lowest average error. A Wilcoxon signed rank test at $0.01$ significance level is done to determine whether a semi-supervised classifier is significantly worse than the supervised classifier, indicated by \relax $\@@underline {\hbox {underlined}}\mathsurround \z@ $\relax  values.}}{18}}
\newlabel{table:cvresults}{{2}{18}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Discussion}{18}}
\citation{Seeger2001,Singh2008}
\citation{Loog2016a}
\citation{Loog2010,Loog2014b}
\citation{Loog2010,Loog2014b}
\citation{Opper1996,Raudys1998}
\citation{Krijthe2014}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Average Training Time for $1000$ repeats. The shaded areas indicate $+/-$ the standard error of the mean.}}{21}}
\newlabel{fig:timecurves}{{6}{21}}
\bibstyle{elsarticle-num}
\bibdata{library}
\bibcite{Cozman2006}{{1}{}{{}}{{}}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Conclusion}{22}}
\bibcite{Cozman2003}{{2}{}{{}}{{}}}
\bibcite{Seeger2001}{{3}{}{{}}{{}}}
\bibcite{Singh2008}{{4}{}{{}}{{}}}
\bibcite{Hastie2001}{{5}{}{{}}{{}}}
\bibcite{Poggio2003}{{6}{}{{}}{{}}}
\bibcite{Rifkin2003}{{7}{}{{}}{{}}}
\bibcite{Suykens1999}{{8}{}{{}}{{}}}
\bibcite{Tibshirani1996}{{9}{}{{}}{{}}}
\bibcite{Bottou2010}{{10}{}{{}}{{}}}
\bibcite{Krijthe2015}{{11}{}{{}}{{}}}
\bibcite{Chapelle2006}{{12}{}{{}}{{}}}
\bibcite{Zhu2009}{{13}{}{{}}{{}}}
\bibcite{Nigam2000}{{14}{}{{}}{{}}}
\bibcite{Kall2007}{{15}{}{{}}{{}}}
\bibcite{Shi2011}{{16}{}{{}}{{}}}
\bibcite{Elworthy1994}{{17}{}{{}}{{}}}
\bibcite{Goldberg2009}{{18}{}{{}}{{}}}
\bibcite{Wang2007a}{{19}{}{{}}{{}}}
\bibcite{McLachlan1975}{{20}{}{{}}{{}}}
\bibcite{Abney2004}{{21}{}{{}}{{}}}
\bibcite{Yarowsky1995}{{22}{}{{}}{{}}}
\bibcite{Grandvalet2005}{{23}{}{{}}{{}}}
\bibcite{Joachims1999}{{24}{}{{}}{{}}}
\bibcite{Bennett1998}{{25}{}{{}}{{}}}
\bibcite{Sindhwani2006}{{26}{}{{}}{{}}}
\bibcite{Collobert2006}{{27}{}{{}}{{}}}
\bibcite{Wang2007}{{28}{}{{}}{{}}}
\bibcite{Loog2010}{{29}{}{{}}{{}}}
\bibcite{Loog2014b}{{30}{}{{}}{{}}}
\bibcite{Li2011}{{31}{}{{}}{{}}}
\bibcite{Little2002}{{32}{}{{}}{{}}}
\bibcite{Healy1956}{{33}{}{{}}{{}}}
\bibcite{Shaffer1991}{{34}{}{{}}{{}}}
\bibcite{Fan2008}{{35}{}{{}}{{}}}
\bibcite{Byrd1995}{{36}{}{{}}{{}}}
\bibcite{Sokolovska2008}{{37}{}{{}}{{}}}
\bibcite{Bache2013}{{38}{}{{}}{{}}}
\bibcite{Opper1996}{{39}{}{{}}{{}}}
\bibcite{Raudys1998}{{40}{}{{}}{{}}}
\bibcite{Skurichina1999}{{41}{}{{}}{{}}}
\bibcite{Loog2016a}{{42}{}{{}}{{}}}
\bibcite{Krijthe2014}{{43}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
