Automatically generated by Mendeley Desktop 1.13.8
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@inproceedings{Singh2008,
author = {Singh, Aarti and Nowak, Robert D. and Zhu, Xiaojin},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Singh, Nowak, Zhu - 2008 - Unlabeled data Now it helps , now it doesn’t.pdf:pdf},
pages = {1513--1520},
title = {{Unlabeled data: Now it helps , now it doesn’t}},
year = {2008}
}
@inproceedings{Miguel2014,
author = {Carreira-Perpinan, Miguel A. and Wang, Weiran},
booktitle = {AISTATS},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Carreira-Perpinan, Wang - 2014 - Distributed Optimization of Deeply Nested Systems.pdf:pdf},
pages = {10--19},
title = {{Distributed Optimization of Deeply Nested Systems}},
year = {2014}
}
@article{Xiao2015,
author = {Xiao, Min and Guo, Yuhong},
doi = {10.1109/TPAMI.2014.2343216},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Xiao, Guo - 2015 - Feature Space Independent Semi-Supervised Domain Adaptation via Kernel Matching.pdf:pdf},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
month = jan,
number = {1},
pages = {54--66},
title = {{Feature Space Independent Semi-Supervised Domain Adaptation via Kernel Matching}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6866177},
volume = {37},
year = {2015}
}
@article{Schmidhuber2012,
author = {Schmidhuber, Jurgen},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Schmidhuber - 2012 - Philosophers \& futurists, catch up.pdf:pdf},
journal = {Journal of Consciousness Studies},
number = {1},
pages = {173--182},
title = {{Philosophers \& futurists, catch up}},
url = {http://www.idsia.ch/~juergen/2012futurists.pdf},
year = {2012}
}
@article{Max2004,
author = {Max, Kybernetik and Max, Kybernetik},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Max, Max - 2004 - Measuring Statistical Dependence with Hilbert-Schmidt Norms.pdf:pdf},
journal = {Biological Cybernetics},
number = {140},
title = {{Measuring Statistical Dependence with Hilbert-Schmidt Norms}},
year = {2004}
}
@inproceedings{Ho2000,
author = {Ho, Tin Kam},
booktitle = {Multiple Classifier Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ho - 2000 - Complexity of Classification Problems and Comparative Advantages of Combined Classifiers.pdf:pdf},
pages = {97--106},
title = {{Complexity of Classification Problems and Comparative Advantages of Combined Classifiers}},
year = {2000}
}
@article{Johnson2013,
author = {Johnson, Valen E.},
doi = {10.1214/13-AOS1123},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Johnson - 2013 - Uniformly most powerful Bayesian tests.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
keywords = {bayes factor,higgs,ily model,jeffreys-lindley paradox,neyman-pearson lemma,non-local prior density,objective bayes,one-parameter exponential fam-,uniformly most powerful test},
month = aug,
number = {4},
pages = {1716--1741},
title = {{Uniformly most powerful Bayesian tests}},
url = {http://projecteuclid.org/euclid.aos/1378386237},
volume = {41},
year = {2013}
}
@article{Bengio2004,
author = {Bengio, Yoshua and Grandvalet, Yves},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bengio, Grandvalet - 2004 - No unbiased estimator of the variance of k-fold cross-validation.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {cross-validation,k-fold cross-validation,statistical comparisons,variance estimators},
pages = {1089--1105},
title = {{No unbiased estimator of the variance of k-fold cross-validation}},
volume = {5},
year = {2004}
}
@article{Clarke2012,
author = {Clarke, Bertrand and Clarke, Jennifer},
doi = {10.1214/12-SS100},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Clarke, Clarke - 2012 - Prediction in several conventional contexts.pdf:pdf},
issn = {1935-7516},
journal = {Statistics Surveys},
keywords = {62M20Prediction, prequential, IID data, time serie,and phrases,iid data,longitudinal data,prediction,prequential,received march 2012,survival analysis,time series},
pages = {1--73},
title = {{Prediction in several conventional contexts}},
url = {http://projecteuclid.org/euclid.ssu/1336481369},
volume = {6},
year = {2012}
}
@techreport{Wainwright,
author = {Wainwright, Martin J.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wainwright - Unknown - Graphical models and message-passing algorithms Some introductory lectures.pdf:pdf},
pages = {1--55},
title = {{Graphical models and message-passing algorithms : Some introductory lectures}}
}
@article{Hanczar2010a,
abstract = {The receiver operator characteristic (ROC) curves are commonly used in biomedical applications to judge the performance of a discriminant across varying decision thresholds. The estimated ROC curve depends on the true positive rate (TPR) and false positive rate (FPR), with the key metric being the area under the curve (AUC). With small samples these rates need to be estimated from the training data, so a natural question arises: How well do the estimates of the AUC, TPR and FPR compare with the true metrics?},
author = {Hanczar, Blaise and Hua, Jianping and Sima, Chao and Weinstein, John and Bittner, Michael and Dougherty, Edward R.},
doi = {10.1093/bioinformatics/btq037},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hanczar et al. - 2010 - Small-sample precision of ROC-related estimates.pdf:pdf},
issn = {1367-4811},
journal = {Bioinformatics (Oxford, England)},
keywords = {Algorithms,False Positive Reactions,Oligonucleotide Array Sequence Analysis,Pattern Recognition, Automated,Pattern Recognition, Automated: methods,ROC Curve},
month = mar,
number = {6},
pages = {822--30},
pmid = {20130029},
title = {{Small-sample precision of ROC-related estimates.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20130029},
volume = {26},
year = {2010}
}
@inproceedings{Ben-David2012,
author = {Ben-David, Shai and Loker, David and Srebro, Nathan and Sridharan, Karthik},
booktitle = {Proceedings of the 29th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ben-David et al. - 2012 - Minimizing the misclassification error rate using a surrogate convex loss.pdf:pdf},
pages = {1863----1870},
title = {{Minimizing the misclassification error rate using a surrogate convex loss}},
year = {2012}
}
@article{Hand2009,
author = {Hand, David J.},
doi = {10.1007/s10994-009-5119-5},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hand - 2009 - Measuring classifier performance a coherent alternative to the area under the ROC curve.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {auc,classification,cost,error rate,loss,misclassification,rate,roc curves,sensitivity,specificity},
month = jun,
number = {1},
pages = {103--123},
title = {{Measuring classifier performance: a coherent alternative to the area under the ROC curve}},
url = {http://link.springer.com/10.1007/s10994-009-5119-5},
volume = {77},
year = {2009}
}
@article{Pribram1978b,
author = {Pribram, Karl H.},
doi = {10.1017/S0140525X00060003},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Pribram - 1978 - On behalf of the neurosciences(3).pdf:pdf},
isbn = {0300104251},
issn = {0140-525X},
journal = {Behavioral and Brain Sciences},
number = {01},
pages = {113},
title = {{On behalf of the neurosciences}},
volume = {1},
year = {1978}
}
@unpublished{Bresson2012,
archivePrefix = {arXiv},
arxivId = {arXiv:1210.0699v1},
author = {Bresson, Xavier and Zhang, Ruiliang},
booktitle = {arXiv preprint},
eprint = {arXiv:1210.0699v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bresson, Zhang - 2012 - TV-SVM Total Variation Support Vector Machine for Semi-Supervised Data Classification.pdf:pdf},
title = {{TV-SVM: Total Variation Support Vector Machine for Semi-Supervised Data Classification}},
url = {http://arxiv.org/abs/1210.0699},
year = {2012}
}
@article{Ben-david2011,
author = {Ben-david, Shai and Srebro, Nati and Urner, Ruth},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ben-david, Srebro, Urner - 2011 - Is learning possible without Prior Knowledge Do Universal Learners exist High level view of ( Statis.pdf:pdf},
title = {{Is learning possible without Prior Knowledge ? Do Universal Learners exist ? High level view of ( Statistical ) Machine Learning}},
year = {2011}
}
@article{Kawakita2013,
archivePrefix = {arXiv},
arxivId = {arXiv:1204.3965v1},
author = {Kawakita, Masanori and Kanamori, Takafumi},
eprint = {arXiv:1204.3965v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kawakita, Kanamori - 2013 - Semi-Supervised learning with Density-Ratio Estimation.pdf:pdf},
journal = {Machine Learning},
number = {2},
pages = {189--209},
title = {{Semi-Supervised learning with Density-Ratio Estimation}},
volume = {91},
year = {2013}
}
@article{Giraud-carrier2004,
author = {Giraud-carrier, Christophe and Vilalta, Ricardo and Brazdil, Pavel B.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Giraud-carrier, Vilalta, Brazdil - 2004 - Introduction to the special issue on meta-learning.pdf:pdf},
journal = {Machine learning},
keywords = {dynamic bias selection,inductive bias,meta-knowledge,meta-learning},
pages = {187--193},
title = {{Introduction to the special issue on meta-learning}},
url = {http://link.springer.com/article/10.1023/B:MACH.0000015878.60765.42},
volume = {54},
year = {2004}
}
@article{Bartlett2006,
author = {Bartlett, Peter L and Jordan, Michael I. and McAuliffe, Jon D},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bartlett, Jordan, McAuliffe - 2006 - Convexity, Classification, and Risk Bounds.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {boosting,convex optimization},
month = mar,
number = {473},
pages = {138--156},
title = {{Convexity, Classification, and Risk Bounds}},
volume = {101},
year = {2006}
}
@article{Zhou2007b,
address = {New York, New York, USA},
author = {Zhou, Dengyong and Burges, Christopher J. C.},
doi = {10.1145/1273496.1273642},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhou, Burges - 2007 - Spectral clustering and transductive learning with multiple views.pdf:pdf},
isbn = {9781595937933},
journal = {Proceedings of the 24th international conference on Machine learning - ICML '07},
pages = {1159--1166},
publisher = {ACM Press},
title = {{Spectral clustering and transductive learning with multiple views}},
url = {http://portal.acm.org/citation.cfm?doid=1273496.1273642},
year = {2007}
}
@article{Braga-Neto2004,
abstract = {MOTIVATION: Microarray classification typically possesses two striking attributes: (1) classifier design and error estimation are based on remarkably small samples and (2) cross-validation error estimation is employed in the majority of the papers. Thus, it is necessary to have a quantifiable understanding of the behavior of cross-validation in the context of very small samples. RESULTS: An extensive simulation study has been performed comparing cross-validation, resubstitution and bootstrap estimation for three popular classification rules-linear discriminant analysis, 3-nearest-neighbor and decision trees (CART)-using both synthetic and real breast-cancer patient data. Comparison is via the distribution of differences between the estimated and true errors. Various statistics for the deviation distribution have been computed: mean (for estimator bias), variance (for estimator precision), root-mean square error (for composition of bias and variance) and quartile ranges, including outlier behavior. In general, while cross-validation error estimation is much less biased than resubstitution, it displays excessive variance, which makes individual estimates unreliable for small samples. Bootstrap methods provide improved performance relative to variance, but at a high computational cost and often with increased bias (albeit, much less than with resubstitution).},
author = {Braga-Neto, Ulisses M and Dougherty, Edward R.},
doi = {10.1093/bioinformatics/btg419},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Braga-Neto, Dougherty - 2004 - Is cross-validation valid for small-sample microarray classification.pdf:pdf},
issn = {1367-4803},
journal = {Bioinformatics (Oxford, England)},
keywords = {Algorithms,Benchmarking,Benchmarking: methods,Breast Neoplasms,Breast Neoplasms: diagnosis,Breast Neoplasms: genetics,Computer Simulation,Gene Expression Profiling,Gene Expression Profiling: methods,Genetic Predisposition to Disease,Genetic Predisposition to Disease: genetics,Genetic Testing,Genetic Testing: methods,Humans,Models, Genetic,Models, Statistical,Oligonucleotide Array Sequence Analysis,Oligonucleotide Array Sequence Analysis: methods,Pattern Recognition, Automated,Reproducibility of Results,Sample Size,Sensitivity and Specificity},
month = feb,
number = {3},
pages = {374--80},
pmid = {14960464},
title = {{Is cross-validation valid for small-sample microarray classification?}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/14960464},
volume = {20},
year = {2004}
}
@article{O'Neill1978,
author = {O'Neill, Terence J.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/O'Neill - 1978 - Normal discrimination with unclassified observations.pdf:pdf},
journal = {Journal of the American Statistical Association},
keywords = {unclassified},
number = {364},
pages = {821--826},
title = {{Normal discrimination with unclassified observations}},
url = {http://amstat.tandfonline.com/doi/full/10.1080/01621459.1978.10480106},
volume = {73},
year = {1978}
}
@article{Bottou2013,
author = {Bottou, L\'{e}on},
doi = {10.1007/s10994-013-5335-x},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bottou - 2013 - From machine learning to machine reasoning.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {machine learning,reasoning,recursive networks},
month = apr,
pages = {133--149},
title = {{From machine learning to machine reasoning}},
url = {http://link.springer.com/10.1007/s10994-013-5335-x},
year = {2013}
}
@article{Pilanci2014,
abstract = {We study randomized sketching methods for approximately solving least-squares problem with a general convex constraint. The quality of a least-squares approximation can be assessed in different ways: either in terms of the value of the quadratic objective function (cost approximation), or in terms of some distance measure between the approximate minimizer and the true minimizer (solution approximation). Focusing on the latter criterion, our first main result provides a general lower bound on any randomized method that sketches both the data matrix and vector in a least-squares problem; as a surprising consequence, the most widely used least-squares sketch is sub-optimal for solution approximation. We then present a new method known as the iterative Hessian sketch, and show that it can be used to obtain approximations to the original least-squares problem using a projection dimension proportional to the statistical complexity of the least-squares minimizer, and a logarithmic number of iterations. We illustrate our general theory with simulations for both unconstrained and constrained versions of least-squares, including \$\backslash ell\_1\$-regularization and nuclear norm constraints. We also numerically demonstrate the practicality of our approach in a real face expression classification experiment.},
archivePrefix = {arXiv},
arxivId = {1411.0347},
author = {Pilanci, Mert and Wainwright, Martin J.},
eprint = {1411.0347},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Pilanci, Wainwright - 2014 - Iterative Hessian sketch Fast and accurate solution approximation for constrained least-squares.pdf:pdf},
month = nov,
number = {1},
pages = {1--33},
title = {{Iterative Hessian sketch: Fast and accurate solution approximation for constrained least-squares}},
url = {http://arxiv.org/abs/1411.0347},
year = {2014}
}
@inproceedings{Muandet2012,
author = {Muandet, Krikamol and Fukumizu, K},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Muandet, Fukumizu - 2012 - Learning from distributions via support measure machines.pdf:pdf},
pages = {1--9},
title = {{Learning from distributions via support measure machines}},
url = {http://arxiv.org/abs/1202.6504},
year = {2012}
}
@inproceedings{Balcan2013,
author = {Balcan, Maria-Florina and Berlind, Christopher and Ehrlich, Steven and Liang, Yingyu},
booktitle = {Proceedings of the 30th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Balcan et al. - 2013 - Efficient Semi-supervised and Active Learning of Disjunctions.pdf:pdf},
pages = {633--641},
title = {{Efficient Semi-supervised and Active Learning of Disjunctions}},
url = {http://machinelearning.wustl.edu/mlpapers/papers/ICML2013\_balcan13},
year = {2013}
}
@article{Juszczak2004,
author = {Juszczak, P and Duin, R P W},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Juszczak, Duin - 2004 - Combining one-class classifiers to classify missing data.pdf:pdf},
issn = {03029743},
journal = {Multiple Classifier Systems},
pages = {92--101},
title = {{Combining one-class classifiers to classify missing data}},
year = {2004}
}
@article{Crammer2006,
author = {Crammer, Koby and Dekel, Ofer and Keshet, Joseph and Shalev-Shwartz, Shai and Singer, Yoram},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Crammer et al. - 2006 - Online passive-aggressive algorithms.pdf:pdf},
journal = {Journal of Machine Learning Research},
pages = {551--585},
title = {{Online passive-aggressive algorithms}},
url = {http://dl.acm.org/citation.cfm?id=1248566},
volume = {7},
year = {2006}
}
@article{Parker2012,
abstract = {Measurements from microarrays and other high-throughput technologies are susceptible to non-biological artifacts like batch effects. It is known that batch effects can alter or obscure the set of significant results and biological conclusions in high-throughput studies. Here we examine the impact of batch effects on predictors built from genomic technologies. To investigate batch effects, we collected publicly available gene expression measurements with known outcomes, and estimated batches using date. Using these data we show (1) the impact of batch effects on prediction depends on the correlation between outcome and batch in the training data, and (2) removing expression measurements most affected by batch before building predictors may improve the accuracy of those predictors. These results suggest that (1) training sets should be designed to minimize correlation between batches and outcome, and (2) methods for identifying batch-affected probes should be developed to improve prediction results for studies with high correlation between batches and outcome.},
author = {Parker, Hilary S. and Leek, Jeffrey T.},
doi = {10.1515/1544-6115.1766},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Parker, Leek - 2012 - The practical effect of batch on genomic prediction.pdf:pdf},
issn = {1544-6115},
journal = {Statistical Applications in Genetics and Molecular Biology},
number = {3},
pmid = {22611599},
title = {{The practical effect of batch on genomic prediction}},
volume = {11},
year = {2012}
}
@article{Peters2013,
author = {Peters, J. and Buhlmann, P.},
doi = {10.1093/biomet/ast043},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Peters, Buhlmann - 2013 - Identifiability of Gaussian structural equation models with equal error variances.pdf:pdf},
issn = {0006-3444},
journal = {Biometrika},
month = nov,
number = {1},
pages = {219--228},
title = {{Identifiability of Gaussian structural equation models with equal error variances}},
url = {http://biomet.oxfordjournals.org/cgi/doi/10.1093/biomet/ast043},
volume = {101},
year = {2013}
}
@article{Hand2006a,
archivePrefix = {arXiv},
arxivId = {arXiv:math/0606441v1},
author = {Hand, David J.},
doi = {10.1214/088342306000000060},
eprint = {0606441v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hand - 2006 - Classifier Technology and the Illusion of Progress.pdf:pdf},
isbn = {0883423060000},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {and phrases,empirical com-,error rate,flat maximum effect,lectivity bias,misclas-,population drift,principle of parsimony,problem uncertainty,se-,sification rate,simplicity,supervised classification},
month = feb,
number = {1},
pages = {1--14},
primaryClass = {arXiv:math},
title = {{Classifier Technology and the Illusion of Progress}},
url = {http://projecteuclid.org/Dienst/getRecord?id=euclid.ss/1149600839/},
volume = {21},
year = {2006}
}
@inproceedings{Kim2014,
author = {Kim, Do-kyum and Der, Matthew and Saul, Lawrence K.},
booktitle = {AISTATS},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kim, Der, Saul - 2014 - A Gaussian Latent Variable Model for Large Margin Classification of Labeled and Unlabeled Data.pdf:pdf},
title = {{A Gaussian Latent Variable Model for Large Margin Classification of Labeled and Unlabeled Data}},
url = {http://jmlr.org/proceedings/papers/v33/kim14a.pdf},
volume = {33},
year = {2014}
}
@article{Claassen2005,
author = {Claassen, Tom and Heskes, Tom},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Claassen, Heskes - 2005 - A Logical Characterization of Constraint-Based Causal Discovery.pdf:pdf},
title = {{A Logical Characterization of Constraint-Based Causal Discovery}},
year = {2005}
}
@article{Kalousis1999,
author = {Kalousis, Alexis and Theoharis, T},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kalousis, Theoharis - 1999 - NOEMON An intelligent Assistant for Classifier Selection.pdf:pdf},
journal = {Intelligent Data Analysis},
keywords = {classifier comparison,classifier selection,dataset morphology,multidimensional metrics},
number = {5},
pages = {319--337},
title = {{NOEMON: An intelligent Assistant for Classifier Selection}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.38.7762},
volume = {3},
year = {1999}
}
@inproceedings{Webb2002,
author = {Webb, Geoffrey I. and Brain, Damien},
booktitle = {Proceedings of the 2002 Pacific Rim Knowledge Acquisition Workshop},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Webb, Brain - 2002 - Generality is predictive of prediction accuracy.pdf:pdf},
pages = {117--130},
title = {{Generality is predictive of prediction accuracy}},
url = {http://www.csse.monash.edu/~webb/cgi-bin/publications.cgi?author=Webb\&keywords=Occams Razor\&pagetitle=Publications\%3A Occam's razor\&format=BibTeX\&sortby=type\&showabstract=y\&showkeywords=y},
year = {2002}
}
@inproceedings{Hernandez-reyes2005,
author = {Hern\'{a}ndez-reyes, Edith and Carrasco-Ochoa, J.A. and Mart\'{\i}nez-trinidad, J Fco},
booktitle = {Proceedings of the 10th Iberoamerican Congress on Pattern Recognition},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hern\'{a}ndez-reyes, Carrasco-Ochoa, Mart\'{\i}nez-trinidad - 2005 - Classifier Selection Based on Data Complexity Measures.pdf:pdf},
pages = {586--592},
title = {{Classifier Selection Based on Data Complexity Measures}},
year = {2005}
}
@article{Breiman1996,
author = {Breiman, Leo},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Breiman - 1996 - Bagging predictors.pdf:pdf},
journal = {Machine learning},
keywords = {aggregation,averaging,bootstrap,combining},
pages = {123--140},
title = {{Bagging predictors}},
url = {http://www.springerlink.com/index/L4780124W2874025.pdf},
volume = {140},
year = {1996}
}
@book{Wu2007,
author = {Wu, Xindong and Kumar, Vipin and {Ross Quinlan}, J. and Ghosh, Joydeep and Yang, Qiang and Motoda, Hiroshi and McLachlan, Geoffrey J. and Ng, Angus and Liu, Bing and Yu, Philip S. and Zhou, Zhi-Hua and Steinbach, Michael and Hand, David J. and Steinberg, Dan},
booktitle = {Knowledge and Information Systems},
doi = {10.1007/s10115-007-0114-2},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wu et al. - 2007 - Top 10 algorithms in data mining.pdf:pdf},
isbn = {1011500701},
issn = {0219-1377},
month = dec,
number = {1},
pages = {1--37},
title = {{Top 10 algorithms in data mining}},
url = {http://www.springerlink.com/index/10.1007/s10115-007-0114-2},
volume = {14},
year = {2007}
}
@article{Burman1989,
author = {Burman, Prabir},
doi = {10.2307/2336116},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Burman - 1989 - A Comparative Study of Ordinary Cross-Validation, v-Fold Cross-Validation and the Repeated Learning-Testing Methods.pdf:pdf},
issn = {00063444},
journal = {Biometrika},
month = sep,
number = {3},
pages = {503},
title = {{A Comparative Study of Ordinary Cross-Validation, v-Fold Cross-Validation and the Repeated Learning-Testing Methods}},
url = {http://www.jstor.org/stable/2336116?origin=crossref},
volume = {76},
year = {1989}
}
@article{Mirowski2008,
author = {Mirowski, Piotr W. and LeCun, Yann and Madhavan, Deepak and Kuzniecky, Ruben},
doi = {10.1109/MLSP.2008.4685487},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Mirowski et al. - 2008 - Comparing SVM and convolutional networks for epileptic seizure prediction from intracranial EEG.pdf:pdf},
isbn = {978-1-4244-2375-0},
journal = {2008 IEEE Workshop on Machine Learning for Signal Processing},
month = oct,
pages = {244--249},
publisher = {Ieee},
title = {{Comparing SVM and convolutional networks for epileptic seizure prediction from intracranial EEG}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4685487},
year = {2008}
}
@inproceedings{Cortes2010,
author = {Cortes, Corinna and Mansour, Yishay and Mohri, Mehryar},
booktitle = {Advances in Neural Information Processing Systems 23},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cortes, Mansour, Mohri - 2010 - Learning bounds for importance weighting.pdf:pdf},
pages = {442--450},
title = {{Learning bounds for importance weighting}},
url = {http://www.cs.nyu.edu/~mohri/pub/importance.pdf},
year = {2010}
}
@article{Doornik2008,
author = {Doornik, Jurgen A. and Hansen, Henrik},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Doornik, Hansen - 2008 - An Omnibus Test for Univariate and Multivariate Normality.pdf:pdf},
journal = {Oxford Bulletin of Economics and Statistics},
keywords = {johnson system,kurtosis,multivariate normality test,ness,skew-,univariate normality test,wilson-hilferty transformation},
number = {1},
pages = {927--939},
title = {{An Omnibus Test for Univariate and Multivariate Normality}},
url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1468-0084.2008.00537.x/full},
volume = {70},
year = {2008}
}
@article{Baldassarre2013,
archivePrefix = {arXiv},
arxivId = {arXiv:1303.3207v3},
author = {Baldassarre, Luca and Bhan, Nirav},
eprint = {arXiv:1303.3207v3},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Baldassarre, Bhan - 2013 - Group-Sparse Model Selection Hardness and Relaxations.pdf:pdf},
journal = {arXiv preprint arXiv: \ldots},
pages = {1--18},
title = {{Group-Sparse Model Selection: Hardness and Relaxations}},
url = {http://arxiv.org/abs/1303.3207},
year = {2013}
}
@article{Sun2010,
author = {Sun, Shiliang and Shawe-taylor, John},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sun, Shawe-taylor - 2010 - Sparse Semi-supervised Learning Using Conjugate Functions.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {fenchel-legendre conjugate,multi-,representer theorem,semi-supervised learning,statistical learning theory,support vector machine,view regularization},
pages = {2423--2455},
title = {{Sparse Semi-supervised Learning Using Conjugate Functions}},
volume = {11},
year = {2010}
}
@article{Ting1997,
author = {Ting, Kai Ming and Witten, Ian H.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ting, Witten - 1997 - Stacked Generalization when does it work.pdf:pdf},
title = {{Stacked Generalization: when does it work?}},
url = {http://researchcommons.waikato.ac.nz/handle/10289/1066},
year = {1997}
}
@article{Macia2013,
author = {Maci\`{a}, N\'{u}ria and Bernad\'{o}-Mansilla, Ester},
doi = {10.1016/j.ins.2013.08.059},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Maci\`{a}, Bernad\'{o}-Mansilla - 2013 - Towards UCI A mindful repository design.pdf:pdf},
issn = {00200255},
journal = {Information Sciences},
month = sep,
title = {{Towards UCI+: A mindful repository design}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0020025513006336},
year = {2013}
}
@article{Kalousis2004,
author = {Kalousis, Alexandros and Gama, Joao and Hilario, Melanie},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kalousis, Gama, Hilario - 2004 - On data and algorithms Understanding inductive performance.pdf:pdf},
journal = {Machine Learning},
number = {3},
pages = {275--312},
title = {{On data and algorithms: Understanding inductive performance}},
url = {http://link.springer.com/article/10.1023/B:MACH.0000015882.38031.85},
volume = {54},
year = {2004}
}
@article{Cesa-Bianchi2007,
author = {Cesa-Bianchi, Nicol\`{o}},
doi = {10.1016/j.tcs.2007.03.053},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cesa-Bianchi - 2007 - Applications of regularized least squares to pattern classification.pdf:pdf},
issn = {03043975},
journal = {Theoretical Computer Science},
keywords = {on-line learning,perceptron,ridge regression,selective sampling},
month = sep,
number = {3},
pages = {221--231},
title = {{Applications of regularized least squares to pattern classification}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S030439750700237X},
volume = {382},
year = {2007}
}
@article{Mooij,
author = {Mooij, Joris M and Heskes, Tom and Janzing, Dominik and Sch\"{o}lkopf, Bernhard},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Mooij et al. - Unknown - On Causal Discovery with Cyclic Additive Noise Models.pdf:pdf},
pages = {1--9},
title = {{On Causal Discovery with Cyclic Additive Noise Models}}
}
@article{McLachlan1982,
author = {McLachlan, Geoffrey J. and Ganesalingam, S.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/McLachlan, Ganesalingam - 1982 - Updating a discriminant function on the basis of unclassified data.pdf:pdf},
journal = {Communication in Statistics- Simulation and Computation},
title = {{Updating a discriminant function on the basis of unclassified data}},
url = {http://www.tandfonline.com/doi/full/10.1080/03610918208812293},
year = {1982}
}
@article{Morrison2011,
abstract = {A growing body of literature shows that one's working memory (WM) capacity can be expanded through targeted training. Given the established relationship between WM and higher cognition, these successful training studies have led to speculation that WM training may yield broad cognitive benefits. This review considers the current state of the emerging WM training literature, and details both its successes and limitations. We identify two distinct approaches to WM training, strategy training and core training, and highlight both the theoretical and practical motivations that guide each approach. Training-related increases in WM capacity have been successfully demonstrated across a wide range of subject populations, but different training techniques seem to produce differential impacts upon the broader landscape of cognitive abilities. In particular, core WM training studies seem to produce more far-reaching transfer effects, likely because they target domain-general mechanisms of WM. The results of individual studies encourage optimism regarding the value of WM training as a tool for general cognitive enhancement. However, we discuss several limitations that should be addressed before the field endorses the value of this approach.},
author = {Morrison, Alexandra B and Chein, Jason M},
doi = {10.3758/s13423-010-0034-0},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Morrison, Chein - 2011 - Does working memory training work The promise and challenges of enhancing cognition by training working memory.pdf:pdf},
issn = {1531-5320},
journal = {Psychonomic bulletin \& review},
keywords = {Adult,Aged,Attention,Child,Cognition,Humans,Intelligence,Judgment,Memory Disorders,Memory Disorders: therapy,Memory, Short-Term,Middle Aged,Pattern Recognition, Visual,Practice (Psychology),Serial Learning,Transfer (Psychology),Treatment Outcome,Verbal Learning,Young Adult},
month = feb,
number = {1},
pages = {46--60},
pmid = {21327348},
title = {{Does working memory training work? The promise and challenges of enhancing cognition by training working memory.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21327348},
volume = {18},
year = {2011}
}
@inproceedings{Airola2010,
author = {Airola, Antti and Pahikkala, Tapio and Boberg, Jorma and Salakoski, Tapio},
booktitle = {Ninth International Conference on Machine Learning and Applications},
doi = {10.1109/ICMLA.2010.158},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Airola et al. - 2010 - Applying Permutation Tests for Assessing the Statistical Significance of Wrapper Based Feature Selection.pdf:pdf},
isbn = {978-1-4244-9211-4},
month = dec,
pages = {989--994},
publisher = {Ieee},
title = {{Applying Permutation Tests for Assessing the Statistical Significance of Wrapper Based Feature Selection}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5708982},
year = {2010}
}
@article{Castelli1996,
author = {Castelli, Vittorio and Cover, Thomas M.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Castelli, Cover - 1996 - The Relative Value of Labeled and Unlabeled Samples in Pattern Recognition with an Unknown Mixing Parameter.pdf:pdf},
journal = {IEEE Transactions on Information Theory},
number = {6},
pages = {2102},
title = {{The Relative Value of Labeled and Unlabeled Samples in Pattern Recognition with an Unknown Mixing Parameter}},
volume = {42},
year = {1996}
}
@inproceedings{Ho2001a,
author = {Ho, Tin Kam},
booktitle = {Multiple Classifier Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ho - 2001 - Data Complexity Analysis for Classifier Combination.pdf:pdf},
pages = {53--67},
title = {{Data Complexity Analysis for Classifier Combination}},
year = {2001}
}
@inproceedings{Taigman2014,
author = {Taigman, Yaniv and Ranzato, Marc Aurelio and Aviv, Tel and Park, Menlo},
booktitle = {Computer Vision and Pattern Recognition},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Taigman et al. - 2014 - DeepFace Closing the Gap to Human-Level Performance in Face Verification.pdf:pdf},
title = {{DeepFace : Closing the Gap to Human-Level Performance in Face Verification}},
year = {2014}
}
@article{Meier2008,
author = {Meier, Lukas and {Van De Geer}, Sara and B\"{u}hlmann, Peter},
doi = {10.1111/j.1467-9868.2007.00627.x},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Meier, Van De Geer, B\"{u}hlmann - 2008 - The group lasso for logistic regression.pdf:pdf},
issn = {13697412},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
keywords = {categorical data,co-ordinate descent algorithm,dna splice site,group variable,high dimensional generalized linear,model,penalized likelihood,selection},
month = jan,
number = {1},
pages = {53--71},
title = {{The group lasso for logistic regression}},
url = {http://doi.wiley.com/10.1111/j.1467-9868.2007.00627.x},
volume = {70},
year = {2008}
}
@inproceedings{Foulds2011,
author = {Foulds, James and Smyth, Padhraic},
booktitle = {SIAM International Conference on Data Mining},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Foulds, Smyth - 2011 - Multi-instance mixture models and semi-supervised learning.pdf:pdf},
number = {Mi},
title = {{Multi-instance mixture models and semi-supervised learning}},
url = {http://siam.omnibooksonline.com/2011datamining/data/papers/256.pdf},
year = {2011}
}
@article{Zadeh2012,
author = {Zadeh, Reza Bosagh and Goel, Ashish},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zadeh, Goel - 2012 - Dimension independent similarity computation.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {cosine,dice,dimension independent,jaccard,mapreduce,overlap,similarity},
pages = {1605--1626},
title = {{Dimension independent similarity computation}},
url = {http://arxiv.org/abs/1206.2082},
volume = {14},
year = {2012}
}
@article{Canonical2008,
archivePrefix = {arXiv},
arxivId = {arXiv:1501.06111v1},
author = {Canonical, Extend and Canonical, Extend and Analysis, Correlation and Analysis, Correlation},
eprint = {arXiv:1501.06111v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Canonical et al. - 2008 - Journal of Statistical Software.pdf:pdf},
journal = {Journal Of Statistical Software},
keywords = {canonical correlations,cross-validation,regularization},
number = {12},
title = {{Journal of Statistical Software}},
volume = {23},
year = {2008}
}
@article{Buhlmann2002,
author = {Buhlmann, Peter and Yu, Bin},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Buhlmann, Yu - 2002 - Analyzing bagging.pdf:pdf},
journal = {The Annals of Statistics},
number = {4},
pages = {927--961},
title = {{Analyzing bagging}},
volume = {30},
year = {2002}
}
@article{Eddelbuettel2014,
author = {Eddelbuettel, Dirk and Sanderson, Conrad},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Eddelbuettel, Sanderson - 2014 - Rcpparmadillo Accelerating R with high-performance C linear algebra.pdf:pdf},
journal = {Computational Statistics \& Data Analysis},
keywords = {c,linear algebra,r,software},
pages = {1054--1063},
title = {{Rcpparmadillo: Accelerating R with high-performance C++ linear algebra}},
url = {http://www.sciencedirect.com/science/article/pii/S0167947313000492},
volume = {71},
year = {2014}
}
@misc{Czepiel2002,
author = {Czepiel, SA},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Czepiel - 2002 - Maximum likelihood estimation of logistic regression models theory and implementation.pdf:pdf},
title = {{Maximum likelihood estimation of logistic regression models: theory and implementation}},
url = {http://www.czep.net/stat/mlelr.pdf},
year = {2002}
}
@article{Ho2002a,
author = {Ho, Tin Kam},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ho - 2002 - A Data Complexity Analysis of Comparative Advantages of Decision Forest Constructors.pdf:pdf},
journal = {Pattern Analysis and Applications},
number = {2},
pages = {102--112},
title = {{A Data Complexity Analysis of Comparative Advantages of Decision Forest Constructors}},
volume = {5},
year = {2002}
}
@article{Sun2014,
author = {Sun, Peng and Reid, Mark D. and Zhou, Jie},
doi = {10.1007/s10994-014-5434-3},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sun, Reid, Zhou - 2014 - An improved multiclass LogitBoost using adaptive-one-vs-one.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
month = feb,
title = {{An improved multiclass LogitBoost using adaptive-one-vs-one}},
url = {http://link.springer.com/10.1007/s10994-014-5434-3},
year = {2014}
}
@inproceedings{Duch2012,
author = {Duch, Włodzisław and Jankowski, Norbert and Maszczyk, Tomasz},
booktitle = {International Joint Conference on Neural Networks},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Duch, Jankowski, Maszczyk - 2012 - Make it cheap learning with O (nd) complexity.pdf:pdf},
number = {2},
title = {{Make it cheap: learning with O (nd) complexity}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=6252380},
year = {2012}
}
@article{Chen,
archivePrefix = {arXiv},
arxivId = {arXiv:1104.2930v3},
author = {Chen, Aiyou and Jordan, Michael I},
eprint = {arXiv:1104.2930v3},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Chen, Jordan - Unknown - Cluster Forests.pdf:pdf},
pages = {1--23},
title = {{Cluster Forests}}
}
@article{Gelman2013,
abstract = {A substantial school in the philosophy of science identifies Bayesian inference with inductive inference and even rationality as such, and seems to be strengthened by the rise and practical success of Bayesian statistics. We argue that the most successful forms of Bayesian statistics do not actually support that particular philosophy but rather accord much better with sophisticated forms of hypothetico-deductivism. We examine the actual role played by prior distributions in Bayesian models, and the crucial aspects of model checking and model revision, which fall outside the scope of Bayesian confirmation theory. We draw on the literature on the consistency of Bayesian updating and also on our experience of applied work in social science. Clarity about these matters should benefit not just philosophy of science, but also statistical practice. At best, the inductivist view has encouraged researchers to fit and compare models without checking them; at worst, theorists have actively discouraged practitioners from performing model checking because it does not fit into their framework.},
author = {Gelman, Andrew and Shalizi, Cosma Rohilla},
doi = {10.1111/j.2044-8317.2011.02037.x},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gelman, Shalizi - 2013 - Philosophy and the practice of Bayesian statistics.pdf:pdf},
issn = {2044-8317},
journal = {The British journal of mathematical and statistical psychology},
month = feb,
number = {1},
pages = {8--38},
pmid = {22364575},
title = {{Philosophy and the practice of Bayesian statistics.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22364575},
volume = {66},
year = {2013}
}
@article{Cortes2012,
author = {Cortes, Corinna and Mohri, Mehryar and Rostamizadeh, Afshin},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cortes, Mohri, Rostamizadeh - 2012 - Ensembles of kernel predictors.pdf:pdf},
journal = {arXiv preprint},
title = {{Ensembles of kernel predictors}},
url = {http://arxiv.org/abs/1202.3712},
year = {2012}
}
@techreport{Vaart2012,
author = {van der Vaart, Aad and van Zanten, Harry},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/van der Vaart, van Zanten - 2012 - Nonparametric Bayesian Statistics.pdf:pdf},
title = {{Nonparametric Bayesian Statistics}},
year = {2012}
}
@article{Bengio2013,
abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning.},
author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
doi = {10.1109/TPAMI.2013.50},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bengio, Courville, Vincent - 2013 - Representation learning a review and new perspectives.pdf:pdf},
issn = {1939-3539},
journal = {IEEE transactions on pattern analysis and machine intelligence},
keywords = {Algorithms,Artificial Intelligence,Artificial Intelligence: trends,Humans,Neural Networks (Computer)},
month = aug,
number = {8},
pages = {1798--828},
pmid = {23787338},
title = {{Representation learning: a review and new perspectives.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23787338},
volume = {35},
year = {2013}
}
@article{Kulesza2012,
abstract = {Determinantal point processes (DPPs) are elegant probabilistic models of repulsion that arise in quantum physics and random matrix theory. In contrast to traditional structured models like Markov random fields, which become intractable and hard to approximate in the presence of negative correlations, DPPs offer efficient and exact algorithms for sampling, marginalization, conditioning, and other inference tasks. We provide a gentle introduction to DPPs, focusing on the intuitions, algorithms, and extensions that are most relevant to the machine learning community, and show how DPPs can be applied to real-world applications like finding diverse sets of high-quality search results, building informative summaries by selecting diverse sentences from documents, modeling non-overlapping human poses in images or video, and automatically building timelines of important news stories.},
archivePrefix = {arXiv},
arxivId = {1207.6083},
author = {Kulesza, Alex and Taskar, Ben},
doi = {10.1561/2200000044},
eprint = {1207.6083},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kulesza, Taskar - 2012 - Determinantal Point Processes for Machine Learning.pdf:pdf},
isbn = {9781601986283},
issn = {1935-8237},
journal = {Foundations and Trends® in Machine Learning},
number = {2-3},
pages = {123--286},
title = {{Determinantal Point Processes for Machine Learning}},
url = {http://arxiv.org/abs/1207.6083$\backslash$nhttp://www.nowpublishers.com/product.aspx?product=MAL\&doi=2200000044},
volume = {5},
year = {2012}
}
@article{Dhillon2013,
author = {Dhillon, Paramveer S. and Foster, Dean P. and Kakade, Sham M. and Ungar, Lyle H.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Dhillon et al. - 2013 - A Risk Comparison of Ordinary Least Squares vs Ridge Regression.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {pca,ridge regression,risk inflation},
pages = {1505--1511},
title = {{A Risk Comparison of Ordinary Least Squares vs Ridge Regression}},
url = {http://adsabs.harvard.edu/abs/2011arXiv1105.0875D},
volume = {14},
year = {2013}
}
@article{Robert2009,
abstract = {This solution manual contains the unabridged and original solutions to all the exercises proposed in Bayesian Core, along with R programs when necessary.},
archivePrefix = {arXiv},
arxivId = {0910.4696},
author = {Robert, Christian P. and Marin, Jean-Michel},
eprint = {0910.4696},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Robert, Marin - 2009 - Bayesian Core The Complete Solution Manual.pdf:pdf},
title = {{Bayesian Core: The Complete Solution Manual}},
url = {http://arxiv.org/abs/0910.4696},
year = {2009}
}
@inproceedings{Cortes2004,
author = {Cortes, Corinna and Mohri, Mehryar},
booktitle = {Advances in Neural Information Processing Systems 16},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cortes, Mohri - 2004 - AUC optimization vs. error rate minimization.pdf:pdf},
pages = {313--320},
title = {{AUC optimization vs. error rate minimization}},
url = {http://books.google.com/books?hl=en\&lr=\&id=0F-9C7K8fQ8C\&oi=fnd\&pg=PA313\&dq=AUC+Optimization+vs+.+Error+Rate+Minimization\&ots=TGKup\_Ra93\&sig=VTdv-C5TW9itNMlz43YJjmxRKAc},
year = {2004}
}
@article{Witten2010,
abstract = {We consider the problem of clustering observations using a potentially large set of features. One might expect that the true underlying clusters present in the data differ only with respect to a small fraction of the features, and will be missed if one clusters the observations using the full set of features. We propose a novel framework for sparse clustering, in which one clusters the observations using an adaptively chosen subset of the features. The method uses a lasso-type penalty to select the features. We use this framework to develop simple methods for sparse K-means and sparse hierarchical clustering. A single criterion governs both the selection of the features and the resulting clusters. These approaches are demonstrated on simulated data and on genomic data sets.},
author = {Witten, Daniela M. and Tibshirani, Robert},
doi = {10.1198/jasa.2010.tm09415},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Witten, Tibshirani - 2010 - A framework for feature selection in clustering.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {hierarchical clustering,high-dimensional,k-means clustering,lasso,model selection,sparsity,unsupervised learning},
month = jun,
number = {490},
pages = {713--726},
pmid = {20811510},
title = {{A framework for feature selection in clustering.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2930825\&tool=pmcentrez\&rendertype=abstract},
volume = {105},
year = {2010}
}
@phdthesis{Colas,
author = {Colas, Fabrice P. R.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Colas - 2009 - Data Mining Scenarios.pdf:pdf},
isbn = {9789090238883},
title = {{Data Mining Scenarios}},
year = {2009}
}
@article{Chandrasekaran2013,
abstract = {Modern massive datasets create a fundamental problem at the intersection of the computational and statistical sciences: how to provide guarantees on the quality of statistical inference given bounds on computational resources, such as time or space. Our approach to this problem is to define a notion of "algorithmic weakening," in which a hierarchy of algorithms is ordered by both computational efficiency and statistical efficiency, allowing the growing strength of the data at scale to be traded off against the need for sophisticated processing. We illustrate this approach in the setting of denoising problems, using convex relaxation as the core inferential tool. Hierarchies of convex relaxations have been widely used in theoretical computer science to yield tractable approximation algorithms to many computationally intractable tasks. In the current paper, we show how to endow such hierarchies with a statistical characterization and thereby obtain concrete tradeoffs relating algorithmic runtime to amount of data.},
author = {Chandrasekaran, Venkat and Jordan, Michael I},
doi = {10.1073/pnas.1302293110},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Chandrasekaran, Jordan - 2013 - Computational and statistical tradeoffs via convex relaxation.pdf:pdf},
issn = {1091-6490},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
month = mar,
number = {13},
pages = {E1181--90},
pmid = {23479655},
title = {{Computational and statistical tradeoffs via convex relaxation.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3612621\&tool=pmcentrez\&rendertype=abstract},
volume = {110},
year = {2013}
}
@article{Cao2007,
author = {Cao, Bin and Chen, Zheng},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cao, Chen - 2007 - Feature Selection in a Kernel Space.pdf:pdf},
title = {{Feature Selection in a Kernel Space}},
year = {2007}
}
@inproceedings{Kulesza2010,
abstract = {We present a novel probabilistic model for distributions over sets of structures— for example, sets of sequences, trees, or graphs. The critical characteristic of our model is a preference for diversity: sets containing dissimilar structures are more likely. Our model is a marriage of},
author = {Kulesza, Alex and Taskar, Ben},
booktitle = {Advances in Neural Information Processing Systems 23},
doi = {10.1080/00036840500405656},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kulesza, Taskar - 2010 - Structured Determinantal Point Processes.pdf:pdf},
isbn = {0003684050040},
issn = {<null>},
pages = {1--9},
title = {{Structured Determinantal Point Processes}},
year = {2010}
}
@inproceedings{Steck2003,
author = {Steck, Harald and Jaakkola, Tommi S.},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Steck, Jaakkola - 2003 - Bias-corrected bootstrap and model uncertainty.pdf:pdf},
title = {{Bias-corrected bootstrap and model uncertainty}},
url = {http://machinelearning.wustl.edu/mlpapers/paper\_files/NIPS2003\_AA66.pdf},
year = {2003}
}
@article{Buntine1991,
author = {Buntine, Wray},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Buntine - 1991 - Learning Classification Trees.pdf:pdf},
journal = {Artificial Intelligence and Statistics},
title = {{Learning Classification Trees}},
year = {1991}
}
@article{Lin,
author = {Lin, Chih-jen},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lin - Unknown - Support Vector Machine Solvers.pdf:pdf},
pages = {1--27},
title = {{Support Vector Machine Solvers}}
}
@inproceedings{Goldberg2007,
author = {Goldberg, Andrew B. and Zhu, Xiaojin and Wright, Stephen},
booktitle = {International Conference on Artificial Intelligence and Statistics},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Goldberg, Zhu, Wright - 2007 - Dissimilarity in graph-based semi-supervised classification.pdf:pdf},
number = {1},
pages = {55--162},
title = {{Dissimilarity in graph-based semi-supervised classification}},
url = {http://machinelearning.wustl.edu/mlpapers/paper\_files/AISTATS07\_GoldbergZW.pdf},
year = {2007}
}
@inproceedings{Ester1996,
author = {Ester, Martin and Kriegel, Hans-Peter and Sander, J\"{o}rg and Xu, Xiaowei},
booktitle = {KDD},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ester et al. - 1996 - A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise.pdf:pdf},
pages = {226--231},
title = {{A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise}},
year = {1996}
}
@article{Vanschoren2012,
author = {Vanschoren, Joaquin and Blockeel, Hendrik and Pfahringer, Bernhard and Holmes, Geoffrey},
doi = {10.1007/s10994-011-5277-0},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Vanschoren et al. - 2012 - Experiment databases.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
month = jan,
number = {2},
pages = {127--158},
title = {{Experiment databases}},
url = {http://www.springerlink.com/index/10.1007/s10994-011-5277-0},
volume = {87},
year = {2012}
}
@article{Vovk2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1502.06254v1},
author = {Vovk, Vladimir},
eprint = {arXiv:1502.06254v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Vovk - 2015 - The fundamental nature of the log loss function arXiv 1502 . 06254v1 cs . LG 22 Feb 2015.pdf:pdf},
pages = {1--6},
title = {{The fundamental nature of the log loss function arXiv : 1502 . 06254v1 [ cs . LG ] 22 Feb 2015}},
year = {2015}
}
@article{Adams,
archivePrefix = {arXiv},
arxivId = {arXiv:1504.01344v1},
author = {Adams, Ryan P},
eprint = {arXiv:1504.01344v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Adams - Unknown - Early Stopping is Nonparametric Variational Inference.pdf:pdf},
title = {{Early Stopping is Nonparametric Variational Inference}}
}
@inproceedings{Lawrence2004,
author = {Lawrence, Neil D. and Jordan, Michael I.},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lawrence, Jordan - 2004 - Semi-supervised learning via Gaussian processes.pdf:pdf},
pages = {753--760},
title = {{Semi-supervised learning via Gaussian processes}},
year = {2004}
}
@book{Zhang2010,
author = {Zhang, Cun-Hui},
booktitle = {The Annals of Statistics},
doi = {10.1214/09-AOS729},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhang - 2010 - Nearly unbiased variable selection under minimax concave penalty.pdf:pdf},
isbn = {9040210063},
issn = {0090-5364},
keywords = {and phrases,least squares,model selection,penalized estimation,variable selection},
month = apr,
number = {2},
pages = {894--942},
title = {{Nearly unbiased variable selection under minimax concave penalty}},
url = {http://projecteuclid.org/euclid.aos/1266586618},
volume = {38},
year = {2010}
}
@article{Spokoiny2012,
archivePrefix = {arXiv},
arxivId = {arXiv:1111.3029v4},
author = {Spokoiny, Vladimir},
eprint = {arXiv:1111.3029v4},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Spokoiny - 2012 - Parametric estimation. Finite sample theory.pdf:pdf},
journal = {The Annals of Statistics},
pages = {1--67},
title = {{Parametric estimation. Finite sample theory}},
url = {http://projecteuclid.org/euclid.aos/1360332187},
year = {2012}
}
@phdthesis{Hillebrand2012,
author = {Hillebrand, Arne},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hillebrand - 2012 - Separating a polygonal environment into a multi-layered environment.pdf:pdf},
keywords = {branch,explicit corridor map,ge-,graphs,local search,multi-layered environment,multicut,netic algorithm,price},
school = {Utrecht University},
title = {{Separating a polygonal environment into a multi-layered environment}},
year = {2012}
}
@article{Blei2012,
author = {Blei, David M.},
doi = {10.1145/2133806.2133826},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Blei - 2012 - Probabilistic topic models.pdf:pdf},
issn = {00010782},
journal = {Communications of the ACM},
month = apr,
number = {4},
pages = {77},
title = {{Probabilistic topic models}},
url = {http://dl.acm.org/citation.cfm?doid=2133806.2133826},
volume = {55},
year = {2012}
}
@article{Sohn1999,
author = {Sohn, So Young},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sohn - 1999 - Meta Analysis of Classification Algorithms for Pattern Recognition.pdf:pdf},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {11},
pages = {1137--1144},
title = {{Meta Analysis of Classification Algorithms for Pattern Recognition}},
volume = {21},
year = {1999}
}
@unpublished{Loogb,
author = {Loog, Marco},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Loog - Unknown - Semi-Supervised Linear Discriminant Analysis through Moment-Constraint Parameter Estimation.pdf:pdf},
keywords = {affine invariant,classification,constraints,linear discriminant analysis,moment,semi-supervised learning},
title = {{Semi-Supervised Linear Discriminant Analysis through Moment-Constraint Parameter Estimation}}
}
@article{VanderKooi2013,
abstract = {OBJECTIVES: We investigated how much the Human Development Index (HDI), a global measure of development, modifies the effect of education on self-reported health.

METHODS: We analyzed cross-sectional World Health Survey data on 217,642 individuals from 49 countries, collected in 2002 to 2005, with random-intercept multilevel linear regression models.

RESULTS: We observed greater positive associations between educational levels and self-reported good health with increasing HDI. The magnitude of this effect modification of the education-health relation tended to increase with educational attainment. For example, before adjustment for effect modification, at comparable HDI, on average, finishing primary school was associated with better general health (b = 1.49; 95\% confidence interval [CI] = 1.18, 1.80). With adjustment for effect modification by HDI, the impact became 4.63 (95\% CI = 3.63, 5.62) for every 0.1 increase in HDI. Among those who completed high school, these associations were, respectively, 5.59 (95\% CI = 5.20, 5.98) and 9.95 (95\% CI = 8.89, 11.00).

CONCLUSIONS: The health benefits of educational attainment are greater in countries with greater human development. Health inequalities attributable to education are, therefore, larger in more developed countries.},
author = {van der Kooi, Anne L F and Stronks, Karien and Thompson, Caroline a and DerSarkissian, Maral and Arah, Onyebuchi a},
doi = {10.2105/AJPH.2013.301593},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/van der Kooi et al. - 2013 - The modifying influence of country development on the effect of individual educational attainment on self-r.pdf:pdf},
issn = {1541-0048},
journal = {American journal of public health},
keywords = {Adult,Aged,Cross-Sectional Studies,Developed Countries,Developed Countries: statistics \& numerical data,Developing Countries,Developing Countries: statistics \& numerical data,Educational Status,Female,Health Status,Health Status Disparities,Health Surveys,Humans,Male,Middle Aged,Self Report,Young Adult},
month = nov,
number = {11},
pages = {e49--54},
pmid = {24028233},
title = {{The modifying influence of country development on the effect of individual educational attainment on self-rated health.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24028233},
volume = {103},
year = {2013}
}
@article{Astorino2007,
author = {Astorino, A and Fuduli, A},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Astorino, Fuduli - 2007 - Nonsmooth optimization techniques for Semi-Supervised Classification.pdf:pdf},
journal = {IEEE transactions on pattern analysis and machine intelligence},
keywords = {nonsmooth optimization,semi,supervised learning},
number = {12},
pages = {2135--2142},
title = {{Nonsmooth optimization techniques for Semi-Supervised Classification}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4359288},
volume = {29},
year = {2007}
}
@article{Gneiting2007,
author = {Gneiting, Tilmann and Raftery, Adrian E},
doi = {10.1198/016214506000001437},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gneiting, Raftery - 2007 - Strictly Proper Scoring Rules, Prediction, and Estimation.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {bayes factor,bregman divergence,brier score,coherent,continuous ranked probability score,cross-validation,distribution,entropy,kernel score,loss function,minimum contrast estimation,negative definite function,prediction interval,predictive,quantile forecast,scoring rule,skill score,strictly proper,utility function},
month = mar,
number = {477},
pages = {359--378},
title = {{Strictly Proper Scoring Rules, Prediction, and Estimation}},
url = {http://www.tandfonline.com/doi/abs/10.1198/016214506000001437},
volume = {102},
year = {2007}
}
@article{Culp2008,
author = {Culp, Mark and Michailidis, G},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Culp, Michailidis - 2008 - An iterative algorithm for extending learners to a semi-supervised setting.pdf:pdf},
journal = {Journal of Computational and Graphical Statistics},
keywords = {convergence,iterative algorithm,linear smoothers,semi-supervised learning},
number = {3},
pages = {545--571},
title = {{An iterative algorithm for extending learners to a semi-supervised setting}},
url = {http://amstat.tandfonline.com/doi/full/10.1198/106186008X344748},
volume = {17},
year = {2008}
}
@article{Ranganath2013,
abstract = {Variational inference has become a widely used method to approximate posteriors in complex latent variables models. However, deriving a variational inference algorithm generally requires significant model-specific analysis, and these efforts can hinder and deter us from quickly developing and exploring a variety of models for a problem at hand. In this paper, we present a "black box" variational inference algorithm, one that can be quickly applied to many models with little additional derivation. Our method is based on a stochastic optimization of the variational objective where the noisy gradient is computed from Monte Carlo samples from the variational distribution. We develop a number of methods to reduce the variance of the gradient, always maintaining the criterion that we want to avoid difficult model-based derivations. We evaluate our method against the corresponding black box sampling based methods. We find that our method reaches better predictive likelihoods much faster than sampling methods. Finally, we demonstrate that Black Box Variational Inference lets us easily explore a wide space of models by quickly constructing and evaluating several models of longitudinal healthcare data.},
archivePrefix = {arXiv},
arxivId = {1401.0118},
author = {Ranganath, Rajesh and Gerrish, Sean and Blei, David M.},
eprint = {1401.0118},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ranganath, Gerrish, Blei - 2013 - Black Box Variational Inference.pdf:pdf},
journal = {arXiv preprint arXiv:1401.0118},
title = {{Black Box Variational Inference}},
url = {http://arxiv.org/abs/1401.0118},
year = {2013}
}
@inproceedings{Chaudhuri2010,
author = {Chaudhuri, Kamalika and Dasgupta, Sanjoy},
booktitle = {Advances in Neural Information Processing Systems 23},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Chaudhuri, Dasgupta - 2010 - Rates of convergence for the cluster tree.pdf:pdf},
pages = {343--351},
title = {{Rates of convergence for the cluster tree}},
url = {http://machinelearning.wustl.edu/mlpapers/paper\_files/NIPS2010\_0496.pdf},
year = {2010}
}
@article{Jordan2013,
archivePrefix = {arXiv},
arxivId = {arXiv:1309.7804v1},
author = {Jordan, Michael I.},
doi = {10.3150/12-BEJSP17},
eprint = {arXiv:1309.7804v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Jordan - 2013 - On statistics, computation and scalability.pdf:pdf},
issn = {1350-7265},
journal = {Bernoulli},
keywords = {()},
month = sep,
number = {4},
pages = {1378--1390},
title = {{On statistics, computation and scalability}},
url = {http://projecteuclid.org/euclid.bj/1377612856},
volume = {19},
year = {2013}
}
@article{DeRidder2004,
author = {de Ridder, D. and Loog, Marco and Reinders, M.J.T.},
doi = {10.1109/ICPR.2004.1334176},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ridder, Loog, Reinders - 2004 - Local Fisher embedding.pdf:pdf},
isbn = {0-7695-2128-2},
journal = {Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004.},
number = {3},
pages = {295--298 Vol.2},
publisher = {Ieee},
title = {{Local Fisher embedding}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1334176},
year = {2004}
}
@inproceedings{Krijthe2012b,
author = {Krijthe, Jesse Hendrik and Ho, Tin Kam and Loog, Marco},
booktitle = {Proceedings of the 21st International Conference on Pattern Recognition},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Krijthe, Ho, Loog - 2012 - Improving cross-validation based classifier selection using meta-learning.pdf:pdf},
number = {1},
pages = {2873--2876},
title = {{Improving cross-validation based classifier selection using meta-learning}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=6460765},
year = {2012}
}
@article{Jenssen2006,
author = {Jenssen, Robert and Eltoft, Torbj\o rn and Erdogmus, Deniz and Principe, Jose C.},
doi = {10.1007/s11265-006-9771-8},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Jenssen et al. - 2006 - Some Equivalences between Kernel Methods and Information Theoretic Methods.pdf:pdf},
issn = {0922-5773},
journal = {The Journal of VLSI Signal Processing Systems for Signal, Image, and Video Technology},
month = dec,
number = {1-2},
pages = {49--65},
title = {{Some Equivalences between Kernel Methods and Information Theoretic Methods}},
url = {http://link.springer.com/10.1007/s11265-006-9771-8},
volume = {45},
year = {2006}
}
@inproceedings{Nigam2000a,
author = {Nigam, Kamal and Ghani, R},
booktitle = {Proceedings of the 9th International Conference on Information and Knowledge Management},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Nigam, Ghani - 2000 - Analyzing the effectiveness and applicability of co-training.pdf:pdf},
isbn = {1581133200},
keywords = {a related set of,blum and mitchell 1,for example,in problem domains where,into,present,research uses labeled and,the features naturally divide,two disjoint sets,unlabeled data},
pages = {86--93},
title = {{Analyzing the effectiveness and applicability of co-training}},
url = {http://dl.acm.org/citation.cfm?id=354805},
year = {2000}
}
@article{Neyshabur2014,
abstract = {We present experiments demonstrating that some other form of capacity control, different from network size, plays a central role in learning multilayer feed-forward networks. We argue, partially through analogy to matrix factorization, that this is an inductive bias that can help shed light on deep learning.},
archivePrefix = {arXiv},
arxivId = {1412.6614},
author = {Neyshabur, Behnam and Tomioka, Ryota and Srebro, Nathan},
eprint = {1412.6614},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Neyshabur, Tomioka, Srebro - 2014 - In Search of the Real Inductive Bias On the Role of Implicit Regularization in Deep Learning.pdf:pdf},
month = dec,
pages = {7},
title = {{In Search of the Real Inductive Bias: On the Role of Implicit Regularization in Deep Learning}},
url = {http://arxiv.org/abs/1412.6614},
year = {2014}
}
@inproceedings{Macia2010,
author = {Macia, Nuria and Ho, Tin Kam and Orriols-puig, Albert and Bernad\'{o}-Mansilla, Ester},
booktitle = {Proceedings of the 20th International Conference on Pattern Recognition},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Macia et al. - 2010 - The Landscape Contest at ICPR 2010.pdf:pdf},
pages = {29--45},
title = {{The Landscape Contest at ICPR 2010}},
year = {2010}
}
@article{Fan2014,
author = {Fan, Jianqing and Ke, Zheng Tracy},
doi = {10.1214/13-AOS1175},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Fan, Ke - 2014 - Discussion “a significance test for the lasso”.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
month = apr,
number = {2},
pages = {483--492},
title = {{Discussion: “a significance test for the lasso”}},
volume = {42},
year = {2014}
}
@article{Breiman2001,
author = {Breiman, Leo},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Breiman - 2001 - Statistical Modeling The Two Cultures.pdf:pdf},
journal = {Statistical Science},
number = {3},
pages = {199--231},
title = {{Statistical Modeling: The Two Cultures}},
url = {http://scholar.google.com/scholar?hl=en\&btnG=Search\&q=intitle:Statistical+Modeling+:+The+Two+Cultures\#2 http://scholar.google.com/scholar?hl=en\&btnG=Search\&q=intitle:Statistical+Modeling:+The+Two+Cultures\#2},
volume = {16},
year = {2001}
}
@article{Lim2014,
author = {Lim, Yongsub and Jung, Kyomin and Kohli, Pushmeet},
doi = {10.1109/TPAMI.2014.2306415},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lim, Jung, Kohli - 2014 - Efficient Energy Minimization for Enforcing Label Statistics.pdf:pdf},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
month = sep,
number = {9},
pages = {1893--1899},
title = {{Efficient Energy Minimization for Enforcing Label Statistics}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6740859},
volume = {36},
year = {2014}
}
@article{Tibshirani2010,
abstract = {We consider rules for discarding predictors in lasso regression and related problems, for computational efficiency. El Ghaoui et al (2010) propose "SAFE" rules that guarantee that a coefficient will be zero in the solution, based on the inner products of each predictor with the outcome. In this paper we propose strong rules that are not foolproof but rarely fail in practice. These can be complemented with simple checks of the Karush- Kuhn-Tucker (KKT) conditions to provide safe rules that offer substantial speed and space savings in a variety of statistical convex optimization problems.},
archivePrefix = {arXiv},
arxivId = {1011.2234},
author = {Tibshirani, Robert and Bien, Jacob and Friedman, Jerome and Hastie, Trevor and Simon, Noah and Taylor, Jonathan and Tibshirani, Ryan J.},
eprint = {1011.2234},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Tibshirani et al. - 2010 - Strong rules for discarding predictors in lasso-type problems.pdf:pdf},
month = nov,
number = {1},
title = {{Strong rules for discarding predictors in lasso-type problems}},
url = {http://arxiv.org/abs/1011.2234},
year = {2010}
}
@inproceedings{Aha1992,
author = {Aha, David W.},
booktitle = {International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Aha - 1992 - Generalizing from case studies A case study.pdf:pdf},
title = {{Generalizing from case studies: A case study}},
year = {1992}
}
@article{Robert2012,
archivePrefix = {arXiv},
arxivId = {arXiv:1006.5366v4},
author = {Robert, Christian P. and Gelman, Andrew},
eprint = {arXiv:1006.5366v4},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Robert, Gelman - 2012 - Not Only Defended But Also Applied The Perceived Absurdity of Bayesian Inference.pdf:pdf},
journal = {arXiv preprint},
keywords = {bayesian,doomsdsay,foundations,frequentist,laplace law of succession},
pages = {1--10},
title = {{"Not Only Defended But Also Applied": The Perceived Absurdity of Bayesian Inference.}},
url = {http://basepub.dauphine.fr/handle/123456789/11069},
year = {2012}
}
@inproceedings{Herbrich2006a,
author = {Herbrich, Ralf and Minka, Tom and Graepel, Thore},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Herbrich, Minka, Graepel - 2006 - TrueSkill A Bayesian Skill Rating System.pdf:pdf},
title = {{TrueSkill: A Bayesian Skill Rating System}},
year = {2006}
}
@article{Ramakrishnan2013,
address = {New York, New York, USA},
author = {Ramakrishnan, Raghu and Cisl, Team Members},
doi = {10.1145/2487575.2492151},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ramakrishnan, Cisl - 2013 - Scale-out beyond map-reduce.pdf:pdf},
isbn = {9781450321747},
journal = {Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '13},
keywords = {analytics,big data,data science,hadoop,machine learning,map-,reduce,reef,scale-out,sql,yarn},
pages = {1},
publisher = {ACM Press},
title = {{Scale-out beyond map-reduce}},
url = {http://dl.acm.org/citation.cfm?doid=2487575.2492151},
year = {2013}
}
@article{Yarowsky1995,
address = {Morristown, NJ, USA},
author = {Yarowsky, David},
doi = {10.3115/981658.981684},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Yarowsky - 1995 - Unsupervised word sense disambiguation rivaling supervised methods.pdf:pdf},
journal = {Proceedings of the 33rd annual meeting on Association for Computational Linguistics},
pages = {189--196},
publisher = {Association for Computational Linguistics},
title = {{Unsupervised word sense disambiguation rivaling supervised methods}},
url = {http://portal.acm.org/citation.cfm?doid=981658.981684},
year = {1995}
}
@article{Scott2009,
author = {Scott, Clayton and Blanchard, Gilles},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Scott, Blanchard - 2009 - Novelty detection Unlabeled data definitely help.pdf:pdf},
pages = {464--471},
title = {{Novelty detection: Unlabeled data definitely help}},
url = {http://eprints.pascal-network.org/archive/00004475/},
volume = {5},
year = {2009}
}
@article{Keiding2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1503.02853v1},
author = {Keiding, Niels and Clayton, David},
doi = {10.1214/13-STS453},
eprint = {arXiv:1503.02853v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Keiding, Clayton - 2014 - Standardization and Control for Confounding in Observational Studies A Historical Perspective.pdf:pdf},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {2,22K table, causality, decomposition of rates, epid,and phrases,causality,decomposition,epidemiology,expected number of deaths,g,h,k table,log-linear model,marginal structural model,national halothane study,odds ratio,of rates,rate,ratio,transportability,u,westergaard,yule},
number = {4},
pages = {529--558},
title = {{Standardization and Control for Confounding in Observational Studies: A Historical Perspective}},
url = {http://projecteuclid.org/euclid.ss/1421330546},
volume = {29},
year = {2014}
}
@article{Arlot2010,
author = {Arlot, Sylvain and Celisse, Alain},
doi = {10.1214/09-SS054},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Arlot, Celisse - 2010 - A survey of cross-validation procedures for model selection.pdf:pdf},
issn = {1935-7516},
journal = {Statistics Surveys},
keywords = {and phrases,cross-validation,leave-one-out,model selection},
pages = {40--79},
title = {{A survey of cross-validation procedures for model selection}},
url = {http://projecteuclid.org/euclid.ssu/1268143839},
volume = {4},
year = {2010}
}
@article{Kulis2009,
author = {Kulis, Brian and Grauman, Kristen},
doi = {10.1109/ICCV.2009.5459466},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kulis, Grauman - 2009 - Kernelized locality-sensitive hashing for scalable image search.pdf:pdf},
isbn = {978-1-4244-4420-5},
journal = {2009 IEEE 12th International Conference on Computer Vision},
month = sep,
number = {Iccv},
pages = {2130--2137},
publisher = {Ieee},
title = {{Kernelized locality-sensitive hashing for scalable image search}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5459466},
year = {2009}
}
@article{Sun2013,
author = {Sun, Quan and Pfahringer, Bernhard},
doi = {10.1007/s10994-013-5387-y},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sun, Pfahringer - 2013 - Pairwise meta-rules for better meta-learning-based algorithm ranking.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {algorithm ranking,ensemble learning,meta-learning,ranking trees},
month = jul,
number = {1},
pages = {141--161},
title = {{Pairwise meta-rules for better meta-learning-based algorithm ranking}},
url = {http://link.springer.com/10.1007/s10994-013-5387-y},
volume = {93},
year = {2013}
}
@article{Hartley1968a,
author = {Hartley, HO and Rao, JNK},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hartley, Rao - 1968 - Classification and estimation in analysis of variance problems(2).pdf:pdf},
journal = {Review of the International Statistical Institute},
number = {2},
pages = {141--147},
title = {{Classification and estimation in analysis of variance problems}},
url = {http://scholar.google.com/scholar?hl=en\&btnG=Search\&q=intitle:Classification+and+Estimation+in+Analysis+of+Variance+Problems\#1 http://scholar.google.com/scholar?hl=en\&btnG=Search\&q=intitle:Classification+and+estimation+in+analysis+of+variance+problems\#1},
volume = {36},
year = {1968}
}
@article{Kleijn2006a,
abstract = {We consider the asymptotic behavior of posterior distributions if the model is misspecified. Given a prior distribution and a random sample from a distribution \$P\_0\$, which may not be in the support of the prior, we show that the posterior concentrates its mass near the points in the support of the prior that minimize the Kullback--Leibler divergence with respect to \$P\_0\$. An entropy condition and a prior-mass condition determine the rate of convergence. The method is applied to several examples, with special interest for infinite-dimensional models. These include Gaussian mixtures, nonparametric regression and parametric models.},
archivePrefix = {arXiv},
arxivId = {math/0607023},
author = {Kleijn, B. J K and {Van Der Vaart}, a. W.},
doi = {10.1214/009053606000000029},
eprint = {0607023},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kleijn, Van Der Vaart - 2006 - Misspecification in infinite-dimensional Bayesian statistics.pdf:pdf},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Infinite-dimensional model,Misspecification,Posterior distribution,Rate of convergence},
number = {2},
pages = {837--877},
primaryClass = {math},
title = {{Misspecification in infinite-dimensional Bayesian statistics}},
volume = {34},
year = {2006}
}
@article{Huang2014,
author = {Huang, Y.},
doi = {10.1093/biomet/asu004},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Huang - 2014 - Bootstrap for the case-cohort design.pdf:pdf},
issn = {0006-3444},
journal = {Biometrika},
month = may,
number = {2},
pages = {465--476},
title = {{Bootstrap for the case-cohort design}},
url = {http://biomet.oxfordjournals.org/cgi/doi/10.1093/biomet/asu004},
volume = {101},
year = {2014}
}
@article{Wang2009,
author = {Wang, Xiaozhe and Smith-Miles, Kate and Hyndman, Rob},
doi = {10.1016/j.neucom.2008.10.017},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wang, Smith-Miles, Hyndman - 2009 - Rule induction for forecasting method selection Meta-learning the characteristics of univariate time.pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
month = jun,
number = {10-12},
pages = {2581--2594},
title = {{Rule induction for forecasting method selection: Meta-learning the characteristics of univariate time series}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0925231208005134},
volume = {72},
year = {2009}
}
@inproceedings{Ho2008,
author = {Ho, Tin Kam},
booktitle = {Proceedings of the 2008 Joint IAPR International Workshop on Structural, Syntactic, and Statistical Pattern Recognition},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ho - 2008 - Data complexity analysis Linkage between context and solution in classification.pdf:pdf},
pages = {986--995},
title = {{Data complexity analysis: Linkage between context and solution in classification}},
url = {http://link.springer.com/chapter/10.1007/978-3-540-89689-0\_102},
year = {2008}
}
@article{Gratiet2014,
author = {Gratiet, Loic and Garnier, Josselin},
doi = {10.1007/s10994-014-5437-0},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gratiet, Garnier - 2014 - Asymptotic analysis of the learning curve for Gaussian process regression.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {asymptotic mean squared error,convergence rate,gaussian process regression,generalization error,learning curves},
month = mar,
title = {{Asymptotic analysis of the learning curve for Gaussian process regression}},
url = {http://link.springer.com/10.1007/s10994-014-5437-0},
year = {2014}
}
@inproceedings{Sindhwani2005,
author = {Sindhwani, Vikas and Niyogi, Partha and Belkin, Mikhail},
booktitle = {Proceedings of the 22nd international conference on Machine learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sindhwani, Niyogi, Belkin - 2005 - Beyond the point cloud from transductive to semi-supervised learning.pdf:pdf},
number = {0},
pages = {824--831},
title = {{Beyond the point cloud: from transductive to semi-supervised learning}},
url = {http://dl.acm.org/citation.cfm?id=1102455},
year = {2005}
}
@article{Stahlecker1996,
author = {Stahlecker, Peter and Knautz, Henning and Trenkler, Gotz},
doi = {10.1007/BF00046994},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Stahlecker, Knautz, Trenkler - 1996 - Minimax adjustment technique in a parameter restricted linear model.pdf:pdf},
issn = {0167-8019},
journal = {Acta Applicandae Mathematicae},
keywords = {linear regression,minimax adjustment,projection estimator},
month = apr,
number = {1},
pages = {139--144},
title = {{Minimax adjustment technique in a parameter restricted linear model}},
url = {http://link.springer.com/10.1007/BF00046994},
volume = {43},
year = {1996}
}
@article{Bengio2010,
author = {Bengio, Yoshua and Delalleau, Olivier and Simard, Clarence},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bengio, Delalleau, Simard - 2010 - Decision trees do not generalize to new variations.pdf:pdf},
journal = {Computational Intelligence},
number = {4},
pages = {449--467},
title = {{Decision trees do not generalize to new variations}},
url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-8640.2010.00366.x/full},
volume = {26},
year = {2010}
}
@article{Xiong2013,
author = {Xiong, S.},
doi = {10.1093/biomet/ast041},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Xiong - 2013 - Better subset regression.pdf:pdf},
issn = {0006-3444},
journal = {Biometrika},
month = nov,
number = {November 2013},
pages = {71--84},
title = {{Better subset regression}},
url = {http://biomet.oxfordjournals.org/cgi/doi/10.1093/biomet/ast041},
year = {2013}
}
@article{Welinder2013,
author = {Welinder, Peter and Welling, Max and Perona, Pietro},
doi = {10.1109/CVPR.2013.419},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Welinder, Welling, Perona - 2013 - A Lazy Man's Approach to Benchmarking Semisupervised Classifier Evaluation and Recalibration.pdf:pdf},
isbn = {978-0-7695-4989-7},
journal = {2013 IEEE Conference on Computer Vision and Pattern Recognition},
month = jun,
pages = {3262--3269},
publisher = {Ieee},
title = {{A Lazy Man's Approach to Benchmarking: Semisupervised Classifier Evaluation and Recalibration}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6619263},
year = {2013}
}
@inproceedings{Szummer2000,
author = {Szummer, Martin and Jaakkola, Tommi},
booktitle = {Advances in Neural Information Processing Systems 13},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Szummer, Jaakkola - 2000 - Kernel expansions with unlabeled examples.pdf:pdf},
pages = {626--632},
title = {{Kernel expansions with unlabeled examples}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.87.222\&rep=rep1\&type=pdf},
year = {2000}
}
@article{Schmidt1995,
author = {Schmidt, Karsten and Stahlecker, Peter},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Schmidt, Stahlecker - 1995 - Reducing the Maximum Risk of Regression Estimators by Polyhedral Projection.pdf:pdf},
journal = {Journal of Statistical Computation and Simulation},
number = {1},
pages = {1--15},
title = {{Reducing the Maximum Risk of Regression Estimators by Polyhedral Projection}},
url = {http://www.tandfonline.com/doi/abs/10.1080/00949659508811648},
volume = {52},
year = {1995}
}
@article{Liu2010,
abstract = {Most genomic data have ultra-high dimensions with more than 10,000 genes (probes). Regularization methods with L₁ and L(p) penalty have been extensively studied in survival analysis with high-dimensional genomic data. However, when the sample size n << m (the number of genes), directly identifying a small subset of genes from ultra-high (m > 10, 000) dimensional data is time-consuming and not computationally efficient. In current microarray analysis, what people really do is select a couple of thousands (or hundreds) of genes using univariate analysis or statistical tests, and then apply the LASSO-type penalty to further reduce the number of disease associated genes. This two-step procedure may introduce bias and inaccuracy and lead us to miss biologically important genes.},
author = {Liu, Zhenqiu and Chen, Dechang and Tan, Ming and Jiang, Feng and Gartenhaus, Ronald B},
doi = {10.1186/1471-2105-11-606},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Liu et al. - 2010 - Kernel based methods for accelerated failure time model with ultra-high dimensional data.pdf:pdf},
issn = {1471-2105},
journal = {BMC bioinformatics},
keywords = {Algorithms,Gene Expression Profiling,Gene Expression Profiling: methods,Linear Models,Oligonucleotide Array Sequence Analysis,Oligonucleotide Array Sequence Analysis: methods,Survival Analysis},
month = jan,
number = {5},
pages = {606},
pmid = {21176134},
title = {{Kernel based methods for accelerated failure time model with ultra-high dimensional data.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21550884},
volume = {11},
year = {2010}
}
@article{Cai2014,
author = {Cai, T. Tony and Yuan, Ming},
doi = {10.1214/13-AOS1175},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cai, Yuan - 2014 - Discussion “a significance test for the lasso”.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
month = apr,
number = {2},
pages = {478--482},
title = {{Discussion: “a significance test for the lasso”}},
volume = {42},
year = {2014}
}
@techreport{Wolpert1996,
author = {Wolpert, David H and Macready, W},
booktitle = {Santa Fe Institute Technical Report},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wolpert, Macready - 1996 - Combining Stacking With Bagging To Improve A Learning Algorithm.pdf:pdf},
pages = {1--28},
title = {{Combining Stacking With Bagging To Improve A Learning Algorithm}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.53.9933\&rep=rep1\&type=pdf},
year = {1996}
}
@article{Talwalkar2013,
author = {Talwalkar, Ameet and Kumar, Sanjiv and Mohri, Mehryar and Rowley, Henry},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Talwalkar et al. - 2013 - Large-scale SVD and manifold learning.pdf:pdf},
journal = {Journal of Machine Learning Research},
pages = {3129--3152},
title = {{Large-scale SVD and manifold learning}},
url = {http://dl.acm.org/citation.cfm?id=2567761},
volume = {14},
year = {2013}
}
@inproceedings{Wager2013,
archivePrefix = {arXiv},
arxivId = {arXiv:1307.1493v2},
author = {Wager, Stefan and Wang, Sida and Liang, Percy},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {arXiv:1307.1493v2},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wager, Wang, Liang - 2013 - Dropout training as adaptive regularization.pdf:pdf},
pages = {351--359},
title = {{Dropout training as adaptive regularization}},
url = {http://papers.nips.cc/paper/4882-dropout-training-as-adaptive-regularization},
year = {2013}
}
@article{Pan2013,
author = {Pan, Wei and Shen, Xiaotong and Liu, Binghui},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Pan, Shen, Liu - 2013 - Cluster Analysis Unsupervised Learning via Supervised Learning with a Non-convex Penalty.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {generalized degrees of freedom,gression,grouping,k-means clustering,lasso,penalized re-,tlp,truncated lasso penalty},
pages = {1865--1889},
title = {{Cluster Analysis: Unsupervised Learning via Supervised Learning with a Non-convex Penalty}},
volume = {14},
year = {2013}
}
@article{Duvenaud,
archivePrefix = {arXiv},
arxivId = {arXiv:1502.03492v1},
author = {Duvenaud, David and Adams, Ryan P},
eprint = {arXiv:1502.03492v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Duvenaud, Adams - Unknown - Gradient-based Hyperparameter Optimization through Reversible Learning.pdf:pdf},
title = {{Gradient-based Hyperparameter Optimization through Reversible Learning}}
}
@article{Li2015,
author = {Li, Yu-Feng and Zhou, Zhi-Hua},
doi = {10.1109/TPAMI.2014.2299812},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Li, Zhou - 2015 - Towards Making Unlabeled Data Never Hurt.pdf:pdf},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
month = jan,
number = {1},
pages = {175--188},
title = {{Towards Making Unlabeled Data Never Hurt}},
volume = {37},
year = {2015}
}
@book{Gelman2003,
author = {Gelman, Andrew and Carlin, John B. and Stern, Hal S. and Rubin, Donald B.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gelman et al. - 2003 - Bayesian Data Analysis.pdf:pdf},
title = {{Bayesian Data Analysis}},
url = {http://onlinelibrary.wiley.com/doi/10.1002/cbdv.200490137/abstract http://scholar.google.com/scholar?hl=en\&btnG=Search\&q=intitle:Bayesian+Data+Analysis\#0},
year = {2003}
}
@misc{Young,
author = {Young, Peter},
booktitle = {Order A Journal On The Theory Of Ordered Sets And Its Applications},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Young - Unknown - Jackknife and Bootstrap Resampling Methods in Statistical Analysis to Correct for Bias.pdf:pdf},
number = {8},
pages = {1--9},
title = {{Jackknife and Bootstrap Resampling Methods in Statistical Analysis to Correct for Bias}},
volume = {1}
}
@article{Li2011a,
abstract = {Classifying biological data into different groups is a central task of bioinformatics: for instance, to predict the function of a gene or protein, the disease state of a patient or the phenotype of an individual based on its genotype. Support Vector Machines are a wide spread approach for classifying biological data, due to their high accuracy, their ability to deal with structured data such as strings, and the ease to integrate various types of data. However, it is unclear how to correct for confounding factors such as population structure, age or gender or experimental conditions in Support Vector Machine classification.},
author = {Li, Limin and Rakitsch, Barbara and Borgwardt, Karsten},
doi = {10.1093/bioinformatics/btr204},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Li, Rakitsch, Borgwardt - 2011 - ccSVM Correcting Support Vector Machines for confounding factors in biological data classification.pdf:pdf},
issn = {13674803},
journal = {Bioinformatics},
number = {13},
pmid = {21685091},
title = {{ccSVM: Correcting Support Vector Machines for confounding factors in biological data classification}},
volume = {27},
year = {2011}
}
@article{VanRooden2010,
abstract = {The clinical variability between patients with Parkinson's disease (PD) may point at the existence of subtypes of the disease. Identification of subtypes is important, since a focus on homogeneous groups may enhance the chance of success of research on mechanisms of disease and may also lead to tailored treatment strategies. Cluster analysis (CA) is an objective method to classify patients into subtypes. We systematically reviewed the methodology and results of CA studies in PD to gain a better understanding of the robustness of identified subtypes. We found seven studies that fulfilled the inclusion criteria. Studies were limited by incomplete reporting and methodological limitations. Differences between studies rendered comparisons of the results difficult. However, it appeared that studies which applied a comparable design identified similar subtypes. The cluster profiles "old age-at-onset and rapid disease progression" and "young age-at-onset and slow disease progression" emerged from the majority of studies. Other cluster profiles were less consistent across studies. Future studies with a rigorous study design that is standardized with respect to the included variables, data processing, and CA technique may advance the knowledge on subtypes in PD.},
author = {van Rooden, Stephanie M and Heiser, Willem J and Kok, Joost N and Verbaan, Dagmar and van Hilten, Jacobus J and Marinus, Johan},
doi = {10.1002/mds.23116},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/van Rooden et al. - 2010 - The identification of Parkinson's disease subtypes using cluster analysis a systematic review.pdf:pdf},
issn = {1531-8257},
journal = {Movement disorders : official journal of the Movement Disorder Society},
keywords = {Algorithms,Cluster Analysis,Humans,Parkinson Disease,Parkinson Disease: classification,PubMed,PubMed: statistics \& numerical data},
month = jun,
number = {8},
pages = {969--78},
pmid = {20535823},
title = {{The identification of Parkinson's disease subtypes using cluster analysis: a systematic review.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20535823},
volume = {25},
year = {2010}
}
@article{Orbanz2013,
abstract = {The natural habitat of most Bayesian methods is data represented by exchangeable sequences of observations, for which de Finetti's theorem provides the theoretical foundation. Dirichlet process clustering, Gaussian process regression, and many other parametric and nonparametric Bayesian models fall within the remit of this framework; many problems arising in modern data analysis do not. This expository paper provides an introduction to Bayesian models of graphs, matrices, and other data that can be modeled by random structures. We describe results in probability theory that generalize de Finetti's theorem to such data and discuss the relevance of these results to nonparametric Bayesian modeling. With the basic ideas in place, we survey example models available in the literature; applications of such models include collaborative filtering, link prediction, and graph and network analysis. We also highlight connections to recent developments in graph theory and probability, and sketch the more general mathematical foundation of Bayesian methods for other types of data beyond sequences and arrays.},
archivePrefix = {arXiv},
arxivId = {1312.7857},
author = {Orbanz, Peter and Roy, Daniel M.},
doi = {10.1109/TPAMI.2014.2334607},
eprint = {1312.7857},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Orbanz, Roy - 2013 - Bayesian Models of Graphs, Arrays and Other Exchangeable Random Structures.pdf:pdf},
issn = {0162-8828},
journal = {arXiv},
keywords = {abstract,arrays and,esian models of graphs,m,most bayesian,other exchangeable random structures,peter orbanz and daniel,roy,the natural habitat of},
number = {2},
pages = {1--25},
title = {{Bayesian Models of Graphs, Arrays and Other Exchangeable Random Structures}},
url = {http://arxiv.org/abs/1312.7857},
volume = {37},
year = {2013}
}
@inproceedings{Scholkopf2012,
author = {Sch\"{o}lkopf, Bernhard and Janzing, Dominik and Peters, Jonas},
booktitle = {Proceedings of the 29th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sch\"{o}lkopf, Janzing, Peters - 2012 - On Causal and Anticausal Learning.pdf:pdf},
pages = {1255--1262},
title = {{On Causal and Anticausal Learning}},
url = {http://arxiv.org/abs/1206.6471},
year = {2012}
}
@article{Filippone2014,
author = {Filippone, Maurizio and Girolami, Mark},
doi = {10.1109/TPAMI.2014.2316530},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Filippone, Girolami - 2014 - Pseudo-Marginal Bayesian Inference for Gaussian Processes.pdf:pdf},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
month = nov,
number = {11},
pages = {2214--2226},
title = {{Pseudo-Marginal Bayesian Inference for Gaussian Processes}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6786502},
volume = {36},
year = {2014}
}
@article{Garcia-Laencina2010,
abstract = {Pattern classification has been successfully applied in many problem domains, such as biometric recognition, document classification or medical diagnosis. Missing or unknown data are a common drawback that pattern recognition techniques need to deal with when solving real-life classification tasks. Machine learning approaches and methods imported from statistical learning theory have been most intensively studied and used in this subject. The aim of this work is to analyze the missing data problem in pattern classification tasks, and to summarize and compare some of the well-known methods used for handling missing values.},
author = {Garc\'{\i}a-Laencina, Pedro J. and Sancho-G\'{o}mez, Jos\'{e}-Luis and Figueiras-Vidal, An\'{\i}bal R.},
doi = {10.1007/s00521-009-0295-6},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Garc\'{\i}a-Laencina, Sancho-G\'{o}mez, Figueiras-Vidal - 2010 - Pattern classification with missing data a review.pdf:pdf},
issn = {0941-0643},
journal = {Neural Computing and Applications},
keywords = {data \'{a},learning,neural networks \'{a} machine,pattern classification \'{a} missing},
number = {2},
pages = {263--282},
title = {{Pattern classification with missing data: a review}},
volume = {19},
year = {2010}
}
@article{Chatterjee2007,
author = {Chatterjee, Sangit and Firat, Aykut},
doi = {10.1198/000313007X220057},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Chatterjee, Firat - 2007 - Generating Data with Identical Statistics but Dissimilar Graphics.pdf:pdf},
isbn = {000313007X},
issn = {0003-1305},
journal = {The American Statistician},
month = aug,
number = {3},
pages = {248--254},
title = {{Generating Data with Identical Statistics but Dissimilar Graphics}},
url = {http://www.tandfonline.com/doi/abs/10.1198/000313007X220057},
volume = {61},
year = {2007}
}
@article{Kasabov2003,
author = {Kasabov, Nikola and Pang, Shaoning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kasabov, Pang - 2003 - Transductive Support Vector Machines and Applications in Bioinformatics for Promoter Recognition.pdf:pdf},
journal = {Proceedings of the International Conference on Neural networks and signal processing},
keywords = {inductive svm,motif,promoter,promoter recognition,transductive svm},
number = {2},
pages = {31--38},
title = {{Transductive Support Vector Machines and Applications in Bioinformatics for Promoter Recognition}},
volume = {3},
year = {2003}
}
@inproceedings{Zhang2000,
author = {Zhang, Tong},
booktitle = {Proceedings of the 17th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhang - 2000 - The value of unlabeled data for classification problems.pdf:pdf},
pages = {1191--1198},
title = {{The value of unlabeled data for classification problems}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.20.6025\&rep=rep1\&type=pdf},
year = {2000}
}
@inproceedings{Dasgupta2002,
author = {Dasgupta, Sanjoy and Littman, Michael L. and McAlles},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Dasgupta, Littman, McAlles - 2002 - PAC generalization bounds for co-training.pdf:pdf},
pages = {375--382},
title = {{PAC generalization bounds for co-training}},
url = {http://books.google.com/books?hl=en\&lr=\&id=PGrlRWV5-v0C\&oi=fnd\&pg=PA375\&dq=PAC+Generalization+Bounds+for+Co-training\&ots=auaN1CGPip\&sig=0dID1oXJYgeENxwSzfsntvwz\_oU},
year = {2002}
}
@article{Janson2013,
abstract = {To most applied statisticians, a fitting procedure's degrees of freedom is synonymous with its model complexity, or its capacity for overfitting to data. In particular, it is often used to parameterize the bias-variance tradeoff in model selection. We argue that, contrary to folk intuition, model complexity and degrees of freedom are not synonymous and may correspond very poorly. We exhibit and theoretically explore various examples of fitting procedures for which degrees of freedom is not monotonic in the model complexity parameter, and can exceed the total dimension of the response space. Even in very simple settings, the degrees of freedom can exceed the dimension of the ambient space by an arbitrarily large amount. We show the degrees of freedom for any non-convex projection method can be unbounded.},
archivePrefix = {arXiv},
arxivId = {1312.7851},
author = {Janson, Lucas and Fithian, William and Hastie, Trevor},
eprint = {1312.7851},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Janson, Fithian, Hastie - 2013 - Effective Degrees of Freedom A Flawed Metaphor.pdf:pdf},
month = dec,
pages = {1--15},
title = {{Effective Degrees of Freedom: A Flawed Metaphor}},
url = {http://arxiv.org/abs/1312.7851},
year = {2013}
}
@inproceedings{Xing2013,
author = {Xing, Yan and Cai, H and Cai, Yanguang and Hejlesen, Ole and Toft, Egon},
booktitle = {Proceedings of 2013 Chinese \ldots},
doi = {10.1007/978-3-642-38466-0},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Xing et al. - 2013 - Preliminary Evaluation of Classification Complexity Measures on Imbalanced Data.pdf:pdf},
isbn = {9783642384660},
keywords = {classification,data complexity,\'{a} imbalanced data \'{a}},
pages = {189--196},
publisher = {Springer},
title = {{Preliminary Evaluation of Classification Complexity Measures on Imbalanced Data}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-38466-0\_22},
year = {2013}
}
@article{Mordelet2010,
archivePrefix = {arXiv},
arxivId = {arXiv:1010.0772v1},
author = {Mordelet, Fantine and Vert, Jean-Philippe},
eprint = {arXiv:1010.0772v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Mordelet, Vert - 2010 - A bagging SVM to learn from positive and unlabeled examples.pdf:pdf},
journal = {arXiv preprint},
pages = {1--15},
title = {{A bagging SVM to learn from positive and unlabeled examples}},
url = {http://arxiv.org/abs/1010.0772},
year = {2010}
}
@article{Senn2011,
author = {Senn, Stephen},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Senn - 2011 - You may believe you are a Bayesian but you are probably wrong.pdf:pdf},
journal = {Rationality, Markets and Morals},
pages = {48--66},
title = {{You may believe you are a Bayesian but you are probably wrong}},
url = {http://www.rmm-journal.com/downloads/Article\_Senn.pdf},
volume = {2},
year = {2011}
}
@article{Cortes2012a,
author = {Cortes, Corinna and Mohri, Mehryar and Rostamizadeh, Afshin},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cortes, Mohri, Rostamizadeh - 2012 - Algorithms for learning kernels based on centered alignment.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {feature selection,kernel methods,learning kernels},
pages = {795--828},
title = {{Algorithms for learning kernels based on centered alignment}},
url = {http://dl.acm.org/citation.cfm?id=2188413},
volume = {13},
year = {2012}
}
@inproceedings{Giraud-Carrier2008,
author = {Giraud-carrier, Christophe},
booktitle = {Tutorial at the 2008 International Conference on Machine Learning and Applications},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Giraud-carrier - 2008 - Metalearning - A Tutorial.pdf:pdf},
number = {December},
title = {{Metalearning - A Tutorial}},
url = {http://dml.cs.byu.edu/~cgc/docs/ICMLA2008Tut/ICMLA 2008.pdf},
year = {2008}
}
@article{Balcan2014,
abstract = {One of the most widely used techniques for data clustering is agglomerative clustering. Such algorithms have been long used across many different fields ranging from computational biology to social sciences to computer vision in part because their output is easy to interpret. Unfortunately, it is well known, however, that many of the classic agglomerative clustering algorithms are not robust to noise $\backslash$cite\{qcluster2005\}. In this paper we propose and analyze a new robust algorithm for bottom-up agglomerative clustering. We show that our algorithm can be used to cluster accurately in cases where the data satisfies a number of natural properties and where the traditional agglomerative algorithms fail. We also show how to adapt our algorithm to the inductive setting where our given data is only a small random sample of the entire data set. Experimental evaluations on synthetic and real world data sets show that our algorithm consistently achieves better performance than other hierarchical algorithms in the presence of noise.},
archivePrefix = {arXiv},
arxivId = {1401.0247},
author = {Balcan, Mf Maria-florina and Liang, Yingyu and Gupta, Pramod},
eprint = {1401.0247},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Balcan, Liang, Gupta - 2014 - Robust hierarchical clustering.pdf:pdf},
isbn = {9780982252925},
journal = {arXiv preprint arXiv:1401.0247},
keywords = {agglomerative algorithms,clustering,robustness,unsupervised learning},
pages = {35},
title = {{Robust hierarchical clustering}},
url = {http://arxiv.org/abs/1401.0247},
volume = {15},
year = {2014}
}
@article{Zhao2013,
author = {Zhao, Ming-Jie and Edakunni, Narayanan and Pocock, Adam and Brown, Gavin},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhao et al. - 2013 - Beyond Fano’s Inequality Bounds on the Optimal F-Score , BER , and Cost-Sensitive Risk and Their Implications.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {balanced error rate,conditional entropy,cost-sensitive risk,f $\beta$ -measure,f-score},
pages = {1033--1090},
title = {{Beyond Fano’s Inequality : Bounds on the Optimal F-Score , BER , and Cost-Sensitive Risk and Their Implications}},
url = {http://jmlr.csail.mit.edu/papers/volume14/zhao13a/zhao13a.pdf},
volume = {14},
year = {2013}
}
@misc{Bache2013,
author = {Bache, K. and Lichman, M.},
publisher = {University of California, Irvine, School of Information and Computer Sciences},
title = {{\{UCI\} Machine Learning Repository}},
url = {http://archive.ics.uci.edu/ml},
year = {2013}
}
@article{Mai2013,
author = {Mai, Qing and Zou, Hui},
doi = {10.1093/biomet/ass062},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Mai, Zou - 2013 - The Kolmogorov filter for variable screening in high-dimensional binary classification.pdf:pdf},
journal = {Biometrika},
number = {1},
pages = {229--234},
title = {{The Kolmogorov filter for variable screening in high-dimensional binary classification}},
url = {http://biomet.oxfordjournals.org/content/100/1/229.short},
volume = {100},
year = {2013}
}
@phdthesis{Hamers2012,
author = {Hamers, Adrian},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hamers - 2012 - The Evolution of Coeval Stellar Hierarchical Triple Systems.pdf:pdf},
school = {Utrecht University},
title = {{The Evolution of Coeval Stellar Hierarchical Triple Systems}},
year = {2012}
}
@article{Meulman2003,
author = {Meulman, Jaqueline J.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Meulman - 2003 - Prediction and classification in nonlinear data analysis Something old, something new, something borrowed, something bl.pdf:pdf},
journal = {Psychometrika},
keywords = {additive prediction components,apoe3 data,boost-,boston housing data,categorical data,cervix cancer data,clustering on variable subsets,cosa,data mining,distance based clustering,forward stagewise additive modeling,genomics,ing,monotonic regression,multiple regression,optimal scaling,optimal scoring,ordinal data,proteomics,regres-,sion splines,statistical learning,sys-,tems biology},
number = {December},
pages = {493--517},
title = {{Prediction and classification in nonlinear data analysis: Something old, something new, something borrowed, something blue}},
url = {http://www.springerlink.com/index/f475j21236661230.pdf},
year = {2003}
}
@inproceedings{Cozman2002,
author = {Cozman, FG and Cohen, Ira},
booktitle = {FLAIRS Conference},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cozman, Cohen - 2002 - Unlabeled Data Can Degrade Classification Performance of Generative Classifiers.pdf:pdf},
pages = {327--331},
title = {{Unlabeled Data Can Degrade Classification Performance of Generative Classifiers.}},
url = {http://www.aaai.org/Papers/FLAIRS/2002/FLAIRS02-065.pdf},
year = {2002}
}
@article{Sivaganesan2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1504.02689v1},
author = {Sivaganesan, Siva},
doi = {10.1214/14-BA935},
eprint = {arXiv:1504.02689v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sivaganesan - 2015 - Comment on Article by Berger, Bernardo, and Sun.pdf:pdf},
issn = {1936-0975},
journal = {Bayesian Analysis},
keywords = {Joint Reference Prior, Logarithmic Divergence, Mul,joint reference prior,logarithmic divergence,multinomial model,objective priors,reference analysis},
number = {1},
pages = {189--221},
title = {{Comment on Article by Berger, Bernardo, and Sun}},
url = {http://projecteuclid.org/euclid.ba/1422556417},
year = {2015}
}
@article{Do2008,
abstract = {Up to this point in class, you have seen multivariate Gaussians arise in a number of appli- cations, such as the probabilistic interpretation of linear regression, Gaussian discriminant analysis, mixture of Gaussians clustering, and most recently, factor analysis. In these lec- ture notes, we attempt to demystify some of the fancier properties of multivariate Gaussians that were introduced in the recent factor analysis lecture. The goal of these notes is to give you some intuition into where these properties come from, so that you can use them with confidence on your homework (hint hint!) and beyond},
author = {Do, Chuong B},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Do - 2008 - More on Multivariate Gaussians.pdf:pdf},
pages = {1--11},
title = {{More on Multivariate Gaussians}},
year = {2008}
}
@inproceedings{Giffin2007,
archivePrefix = {arXiv},
arxivId = {arXiv:0708.1593v2},
author = {Giffin, Adom and Caticha, Ariel},
booktitle = {27th International Workshop on Bayesian Inference and Maximum Entropy Methods in Science and Engineering},
eprint = {arXiv:0708.1593v2},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Giffin, Caticha - 2007 - Updating Probabilities with Data and Moments.pdf:pdf},
keywords = {bayes theorem,expectation value,moment,relative entropy},
pages = {74--84},
title = {{Updating Probabilities with Data and Moments}},
url = {http://arxiv.org/abs/0708.1593},
year = {2007}
}
@article{Cook2012,
author = {Cook, Dianne and Wickham, Hadley},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cook, Wickham - 2012 - tourrGui A gWidgets GUI for the Tour to Explore.pdf:pdf},
keywords = {dynamic graphics,exploratory data analysis,interactive graphics,mining,multivariate data visualization,visual data},
number = {6},
title = {{tourrGui : A gWidgets GUI for the Tour to Explore}},
volume = {49},
year = {2012}
}
@article{Kucukelbir2014,
abstract = {Predictive inference uses a model to analyze a dataset and make predictions about new observations. When a model does not match the data, predictive accuracy suffers. To mitigate this effect, we develop the profile predictive, a predictive density that incorporates the population distribution of data into Bayesian inference. This leads to a practical method for reducing the effect of model mismatch. We extend this method into variational inference and propose a stochastic optimization algorithm, called bumping variational inference. We demonstrate improved predictive accuracy over classical variational inference in two models: a Bayesian mixture model of image histograms and a latent Dirichlet allocation topic model of a text corpus.},
archivePrefix = {arXiv},
arxivId = {1411.0292},
author = {Kucukelbir, Alp and Blei, David M.},
eprint = {1411.0292},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kucukelbir, Blei - 2014 - Profile Predictive Inference.pdf:pdf},
month = nov,
pages = {8},
title = {{Profile Predictive Inference}},
url = {http://arxiv.org/abs/1411.0292},
year = {2014}
}
@article{Nemenman2001,
author = {Nemenman, Ilya and Shafee, Fariel and Bialek, William},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Nemenman, Shafee, Bialek - 2001 - Entropy and Inference, Revisited.pdf:pdf},
journal = {arXiv preprint},
title = {{Entropy and Inference, Revisited}},
url = {http://arxiv.org/abs/physics/0108025},
year = {2001}
}
@inproceedings{Bottou2010,
author = {Bottou, Leon},
booktitle = {Proceedings of COMPSTAT'2010},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bottou - 2010 - Large-scale machine learning with stochastic gradient descent.pdf:pdf},
keywords = {efficiency,online learning,stochastic gradient descent},
pages = {177--186},
publisher = {Springer},
title = {{Large-scale machine learning with stochastic gradient descent}},
year = {2010}
}
@incollection{Opper1996,
address = {New York},
author = {Opper, Manfred and Kinzel, Wolfgang},
booktitle = {Models of Neural Networks III},
doi = {10.1007/978-1-4612-0723-8\_5},
editor = {Domany, Eytan and Hemmen, J. Leo and Schulten, Klaus},
isbn = {978-1-4612-6882-6},
pages = {151--209},
publisher = {Springer},
title = {{Statistical Mechanics of Generalization}},
year = {1996}
}
@article{Kanamori2012,
author = {Kanamori, Takafumi and Takeda, A and Suzuki, T},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kanamori, Takeda, Suzuki - 2012 - A Conjugate Property between Loss Functions and Uncertainty Sets in Classification Problems.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {consistency,convex conjugate,loss function,uncertainty set},
pages = {1461--1504},
title = {{A Conjugate Property between Loss Functions and Uncertainty Sets in Classification Problems}},
url = {http://arxiv.org/abs/1204.6583},
volume = {14},
year = {2012}
}
@misc{Shalev-Shwartz2014,
author = {Shalev-Shwartz, Shai and Ben-David, Shai},
publisher = {Cambridge University Press},
title = {{Understanding Machine Learning}},
year = {2014}
}
@article{Gelman2013c,
author = {Gelman, Andrew and Betancourt, Michael},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gelman, Betancourt - 2013 - Does quantum uncertainty have a place in everyday applied statistics.pdf:pdf},
journal = {The Behavioral and brain sciences},
number = {August},
title = {{Does quantum uncertainty have a place in everyday applied statistics?}},
url = {http://www.stat.columbia.edu/~gelman/research/published/quantum.pdf},
year = {2013}
}
@article{Han2013,
author = {Han, Fang and Zhao, Tuo and Liu, Han},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Han, Zhao, Liu - 2013 - CODA High Dimensional Copula Discriminant Analysis.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {gaussian copula,high dimensional statistics,nonparanormal distribution,rank-based statistics,sparse nonlinear discriminant analysis},
pages = {629--671},
title = {{CODA: High Dimensional Copula Discriminant Analysis}},
volume = {14},
year = {2013}
}
@article{Michie1994,
author = {Michie, D and Spiegelhalter, D J and Taylor, C C},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Michie, Spiegelhalter, Taylor - 1994 - Statlog.pdf:pdf},
title = {{Statlog}},
year = {1994}
}
@article{Goldberg2009,
author = {Goldberg, Andrew B. and Zhu, Xiaojin},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Goldberg, Zhu - 2009 - Keepin'it real semi-supervised learning with realistic tuning.pdf:pdf},
journal = {NAACL HLT 2009 Workshop on Semi-supervised Learning for Natural Language Processing},
title = {{Keepin'it real: semi-supervised learning with realistic tuning}},
year = {2009}
}
@article{Soleymani2014,
author = {Soleymani, M. and Lee, S. M. S.},
doi = {10.1093/biomet/ast068},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Soleymani, Lee - 2014 - Sequential combination of weighted and nonparametric bagging for classification.pdf:pdf},
issn = {0006-3444},
journal = {Biometrika},
month = feb,
number = {2},
pages = {491--498},
title = {{Sequential combination of weighted and nonparametric bagging for classification}},
url = {http://biomet.oxfordjournals.org/cgi/doi/10.1093/biomet/ast068},
volume = {101},
year = {2014}
}
@article{Weston2005,
abstract = {MOTIVATION: Building an accurate protein classification system depends critically upon choosing a good representation of the input sequences of amino acids. Recent work using string kernels for protein data has achieved state-of-the-art classification performance. However, such representations are based only on labeled data--examples with known 3D structures, organized into structural classes--whereas in practice, unlabeled data are far more plentiful. RESULTS: In this work, we develop simple and scalable cluster kernel techniques for incorporating unlabeled data into the representation of protein sequences. We show that our methods greatly improve the classification performance of string kernels and outperform standard approaches for using unlabeled data, such as adding close homologs of the positive examples to the training data. We achieve equal or superior performance to previously presented cluster kernel methods and at the same time achieving far greater computational efficiency. AVAILABILITY: Source code is available at www.kyb.tuebingen.mpg.de/bs/people/weston/semiprot. The Spider matlab package is available at www.kyb.tuebingen.mpg.de/bs/people/spider. SUPPLEMENTARY INFORMATION: www.kyb.tuebingen.mpg.de/bs/people/weston/semiprot.},
author = {Weston, Jason and Leslie, Christina and Ie, Eugene and Zhou, Dengyong and Elisseeff, Andre and Noble, William Stafford},
doi = {10.1093/bioinformatics/bti497},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Weston et al. - 2005 - Semi-supervised protein classification using cluster kernels.pdf:pdf},
issn = {1367-4803},
journal = {Bioinformatics},
keywords = {Algorithms,Artificial Intelligence,Automated,Automated: methods,Cluster Analysis,Pattern Recognition,Protein,Protein: methods,Proteins,Proteins: analysis,Proteins: chemistry,Proteins: classification,Sequence Alignment,Sequence Alignment: methods,Sequence Analysis,Software},
month = aug,
number = {15},
pages = {3241--7},
pmid = {15905279},
title = {{Semi-supervised protein classification using cluster kernels.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15905279},
volume = {21},
year = {2005}
}
@article{Betancourt2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1502.01510v1},
author = {Betancourt, Michael},
eprint = {arXiv:1502.01510v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Betancourt - 2015 - The Fundamental Incompatibility of Hamiltonian Monte Carlo and Data Subsampling.pdf:pdf},
journal = {Arxiv preprint arXiv:1502.01510v1},
title = {{The Fundamental Incompatibility of Hamiltonian Monte Carlo and Data Subsampling}},
year = {2015}
}
@article{Tuia,
archivePrefix = {arXiv},
arxivId = {arXiv:1504.02338v1},
author = {Tuia, Devis and Member, Senior and Camps-valls, Gustau and Member, Senior},
eprint = {arXiv:1504.02338v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Tuia et al. - Unknown - Kernel Manifold Alignment.pdf:pdf},
pages = {1--6},
title = {{Kernel Manifold Alignment}}
}
@article{Huang2009,
abstract = {This letter discusses the robustness issue of kernel principal component analysis. A class of new robust procedures is proposed based on eigenvalue decomposition of weighted covariance. The proposed procedures will place less weight on deviant patterns and thus be more resistant to data contamination and model deviation. Theoretical influence functions are derived, and numerical examples are presented as well. Both theoretical and numerical results indicate that the proposed robust method outperforms the conventional approach in the sense of being less sensitive to outliers. Our robust method and results also apply to functional principal component analysis.},
author = {Huang, Su-Yun and Yeh, Yi-Ren and Eguchi, Shinto},
doi = {10.1162/neco.2009.02-08-706},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Huang, Yeh, Eguchi - 2009 - Robust kernel principal component analysis.pdf:pdf},
issn = {0899-7667},
journal = {Neural computation},
keywords = {Algorithms,Humans,Linear Models,Models, Statistical,Principal Component Analysis,Principal Component Analysis: methods,Reproducibility of Results},
month = nov,
number = {11},
pages = {3179--213},
pmid = {19686071},
title = {{Robust kernel principal component analysis.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22481823},
volume = {21},
year = {2009}
}
@article{Tanaka2014a,
archivePrefix = {arXiv},
arxivId = {1410.3639},
author = {Tanaka, Fuyuhiko},
eprint = {1410.3639},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Tanaka - 2014 - Quantum Minimax Theorem.pdf:pdf},
month = oct,
pages = {1--30},
title = {{Quantum Minimax Theorem}},
url = {http://arxiv.org/abs/1410.3639v1},
year = {2014}
}
@article{Minka2005,
abstract = {This paper presents a unifying view of message-passing algorithms, as methods to approximate a complex Bayesian network by a simpler network with minimum information divergence. In this view, the difference between mean-field methods and belief propagation is not the amount of structure they model, but only the measure of loss they minimize (`exclusive' versus `inclusive' Kullback-Leibler divergence). In each case, message-passing arises by minimizing a localized version of the divergence, local to each factor. By examining these divergence measures, we can intuit the types of solution they prefer (symmetry-breaking, for example) and their suitability for different tasks. Furthermore, by considering a wider variety of divergence measures (such as alpha-divergences), we can achieve different complexity and performance goals.},
author = {Minka, Thomas},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Minka - 2005 - Divergence measures and message passing.pdf:pdf},
pages = {MSR--TR--2005--173},
title = {{Divergence measures and message passing}},
year = {2005}
}
@article{Hyvarinen2000,
abstract = {A fundamental problem in neural network research, as well as in many other disciplines, is finding a suitable representation of multivariate data, i.e. random vectors. For reasons of computational and conceptual simplicity, the representation is often sought as a linear transformation of the original data. In other words, each component of the representation is a linear combination of the original variables. Well-known linear transformation methods include principal component analysis, factor analysis, and projection pursuit. Independent component analysis (ICA) is a recently developed method in which the goal is to find a linear representation of non-Gaussian data so that the components are statistically independent, or as independent as possible. Such a representation seems to capture the essential structure of the data in many applications, including feature extraction and signal separation. In this paper, we present the basic theory and applications of ICA, and our recent work on the subject.},
author = {Hyv\"{a}rinen, Aapo and Oja, Erkki},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hyv\"{a}rinen, Oja - 2000 - Independent component analysis algorithms and applications.pdf:pdf},
issn = {0893-6080},
journal = {Neural networks : the official journal of the International Neural Network Society},
keywords = {Algorithms,Artifacts,Brain,Brain: physiology,Humans,Magnetoencephalography,Neural Networks (Computer),Normal Distribution},
number = {4-5},
pages = {411--30},
pmid = {10946390},
title = {{Independent component analysis: algorithms and applications.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/10946390},
volume = {13},
year = {2000}
}
@inproceedings{Golovin2013,
author = {Golovin, Daniel and Sculley, D},
booktitle = {International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Golovin, Sculley - 2013 - Large-Scale Learning with Less RAM via Randomization.pdf:pdf},
pages = {325--333},
title = {{Large-Scale Learning with Less RAM via Randomization}},
url = {http://arxiv.org/abs/1303.4664},
year = {2013}
}
@article{Dagenais1971,
abstract = {The purpose of this article is to suggest a method of estimating parameters of linear regressions containing two independent variables, when data is missing among these variables. The problem envisaged concerns the case where: (1) the independent variables are considered as fixed numbers; (2) each observation contains the values of the dependent variable and at least one of the independent variables; (3) some observations are complete. In contrast with other approaches dealing with similar problems, the technique developed in this article has the following advantages: (1) it is based on rather unrestrictive hypotheses; (2) the resulting estimators are consistent; (3) the asymptotic variances of these estimators are smaller than those of comparable estimators described in the literature. Although the question is not examined in the present article, it seems also that the proposed method offers good possibilities of generalization.},
author = {Dagenais, Marcel G},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Dagenais - 1971 - Further suggestions concerning the utilization of incomplete observations in regression analysis.pdf:pdf},
journal = {Journal of the American Statistical Association},
keywords = {Missing data,Model specification},
number = {333},
pages = {93--98},
title = {{Further suggestions concerning the utilization of incomplete observations in regression analysis }},
volume = {66},
year = {1971}
}
@article{Pribram1978a,
author = {Pribram, Karl H.},
doi = {10.1017/S0140525X00060003},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Pribram - 1978 - On behalf of the neurosciences(2).pdf:pdf},
isbn = {0300104251},
issn = {0140-525X},
journal = {Behavioral and Brain Sciences},
number = {01},
pages = {113},
title = {{On behalf of the neurosciences}},
volume = {1},
year = {1978}
}
@article{Despres2014,
abstract = {The Vapnik-Chervonenkis dimension of a collection of subsets of a set is an important combinatorial parameter in machine learning. In this paper we show that the VC dimension of the family of d-dimensional cubes in \$\backslash mathbb\{R\}\^{}d\$ (that is, the closed balls according to the \$\backslash ell\^{}\backslash infty\$ norm) is \$\backslash lfloor (3d+1)/2 \backslash rfloor\$. We also prove that the VC dimension of certain families of convex sets in \$\backslash mathbb\{R\}\^{}2\$ (including the balls of all norms) is at most 3, and that there is a norm in \$\backslash mathbb\{R\}\^{}3\$ the collection of whose balls has infinite VC dimension.},
archivePrefix = {arXiv},
arxivId = {1412.6612},
author = {Despres, Christian J. J.},
eprint = {1412.6612},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Despres - 2014 - The Vapnik-Chervonenkis Dimension of Norms on \$mathbb\{R\}d\$.pdf:pdf},
month = dec,
pages = {20},
title = {{The Vapnik-Chervonenkis Dimension of Norms on \$\backslash mathbb\{R\}\^{}d\$}},
url = {http://arxiv.org/abs/1412.6612},
year = {2014}
}
@article{Moore2001,
author = {Moore, Andrew W},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Moore - 2001 - VC-dimension for characterizing classifiers.pdf:pdf},
pages = {1--20},
title = {{VC-dimension for characterizing classifiers}},
year = {2001}
}
@article{Fraley2002,
author = {Fraley, Chris and Raftery, Adrian. E.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Fraley, Raftery - 2002 - Model-based clustering, discriminant analysis, and density estimation.pdf:pdf},
journal = {Journal of the American Statistical Association},
number = {458},
title = {{Model-based clustering, discriminant analysis, and density estimation}},
url = {http://www.tandfonline.com/doi/abs/10.1198/016214502760047131},
volume = {97},
year = {2002}
}
@article{Erren2007,
author = {Erren, Thomas C and Bourne, Philip E},
doi = {10.1371/journal.pcbi.0030102},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Erren, Bourne - 2007 - Ten simple rules for a good poster presentation.pdf:pdf},
issn = {1553-7358},
journal = {PLoS computational biology},
keywords = {Algorithms,Audiovisual Aids,Biomedical Research,Communication,Congresses as Topic,Exhibits as Topic,Information Dissemination,Information Dissemination: methods,Professional Competence},
month = may,
number = {5},
pages = {e102},
pmid = {17530921},
title = {{Ten simple rules for a good poster presentation.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=1876493\&tool=pmcentrez\&rendertype=abstract},
volume = {3},
year = {2007}
}
@inproceedings{Pranckeviciene2006,
author = {Pranckeviciene, Erinija and Ho, Tin Kam and Somorjai, Ray},
booktitle = {International Conference on Pattern Recognition},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Pranckeviciene, Ho, Somorjai - 2006 - Class separability in spaces reduced by feature selection.pdf:pdf},
title = {{Class separability in spaces reduced by feature selection}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1699514},
year = {2006}
}
@article{Taylor1977,
author = {Taylor, Publisher and Mclachlan, Geoffrey John},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Taylor, Mclachlan - 1977 - Estimating the Linear Discriminant Function from Initial Samples Containing a Small Number of Unclassified Ob.pdf:pdf},
journal = {Journal of the American Statistical Association},
keywords = {completely classified,conditional and expected error,initial samples in-,m j f j,mi,n,rates,sample discriminant functions},
number = {358},
pages = {403--406},
title = {{Estimating the Linear Discriminant Function from Initial Samples Containing a Small Number of Unclassified Observations}},
volume = {72},
year = {1977}
}
@article{Arnold2000,
author = {Arnold, Bernard F. and Stahlecker, Peter},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Arnold, Stahlecker - 2000 - The minimax adjustment principle.pdf:pdf},
journal = {Mathematical methods of operations research},
keywords = {ellipsoidal information,minimax,minimax adjustment principle,principle,projection estimator,supply policy},
pages = {103--113},
title = {{The minimax adjustment principle}},
url = {http://link.springer.com/article/10.1007/s001860050005},
volume = {51},
year = {2000}
}
@inproceedings{Chaubey2003,
author = {Chaubey, Yogendra P. and Nebebe, Fassil and Sen, Debaraj},
booktitle = {Joint Statistical Meetings},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Chaubey, Nebebe, Sen - 2003 - Estimation of Joint Distribution from Marginal Distributions.pdf:pdf},
keywords = {bayesian prediction,because they require multidimensional,ble,contingency ta- methods,dirichlet prior,however,is preferred as it,merical integration,nu-,of the,readily presents an estimate,the bayesian method},
pages = {883--889},
title = {{Estimation of Joint Distribution from Marginal Distributions}},
url = {http://www.amstat.org/sections/SRMS/Proceedings/y2003/Files/JSM2003-000794.pdf},
year = {2003}
}
@article{Hanczar2010,
author = {Hanczar, Blaise and Dougherty, Edward R.},
doi = {10.2174/157489310790596376},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hanczar, Dougherty - 2010 - On the Comparison of Classifiers for Microarray Data.pdf:pdf},
issn = {15748936},
journal = {Current Bioinformatics},
keywords = {classifier comparison,error estimation,microarray classification,variance study},
month = mar,
number = {1},
pages = {29--39},
title = {{On the Comparison of Classifiers for Microarray Data}},
url = {http://openurl.ingenta.com/content/xref?genre=article\&issn=1574-8936\&volume=5\&issue=1\&spage=29},
volume = {5},
year = {2010}
}
@article{Pearl2009,
author = {Pearl, Judea},
doi = {10.1214/09-SS057},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Pearl - 2009 - Causal inference in statistics An overview.pdf:pdf},
issn = {1935-7516},
journal = {Statistics Surveys},
keywords = {Structural equation models, confounding, graphical,and phrases,causal effects,causes of effects,confounding,counterfactuals,graph-,ical methods,mediation,policy evaluation,potential-outcome,received september 2009,structural equation models},
number = {September},
pages = {96--146},
title = {{Causal inference in statistics: An overview}},
url = {http://projecteuclid.org/euclid.ssu/1255440554},
volume = {3},
year = {2009}
}
@techreport{Zhu2005,
author = {Zhu, Xiaojin},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhu - 2005 - Semi-supervised learning literature survey.pdf:pdf},
institution = {University of Wisconsin - Madison},
pages = {1--59},
title = {{Semi-supervised learning literature survey}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.99.9681\&rep=rep1\&type=pdf},
year = {2005}
}
@article{Kuncheva2010,
abstract = {Classification of brain images obtained through functional magnetic resonance imaging (fMRI) poses a serious challenge to pattern recognition and machine learning due to the extremely large feature-to-instance ratio. This calls for revision and adaptation of the current state-of-the-art classification methods. We investigate the suitability of the random subspace (RS) ensemble method for fMRI classification. RS samples from the original feature set and builds one (base) classifier on each subset. The ensemble assigns a class label by either majority voting or averaging of output probabilities. Looking for guidelines for setting the two parameters of the method-ensemble size and feature sample size-we introduce three criteria calculated through these parameters: usability of the selected feature sets, coverage of the set of "important" features, and feature set diversity. Optimized together, these criteria work toward producing accurate and diverse individual classifiers. RS was tested on three fMRI datasets from single-subject experiments: the Haxby data (Haxby, 2001.) and two datasets collected in-house. We found that RS with support vector machines (SVM) as the base classifier outperformed single classifiers as well as some of the most widely used classifier ensembles such as bagging, AdaBoost, random forest, and rotation forest. The closest rivals were the single SVM and bagging of SVM classifiers. We use kappa-error diagrams to understand the success of RS.},
author = {Kuncheva, Ludmila I and Rodriguez, Juan J and Plumpton, Catrin O and Linden, David E J and Johnston, Stephen J},
doi = {10.1109/TMI.2009.2037756},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kuncheva et al. - 2010 - Random subspace ensembles for FMRI classification.pdf:pdf},
issn = {1558-254X},
journal = {IEEE transactions on medical imaging},
keywords = {Adult,Algorithms,Brain,Brain: physiology,Computer Simulation,Humans,Magnetic Resonance Imaging,Magnetic Resonance Imaging: methods,Male,Multivariate Analysis,Pattern Recognition, Automated,Pattern Recognition, Automated: methods,Reproducibility of Results},
month = feb,
number = {2},
pages = {531--42},
pmid = {20129853},
title = {{Random subspace ensembles for FMRI classification.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20129853},
volume = {29},
year = {2010}
}
@inproceedings{Niu2013,
author = {Niu, G and Jitkrittum, W and Dai, Bo and Hachiya, H and Sugiyama, M},
booktitle = {Proceedings of the 30th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Niu et al. - 2013 - Squared-loss Mutual Information Regularization A Novel Information-theoretic Approach to Semi-supervised Learning.pdf:pdf},
pages = {10--18},
title = {{Squared-loss Mutual Information Regularization: A Novel Information-theoretic Approach to Semi-supervised Learning}},
url = {http://sugiyama-www.cs.titech.ac.jp/~gang/paper/niu\_icml13.pdf},
year = {2013}
}
@article{Susko2013,
author = {Susko, E.},
doi = {10.1093/biomet/ast032},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Susko - 2013 - Likelihood ratio tests with boundary constraints using data-dependent degrees of freedom.pdf:pdf},
issn = {0006-3444},
journal = {Biometrika},
month = aug,
number = {4},
pages = {1019--1023},
title = {{Likelihood ratio tests with boundary constraints using data-dependent degrees of freedom}},
url = {http://biomet.oxfordjournals.org/cgi/doi/10.1093/biomet/ast032},
volume = {100},
year = {2013}
}
@article{Leistner2009,
author = {Leistner, Christian and Saffari, Amir and Santner, Jakob and Bischof, Horst},
doi = {10.1109/ICCV.2009.5459198},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Leistner et al. - 2009 - Semi-Supervised Random Forests.pdf:pdf},
isbn = {978-1-4244-4420-5},
journal = {2009 IEEE 12th International Conference on Computer Vision},
month = sep,
pages = {506--513},
publisher = {Ieee},
title = {{Semi-Supervised Random Forests}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5459198},
year = {2009}
}
@article{Ireland1968,
author = {Ireland, C.T. and Kullback, S.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ireland, Kullback - 1968 - Minimum Discrimination Information Estimation.pdf:pdf},
journal = {Biometrics},
number = {3},
pages = {707--713},
title = {{Minimum Discrimination Information Estimation}},
url = {http://www.jstor.org/stable/10.2307/2528330},
volume = {24},
year = {1968}
}
@article{Jaffe2013,
abstract = {BACKGROUND: Significance analysis plays a major role in identifying and ranking genes, transcription factor binding sites, DNA methylation regions, and other high-throughput features associated with illness. We propose a new approach, called gene set bagging, for measuring the probability that a gene set replicates in future studies. Gene set bagging involves resampling the original high-throughput data, performing gene-set analysis on the resampled data, and confirming that biological categories replicate in the bagged samples.

RESULTS: Using both simulated and publicly-available genomics data, we demonstrate that significant categories in a gene set enrichment analysis may be unstable when subjected to resampling. We show our method estimates the replication probability (R), the probability that a gene set will replicate as a significant result in future studies, and show in simulations that this method reflects replication better than each set's p-value.

CONCLUSIONS: Our results suggest that gene lists based on p-values are not necessarily stable, and therefore additional steps like gene set bagging may improve biological inference on gene sets.},
author = {Jaffe, Andrew E and Storey, John D and Ji, Hongkai and Leek, Jeffrey T},
doi = {10.1186/1471-2105-14-360},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Jaffe et al. - 2013 - Gene set bagging for estimating the probability a statistically significant result will replicate.pdf:pdf},
issn = {1471-2105},
journal = {BMC bioinformatics},
keywords = {dna methylation,gene expression,gene ontology,gene set enrichment analysis},
month = jan,
pages = {360},
pmid = {24330332},
title = {{Gene set bagging for estimating the probability a statistically significant result will replicate.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3890500\&tool=pmcentrez\&rendertype=abstract},
volume = {14},
year = {2013}
}
@inproceedings{Tax2005,
author = {Tax, David M.J. and Duin, Robert P.W.},
booktitle = {Proceedings of the Sixteenth Annual Symposium of the Pattern Recognition Association of South Africa},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Tax, Duin - 2005 - Characterizing one-class datasets.pdf:pdf},
number = {4},
pages = {21--26},
title = {{Characterizing one-class datasets}},
url = {http://mediamatica.ewi.tudelft.nl/sites/default/files/TaxDui2005.pdf},
volume = {1},
year = {2005}
}
@article{King1995,
author = {King, R.D. and Feng, C and Sutherland, A},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/King, Feng, Sutherland - 1995 - Statlog comparison of classification algorithms on large real-world problems.pdf:pdf},
journal = {Applied Artificial Intelligence an International Journal},
number = {3},
pages = {289--333},
title = {{Statlog: comparison of classification algorithms on large real-world problems}},
url = {http://www.tandfonline.com/doi/abs/10.1080/08839519508945477},
volume = {9},
year = {1995}
}
@article{Herrero-lopez,
author = {Herrero-lopez, Sergio and Williams, John R and Sanchez, Abel},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Herrero-lopez, Williams, Sanchez - Unknown - Parallel Multiclass Classification using SVMs on GPUs.pdf:pdf},
isbn = {9781605589350},
journal = {Memory},
keywords = {gpu,support vector machine},
pages = {2--11},
title = {{Parallel Multiclass Classification using SVMs on GPUs}}
}
@inproceedings{Maddison2014,
author = {Maddison, Chris J and Tarlow, Daniel and Minka, Tom},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Maddison, Tarlow, Minka - 2014 - A Sampling.pdf:pdf},
title = {{A* Sampling}},
year = {2014}
}
@article{Shafer2008,
author = {Shafer, Glenn and Vovk, Vladimir},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Shafer, Vovk - 2008 - A tutorial on conformal prediction.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {confidence,on-line compression modeling,on-line learning,prediction regions},
pages = {371--421},
title = {{A tutorial on conformal prediction}},
url = {http://dl.acm.org/citation.cfm?id=1390693},
volume = {9},
year = {2008}
}
@inproceedings{Giraud-Carrier2005,
author = {Giraud-carrier, Christophe and Provost, Foster},
booktitle = {In Proceedings of the ICML-2005 Workshop on Meta-learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Giraud-carrier, Provost - 2005 - Toward a justification of meta-learning Is the no free lunch theorem a show-stopper.pdf:pdf},
pages = {12--19},
title = {{Toward a justification of meta-learning: Is the no free lunch theorem a show-stopper}},
url = {http://dml.cs.byu.edu/~cgc/pubs/ICML2005WS.pdf},
year = {2005}
}
@article{Efron2014,
author = {Efron, Bradley},
doi = {10.1214/13-STS455},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Efron - 2014 - Two Modeling Strategies for Empirical Bayes Estimation.pdf:pdf},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {and phrases,bayes rule in terms,f -modeling,f-modeling, g-modeling, Bayes rule in terms of f,,g -modeling,of f,prior exponential families},
month = may,
number = {2},
pages = {285--301},
title = {{Two Modeling Strategies for Empirical Bayes Estimation}},
url = {http://projecteuclid.org/euclid.ss/1408368582},
volume = {29},
year = {2014}
}
@article{Shannon1948,
author = {Shannon, Claude Elwood},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Shannon - 1948 - A mathematical theory of communication.pdf:pdf},
journal = {The Bell System Technical Journal},
number = {J},
pages = {379--423},
title = {{A mathematical theory of communication}},
url = {http://dl.acm.org/citation.cfm?id=584093},
volume = {27},
year = {1948}
}
@article{Devroye1982,
abstract = {Consider the basic discrimination problem based on a sample of size n drawn from the distribution of (X, Y) on the Borel sets of Rdx \{O, 1\}. If 0 < R*< is a given number, and 'n - 0 is an arbitrary positive sequence, then for any discrimination rule one can find a distribution for (X, Y), not depending upon n, with Bayes probability of error R* such that the probability of error (Rn) of the discrimination rule is larger than R* + 'On for infinitely many n. We give a formal proof of this result, which is a generalization of a result by Cover [1].},
author = {Devroye, L},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Devroye - 1982 - Any discrimination rule can have an arbitrarily bad probability of error for finite sample size.pdf:pdf},
issn = {0162-8828},
journal = {IEEE transactions on pattern analysis and machine intelligence},
month = feb,
number = {2},
pages = {154--7},
pmid = {21869021},
title = {{Any discrimination rule can have an arbitrarily bad probability of error for finite sample size.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21869021},
volume = {4},
year = {1982}
}
@article{Grasse1997,
author = {Bar-On, J.R. and Grasse, K. A.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bar-On, Grasse - 1997 - Global Optimization of a Quadratic Functional with Quadratic Equality Constraints , Part 2 1.pdf:pdf},
journal = {Journal of Optimization Theory and Applications},
keywords = {global optimization,quadratic equality constraints,quadratic functionals},
number = {3},
pages = {547--556},
title = {{Global Optimization of a Quadratic Functional with Quadratic Equality Constraints , Part 2 1}},
volume = {93},
year = {1997}
}
@article{Srivastava2007,
author = {Srivastava, Santosh},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Srivastava - 2007 - Bayesian Quadratic Discriminant Analysis.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {bregman,data-dependent prior,divergence,eigenvalue decomposition,functional analysis,quadratic discriminant analysis,regularized quadratic discriminant analysis,wishart},
pages = {1277--1305},
title = {{Bayesian Quadratic Discriminant Analysis.}},
url = {http://people.ee.duke.edu/~lcarin/SrivastavaGuptaFrigyikBDA.pdf},
volume = {8},
year = {2007}
}
@article{Nguyen2010,
author = {Nguyen, XuanLong and Wainwright, Martin J. and Jordan, Michael I.},
doi = {10.1109/TIT.2010.2068870},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Nguyen, Wainwright, Jordan - 2010 - Estimating Divergence Functionals and the Likelihood Ratio by Convex Risk Minimization(2).pdf:pdf},
issn = {0018-9448},
journal = {IEEE Transactions on Information Theory},
month = nov,
number = {11},
pages = {5847--5861},
title = {{Estimating Divergence Functionals and the Likelihood Ratio by Convex Risk Minimization}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5605355},
volume = {56},
year = {2010}
}
@inproceedings{Balcan2006,
author = {Balcan, Maria-Florina and Beygelzimer, Alina and Langford, John},
booktitle = {Proceedings of the 23rd International Conference on Machine learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Balcan, Beygelzimer, Langford - 2006 - Agnostic active learning.pdf:pdf},
keywords = {active learning,agnostic setting,linear,sample complexity},
pages = {65--72},
title = {{Agnostic active learning}},
url = {http://www.sciencedirect.com/science/article/pii/S0022000008000652},
year = {2006}
}
@inproceedings{Shalev-Shwartz2007,
author = {Shalev-Shwartz, Shai and Singer, Yoram},
booktitle = {Proceedings of the 24th International Conference on Machine learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Shalev-Shwartz, Singer - 2007 - Pegasos Primal estimated sub-gradient solver for svm.pdf:pdf},
pages = {807--814},
title = {{Pegasos: Primal estimated sub-gradient solver for svm}},
url = {http://link.springer.com/article/10.1007/s10107-010-0420-4},
year = {2007}
}
@article{Gelman2011,
author = {Gelman, Andrew},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gelman - 2011 - Induction and deduction in Bayesian data analysis.pdf:pdf},
journal = {Rationality, Markets and Morals (RMM)},
pages = {67--78},
title = {{Induction and deduction in Bayesian data analysis}},
url = {http://www.stat.columbia.edu/~gelman/research/unpublished/philosophy\_online4.pdf},
volume = {2},
year = {2011}
}
@misc{Mitchell1980,
author = {Mitchell, Tom M.},
booktitle = {Psychology},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Mitchell - 1980 - The need for biases in learning generalizations.pdf:pdf},
title = {{The need for biases in learning generalizations}},
url = {http://dml.cs.byu.edu/~cgc/docs/mldm\_tools/Reading/Need for Bias.pdf},
year = {1980}
}
@inproceedings{Fujino2005,
author = {Fujino, Akinori and Ueda, Naonori and Saito, Kazumi},
booktitle = {Proceedings of the National Conference on Artificial Intelligence},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Fujino, Ueda, Saito - 2005 - A hybrid generativediscriminative approach to semi-supervised classifier design.pdf:pdf},
number = {2},
pages = {764--769},
title = {{A hybrid generative/discriminative approach to semi-supervised classifier design}},
url = {http://www.aaai.org/Papers/AAAI/2005/AAAI05-120.pdf},
volume = {20},
year = {2005}
}
@article{Anand2013,
abstract = {Mean shift clustering is a powerful nonparametric technique that does not require prior knowledge of the number of clusters and does not constrain the shape of the clusters. However, being completely unsupervised, its performance suffers when the original distance metric fails to capture the underlying cluster structure. Despite recent advances in semi-supervised clustering methods, there has been little effort towards incorporating supervision into mean shift. We propose a semi-supervised framework for kernel mean shift clustering (SKMS) that uses only pairwise constraints to guide the clustering procedure. The points are first mapped to a high-dimensional kernel space where the constraints are imposed by a linear transformation of the mapped points. This is achieved by modifying the initial kernel matrix by minimizing a log det divergence-based objective function.We show the advantages of SKMS by evaluating its performance on various synthetic and real datasets while comparing with state-of-the-art semi-supervised clustering algorithms.},
author = {Anand, Saket and Mittal, Sushil and Tuzel, Oncel and Meer, Peter},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Anand et al. - 2013 - Semi-Supervised Kernel Mean Shift Clustering.pdf:pdf},
issn = {1939-3539},
journal = {IEEE transactions on pattern analysis and machine intelligence},
month = sep,
number = {6},
pages = {1201--1215},
pmid = {24101327},
title = {{Semi-Supervised Kernel Mean Shift Clustering.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24101327},
volume = {36},
year = {2013}
}
@article{Lampert2014,
author = {Lampert, Thomas a. and Gan\c{c}arski, Pierre},
doi = {10.1007/s10994-013-5432-x},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lampert, Gan\c{c}arski - 2014 - The bane of skew.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {aucpr,evaluation,performance,precision,recall,roc,skew},
month = jan,
title = {{The bane of skew}},
url = {http://link.springer.com/10.1007/s10994-013-5432-x},
year = {2014}
}
@unpublished{Mikolov2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1502.05698v1},
author = {Mikolov, Tomas and Com, Tmikolov F B},
eprint = {arXiv:1502.05698v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Mikolov, Com - 2014 - Towards AI-Complete Question Answering A Set of Prerequisite Toy Tasks.pdf:pdf},
title = {{Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks}},
year = {2014}
}
@article{Ma2008,
abstract = {In bioinformatics studies, supervised classification with high-dimensional input variables is frequently encountered. Examples routinely arise in genomic, epigenetic and proteomic studies. Feature selection can be employed along with classifier construction to avoid over-fitting, to generate more reliable classifier and to provide more insights into the underlying causal relationships. In this article, we provide a review of several recently developed penalized feature selection and classification techniques--which belong to the family of embedded feature selection methods--for bioinformatics studies with high-dimensional input. Classification objective functions, penalty functions and computational algorithms are discussed. Our goal is to make interested researchers aware of these feature selection and classification methods that are applicable to high-dimensional bioinformatics data.},
author = {Ma, Shuangge and Huang, Jian},
doi = {10.1093/bib/bbn027},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ma, Huang - 2008 - Penalized feature selection and classification in bioinformatics.pdf:pdf},
isbn = {1477-4054},
issn = {14675463},
journal = {Briefings in Bioinformatics},
keywords = {Bioinformatics application,Feature selection,Penalization},
number = {5},
pages = {392--403},
pmid = {18562478},
title = {{Penalized feature selection and classification in bioinformatics}},
volume = {9},
year = {2008}
}
@article{Bengio2009,
author = {Bengio, Yoshua},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bengio - 2009 - Learning deep architectures for AI.pdf:pdf},
journal = {Foundations and trends® in Machine Learning},
title = {{Learning deep architectures for AI}},
url = {http://dl.acm.org/citation.cfm?id=1658424},
year = {2009}
}
@inproceedings{Kohavi1995,
author = {Kohavi, Ron},
booktitle = {International Joint Conferences on Artificial Intelligence},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kohavi - 1995 - A study of cross-validation and bootstrap for accuracy estimation and model selection.pdf:pdf},
number = {2},
pages = {1137--1145},
title = {{A study of cross-validation and bootstrap for accuracy estimation and model selection}},
url = {http://frostiebek.free.fr/docs/Machine Learning/validation-1.pdf},
volume = {14},
year = {1995}
}
@inproceedings{Joulin2012,
author = {Joulin, Armand and Bach, Francis},
booktitle = {Proceedings of the 29th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Joulin, Bach - 2012 - A convex relaxation for weakly supervised classifiers.pdf:pdf},
keywords = {MIL,convex relaxation,weak supervision},
pages = {1279--1286},
title = {{A convex relaxation for weakly supervised classifiers}},
url = {http://arxiv.org/abs/1206.6413},
year = {2012}
}
@article{Le2014,
abstract = {We develop a class of rules spanning the range between quadratic discriminant analysis and naive Bayes, through a path of sparse graphical models. A group lasso penalty is used to introduce shrinkage and encourage a similar pattern of sparsity across precision matrices. It gives sparse estimates of interactions and produces interpretable models. Inspired by the connected-components structure of the estimated precision matrices, we propose the community Bayes model, which partitions features into several conditional independent communities and splits the classification problem into separate smaller ones. The community Bayes idea is quite general and can be applied to non-gaussian data and likelihood-based classifiers.},
archivePrefix = {arXiv},
arxivId = {1407.4543},
author = {Le, Ya and Hastie, Trevor},
eprint = {1407.4543},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Le, Hastie - 2014 - Sparse Quadratic Discriminant Analysis and Community Bayes.pdf:pdf},
month = jul,
pages = {20},
title = {{Sparse Quadratic Discriminant Analysis and Community Bayes}},
url = {http://arxiv.org/abs/1407.4543},
year = {2014}
}
@article{Guo2010,
author = {Guo, Yuanyuan and Niu, Xiaoda and Zhang, Harry},
doi = {10.1109/ICDM.2010.66},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Guo, Niu, Zhang - 2010 - An Extensive Empirical Study on Semi-supervised Learning.pdf:pdf},
isbn = {978-1-4244-9131-5},
journal = {IEEE International Conference on Data Mining},
keywords = {-semi-supervised learning,bayesian classifiers},
month = dec,
pages = {186--195},
publisher = {Ieee},
title = {{An Extensive Empirical Study on Semi-supervised Learning}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5693972},
year = {2010}
}
@article{Gelman2014a,
abstract = {We revisit expectation propagation (EP) as a prototype for scalable algorithms that partition big datasets into many parts and analyze each part in parallel to perform inference of shared parameters. The algorithm should be particularly efficient for hierarchical models, for which the EP algorithm works on the shared parameters (hyperparameters) of the model.   The central idea of EP is to work at each step with a "tilted distribution" that combines the likelihood for a part of the data with the "cavity distribution," which is the approximate model for the prior and all other parts of the data. EP iteratively approximates the moments of the tilted distributions and incorporates those approximations into a global posterior approximation. As such, EP can be used to divide the computation for large models into manageable sizes. The computation for each partition can be made parallel with occasional exchanging of information between processes through the global posterior approximation. Moments of multivariate tilted distributions can be approximated in various ways, including, MCMC, Laplace approximations, and importance sampling.},
archivePrefix = {arXiv},
arxivId = {1412.4869},
author = {Gelman, Andrew and Vehtari, Aki and Jyl\"{a}nki, Pasi and Robert, Christian and Chopin, Nicolas and Cunningham, John P.},
eprint = {1412.4869},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gelman et al. - 2014 - Expectation propagation as a way of life.pdf:pdf},
keywords = {bayesian computation,big data,data partitioning,expectation propagation,hierarchical models,stan,statistical computing},
pages = {19},
title = {{Expectation propagation as a way of life}},
url = {http://arxiv.org/abs/1412.4869},
year = {2014}
}
@article{Soares2004,
author = {Soares, Carlos and Brazdil, Pavel B. and Kuba, Petr},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Soares, Brazdil, Kuba - 2004 - A Meta-Learning Method to Select the KernelWidth in Support Vector Regression.pdf:pdf},
journal = {Machine learning},
keywords = {gaussian kernel,learning rankings,meta-learning,parameter setting,support vector machines},
pages = {195--209},
title = {{A Meta-Learning Method to Select the KernelWidth in Support Vector Regression}},
url = {http://link.springer.com/article/10.1023/b:mach.0000015879.28004.9b},
volume = {54},
year = {2004}
}
@article{Shervashidze2014,
abstract = {Structured sparsity has recently emerged in statistics, machine learning and signal process- ing as a promising paradigm for learning in high-dimensional settings. A number of methods have been proposed for learning under the assumption of structured sparsity, including group LASSO and graph LASSO. All of these methods rely on prior knowledge on how to weight (equivalently, how to penalize) individual subsets of variables during the subset selection pro- cess. However, these weights on groups of variables are in general unknown. Inferring group weights from data is a key open problem in research on structured sparsity. In this paper, we propose a Bayesian approach to the problem of group weight learning. We model the group weights as hyperparameters of heavy-tailed priors on groups of variables and derive an approximate inference scheme to infer these hyperparameters. We empirically show that we are able to recover the model hyperparameters when the data are generated from the model, and moreover, we demonstrate the utility of learning group weights in synthetic and real denoising problems.},
archivePrefix = {arXiv},
arxivId = {arXiv:1503.03082v1},
author = {Shervashidze, Nino and Bach, Francis},
eprint = {arXiv:1503.03082v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Shervashidze, Bach - 2014 - Learning to Learn for Structured Sparsity.pdf:pdf},
journal = {arXiv},
pages = {1--20},
title = {{Learning to Learn for Structured Sparsity}},
url = {http://hal.archives-ouvertes.fr/hal-00986380/},
year = {2014}
}
@inproceedings{Macia2009,
author = {Macia, Nuria and Orriols-puig, Albert and Bernad\'{o}-Mansilla, Ester},
booktitle = {Hybrid Artificial Intelligence Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Macia, Orriols-puig, Bernad\'{o}-Mansilla - 2009 - Beyond Homemade Artificial Data Sets.pdf:pdf},
keywords = {artificial data sets,data complexity,machine learning},
pages = {605--612},
title = {{Beyond Homemade Artificial Data Sets}},
url = {http://www.springerlink.com/index/N23720WL67U355MV.pdf http://link.springer.com/chapter/10.1007/978-3-642-02319-4\_73},
year = {2009}
}
@article{Gaffke1989,
author = {Gaffke, Norbert and Heiligers, Berthold},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gaffke, Heiligers - 1989 - Bayes, Admissible, and Minimax Linear Estimators in Linear Models with Restricted Parameter Space.pdf:pdf},
journal = {Statistics: A Journal of Theoretical and Applied Statistics},
pages = {487--508},
title = {{Bayes, Admissible, and Minimax Linear Estimators in Linear Models with Restricted Parameter Space}},
volume = {4},
year = {1989}
}
@article{Kingma2014,
abstract = {The ever-increasing size of modern data sets combined with the difficulty of obtaining label information has made semi-supervised learning one of the problems of significant practical importance in modern data analysis. We revisit the approach to semi-supervised learning with generative models and develop new models that allow for effective generalisation from small labelled data sets to large unlabelled ones. Generative approaches have thus far been either inflexible, inefficient or non-scalable. We show that deep generative models and approximate Bayesian inference exploiting recent advances in variational methods can be used to provide significant improvements, making generative approaches highly competitive for semi-supervised learning.},
archivePrefix = {arXiv},
arxivId = {1406.5298},
author = {Kingma, Diederik P. and Rezende, Danilo J. and Mohamed, Shakir and Welling, Max},
eprint = {1406.5298},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kingma et al. - 2014 - Semi-Supervised Learning with Deep Generative Models.pdf:pdf},
month = jun,
pages = {1--9},
title = {{Semi-Supervised Learning with Deep Generative Models}},
url = {http://arxiv.org/abs/1406.5298},
year = {2014}
}
@article{Reif2012,
author = {Reif, Matthias and Shafait, Faisal and Goldstein, Markus and Breuel, Thomas and Dengel, Andreas},
doi = {10.1007/s10044-012-0280-z},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Reif et al. - 2012 - Automatic classifier selection for non-experts.pdf:pdf},
issn = {1433-7541},
journal = {Pattern Analysis and Applications},
keywords = {classifier recommendation,classifier selection,landmarking,meta-features,meta-learning,regression},
month = jul,
title = {{Automatic classifier selection for non-experts}},
url = {http://www.springerlink.com/index/10.1007/s10044-012-0280-z},
year = {2012}
}
@phdthesis{Marlin2008,
author = {Marlin, BM},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Marlin - 2008 - Missing data problems in machine learning.pdf:pdf},
school = {University of Toronto},
title = {{Missing data problems in machine learning}},
url = {http://www-devel.cs.ubc.ca/~bmarlin/research/phd\_thesis/marlin-phd-thesis.pdf},
year = {2008}
}
@article{Nguyen2009,
author = {Nguyen, XuanLong and Wainwright, Martin J. and Jordan, Michael I.},
doi = {10.1214/08-AOS595},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Nguyen, Wainwright, Jordan - 2009 - On surrogate loss functions and f -divergences.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
month = apr,
number = {2},
pages = {876--904},
title = {{On surrogate loss functions and f -divergences}},
url = {http://projecteuclid.org/euclid.aos/1236693153},
volume = {37},
year = {2009}
}
@book{Berger1985,
author = {Berger, James O},
publisher = {Springer},
title = {{Statistical decision theory and Bayesian analysis}},
year = {1985}
}
@article{Rendell1990,
author = {Rendell, Larry and Cho, Howard},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Rendell, Cho - 1990 - Empirical learning as a function of concept character.pdf:pdf},
journal = {Machine Learning},
keywords = {concepts as functions,empirical concept learning,experimental studies},
pages = {267--298},
title = {{Empirical learning as a function of concept character}},
url = {http://www.springerlink.com/index/K5311727465WLH07.pdf},
volume = {5},
year = {1990}
}
@article{Lv2014,
author = {Lv, Jinchi and Zheng, Zemin},
doi = {10.1214/13-AOS1175},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lv, Zheng - 2014 - Discussion “a significance test for the lasso”.pdf:pdf},
journal = {The Annals of Statistics},
number = {2},
pages = {493--500},
title = {{Discussion: “a significance test for the lasso”}},
volume = {42},
year = {2014}
}
@article{Janzing2013,
author = {Janzing, Dominik and Balduzzi, David and Grosse-Wentrup, Moritz and Sch\"{o}lkopf, Bernhard},
doi = {10.1214/13-AOS1145},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Janzing et al. - 2013 - Quantifying causal influences.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
month = oct,
number = {5},
pages = {2324--2358},
title = {{Quantifying causal influences}},
url = {http://projecteuclid.org/euclid.aos/1383661266},
volume = {41},
year = {2013}
}
@inproceedings{Szummer2001,
author = {Szummer, Martin and Jaakkola, Tommi},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Szummer, Jaakkola - 2001 - Clustering and efficient use of unlabeled examples.pdf:pdf},
title = {{Clustering and efficient use of unlabeled examples}},
url = {http://www.ai.mit.edu/projects/ntt/projects/MIT2000-08/documents/SzummerJaakkola.pdf},
year = {2001}
}
@article{Ferkingstad2014,
abstract = {As increasingly complex hypothesis-testing scenarios are considered in many scientific fields, analytic derivation of null distributions is often out of reach. To the rescue comes Monte Carlo testing, which may appear deceptively simple: as long as you can sample test statistics under the null hypothesis, the p-value is just the proportion of sampled test statistics that exceed the observed test statistic. Sampling test statistics is often simple once you have a Monte Carlo null model for your data, and defining some form of randomization procedure is also, in many cases, relatively straightforward. However, there may be several possible choices of randomization null model for the data, and no clear-cut criteria for choosing among them. Obviously, different null models may lead to very different p-values, and a very low p-value may thus occur due to the inadequacy of the chosen null model. It is preferable to use assumptions about the underlying random data generation process to guide selection of a null model. In many cases, we may order the null models by increasing preservation of the data characteristics, and we argue in this paper that this ordering in most cases gives increasing p-values, i.e. lower significance. We denote this as the null complexity principle. The principle gives a better understanding of the different null models and may guide in the choice between the different models.},
archivePrefix = {arXiv},
arxivId = {1404.5970},
author = {Ferkingstad, Egil and Holden, Lars and Sandve, Geir Kjetil},
eprint = {1404.5970},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ferkingstad, Holden, Sandve - 2014 - Monte Carlo null models for genomic data.pdf:pdf},
keywords = {Genomics,Hypothesis testing,Monte Carlo methods,and phrases,ge-,hypothesis testing,monte carlo methods},
pages = {1--20},
title = {{Monte Carlo null models for genomic data}},
url = {http://arxiv.org/abs/1404.5970},
year = {2014}
}
@article{Taddy2001,
archivePrefix = {arXiv},
arxivId = {arXiv:1502.02312v1},
author = {Taddy, Matt and Chen, Chun-sheng and Yun, Jun},
eprint = {arXiv:1502.02312v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Taddy, Chen, Yun - 2001 - Bayesian and Empirical Bayesian Forests.pdf:pdf},
number = {1},
title = {{Bayesian and Empirical Bayesian Forests}},
year = {2001}
}
@article{Robert1994,
archivePrefix = {arXiv},
arxivId = {arXiv:1504.01896v1},
author = {Robert, C P},
eprint = {arXiv:1504.01896v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Robert - 1994 - The Metropolis – Hastings algorithm.pdf:pdf},
keywords = {and phrases,bayesian inference,gibbs sampler,hamiltonian monte carlo,hastings algorithm,intractable density,langevin diffusion,markov chains,mcmc meth-,metropolis,ods},
number = {Mcmc},
pages = {1--15},
title = {{The Metropolis – Hastings algorithm}},
year = {1994}
}
@article{Liu2013,
author = {Liu, J. and Gelman, a. and Hill, J. and Su, Y.-S. and Kropko, J.},
doi = {10.1093/biomet/ast044},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Liu et al. - 2013 - On the stationary distribution of iterative imputations.pdf:pdf},
issn = {0006-3444},
journal = {Biometrika},
month = nov,
number = {1},
pages = {155--173},
title = {{On the stationary distribution of iterative imputations}},
url = {http://biomet.oxfordjournals.org/cgi/doi/10.1093/biomet/ast044},
volume = {101},
year = {2013}
}
@inproceedings{Ben-David2008,
author = {Ben-David, Shai and Lu, Tyler and P\'{a}l, David},
booktitle = {Proceedings of the 21st Annual Conference on Learning Theory},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ben-David, Lu, P\'{a}l - 2008 - Does Unlabeled Data Provably Help Worst-case Analysis of the Sample Complexity of Semi-Supervised Learning.pdf:pdf},
pages = {33--44},
title = {{Does Unlabeled Data Provably Help? Worst-case Analysis of the Sample Complexity of Semi-Supervised Learning.}},
url = {http://www.cs.toronto.edu/~tl/papers/ssl.pdf},
year = {2008}
}
@article{Lin2012,
address = {New York, New York, USA},
author = {Lin, Jimmy and Kolcz, Alek},
doi = {10.1145/2213836.2213958},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lin, Kolcz - 2012 - Large-scale machine learning at twitter.pdf:pdf},
isbn = {9781450312479},
journal = {Proceedings of the 2012 international conference on Management of Data - SIGMOD '12},
keywords = {en-,logistic regression,online learning,sembles,stochastic gradient descent},
pages = {793},
publisher = {ACM Press},
title = {{Large-scale machine learning at twitter}},
url = {http://dl.acm.org/citation.cfm?doid=2213836.2213958},
year = {2012}
}
@inproceedings{Ghahramani1994,
author = {Ghahramani, Zoubin and Jordan, Michael I.},
booktitle = {Advances in Neural Information Processing Systems 6},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ghahramani, Jordan - 1994 - Supervised Learning from incomplete data via an EM approach.pdf:pdf},
title = {{Supervised Learning from incomplete data via an EM approach}},
year = {1994}
}
@article{DeBlasi2013,
abstract = {Discrete random probability measures and the exchangeable random partitions they induce are key tools for addressing a variety of estimation and prediction problems in Bayesian inference. Here we focus on the family of Gibbs-type priors, a recent elegant generalization of the Dirichlet and the Pitman-Yor process priors. These priors share properties that are appealing both from a theoretical and an applied point of view: (i) they admit an intuitive predictive characterization justifying their use in terms of a precise assumption on the learning mechanism; (ii) they stand out in terms of mathematical tractability; (iii) they include several interesting special cases besides the Dirichlet and the Pitman-Yor processes. The goal of our paper is to provide a systematic and unified treatment of Gibbs-type priors and highlight their implications for Bayesian nonparametric inference. We deal with their distributional properties, the resulting estimators, frequentist asymptotic validation and the construction of time-dependent versions. Applications, mainly concerning mixture models and species sampling, serve to convey the main ideas. The intuition inherent to this class of priors and the neat results they lead to make one wonder whether it actually represents the most natural generalization of the Dirichlet process.},
author = {{De Blasi}, Pierpaolo and Favaro, Stefano and Lijoi, Antonio and Mena, Ramses H. and Prunster, Igor and Ruggiero, Matteo},
doi = {10.1109/TPAMI.2013.217},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/De Blasi et al. - 2013 - Are Gibbs-Type Priors the Most Natural Generalization of the Dirichlet Process.pdf:pdf},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Analytical models,Bayes methods,Computational modeling,Educational institutions,Learning systems,Nonparametric statistics,Proposals,Q measurement,Stochastic processes},
number = {2},
pages = {1--1},
title = {{Are Gibbs-Type Priors the Most Natural Generalization of the Dirichlet Process?}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6654160},
volume = {PP},
year = {2013}
}
@phdthesis{Druck2011,
author = {Druck, Gregory},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Druck - 2011 - Generalized Expectation Criteria for Lightly Supervised Learning.pdf:pdf},
number = {September},
title = {{Generalized Expectation Criteria for Lightly Supervised Learning}},
url = {http://scholarworks.umass.edu/open\_access\_dissertations/440/},
year = {2011}
}
@article{Huang2013,
abstract = {Traditionally, the hinge loss is used to construct support vector machine (SVM) classifiers. The hinge loss is related to the shortest distance between sets and the corresponding classifier is hence sensitive to noise and unstable for re-sampling. In contrast, the pinball loss is related to the quantile distance and the result is less sensitive. The pinball loss has been deeply studied and widely applied in regression but it has not been used for classification. In this paper, we propose a SVM classifier with the pinball loss, called pin-SVM, and investigate its properties, including noise insensitivity, robustness, and misclassification error. Besides, insensitive zone is applied to the pin-SVM and a sparse model is obtained. Compared to the SVM with the hinge loss, the proposed pin-SVM has the same computational complexity and enjoys noise insensitivity and re-sampling stability.},
author = {Huang, Xiaolin and Shi, Lei and Suykens, Johan a K},
doi = {D7CF84C8-DF7E-492B-A669-0B4CFDEE5D3D},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Huang, Shi, Suykens - 2013 - Support Vector Machine Classifier with Pinball Loss.pdf:pdf},
issn = {1939-3539},
journal = {IEEE transactions on pattern analysis and machine intelligence},
month = sep,
number = {5},
pages = {984--997},
pmid = {24062537},
title = {{Support Vector Machine Classifier with Pinball Loss.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24062537},
volume = {36},
year = {2013}
}
@inproceedings{VanderMaaten2013,
author = {{Van der Maaten}, Laurens and Chen, Minmin and Tyree, Stephen and Weinberger, Kilian Q.},
booktitle = {Proceedings of the 30th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Van der Maaten et al. - 2013 - Learning with Marginalized Corrupted Features.pdf:pdf},
pages = {410--418},
title = {{Learning with Marginalized Corrupted Features}},
url = {http://machinelearning.wustl.edu/mlpapers/papers/ICML2013\_vandermaaten13},
year = {2013}
}
@article{Xu2009,
address = {New York, New York, USA},
author = {Xu, Linli and White, Martha and Schuurmans, Dale},
doi = {10.1145/1553374.1553519},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Xu, White, Schuurmans - 2009 - Optimal reverse prediction.pdf:pdf},
isbn = {9781605585161},
journal = {Proceedings of the 26th Annual International Conference on Machine Learning - ICML '09},
pages = {1--8},
publisher = {ACM Press},
title = {{Optimal reverse prediction}},
url = {http://portal.acm.org/citation.cfm?doid=1553374.1553519},
year = {2009}
}
@inproceedings{Pfahringer2000,
author = {Pfahringer, Bernhard and Giraud-carrier, Christophe},
booktitle = {Proceedings of the 17th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Pfahringer, Giraud-carrier - 2000 - Meta-Learning by Landmarking Various Learning Algorithms.pdf:pdf},
pages = {743--750},
title = {{Meta-Learning by Landmarking Various Learning Algorithms}},
year = {2000}
}
@article{Qiu2013,
abstract = {A low-rank transformation learning framework for subspace clustering and classification is here proposed. Many high-dimensional data, such as face images and motion sequences, approximately lie in a union of low-dimensional subspaces. The corresponding subspace clustering problem has been extensively studied in the literature to partition such high-dimensional data into clusters corresponding to their underlying low-dimensional subspaces. However, low-dimensional intrinsic structures are often violated for real-world observations, as they can be corrupted by errors or deviate from ideal models. We propose to address this by learning a linear transformation on subspaces using matrix rank, via its convex surrogate nuclear norm, as the optimization criteria. The learned linear transformation restores a low-rank structure for data from the same subspace, and, at the same time, forces a high-rank structure for data from different subspaces. In this way, we reduce variations within the subspaces, and increase separation between the subspaces for a more robust subspace clustering. This proposed learned robust subspace clustering framework significantly enhances the performance of existing subspace clustering methods. Basic theoretical results here presented help to further support the underlying framework. To exploit the low-rank structures of the transformed subspaces, we further introduce a fast subspace clustering technique, called Robust Sparse Subspace Clustering, which efficiently combines robust PCA with sparse modeling. When class labels are present at the training stage, we show this low-rank transformation framework also significantly enhances classification performance. Extensive experiments using public datasets are presented, showing that the proposed approach significantly outperforms state-of-the-art methods for subspace clustering and classification.},
archivePrefix = {arXiv},
arxivId = {1309.2074},
author = {Qiu, Qiang and Sapiro, Guillermo},
eprint = {1309.2074},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Qiu, Sapiro - 2013 - Learning Transformations for Clustering and Classification.pdf:pdf},
pages = {187--225},
title = {{Learning Transformations for Clustering and Classification}},
url = {http://arxiv.org/abs/1309.2074},
volume = {16},
year = {2013}
}
@article{Hussami2013,
abstract = {We propose a new sparse regression method called the component lasso, based on a simple idea. The method uses the connected-components structure of the sample covariance matrix to split the problem into smaller ones. It then applies the lasso to each subproblem separately, obtaining a coefficient vector for each one. Then, it uses non-negative least squares to recombine the different vectors into a single so- lution. This step is useful in selecting and reweighting components that are correlated with the response. Simulated and real data examples show that the component lasso can outperform standard regression methods such as the lasso and elastic net, achieving a lower mean squared error as well as better support recovery. Keywords.},
archivePrefix = {arXiv},
arxivId = {arXiv:1311.4472v2},
author = {Hussami, Nadine and Tibshirani, Robert},
eprint = {arXiv:1311.4472v2},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hussami, Tibshirani - 2013 - A Component Lasso.pdf:pdf},
journal = {arXiv preprint arXiv:1311.4472},
keywords = {connected components,elastic net,graphical lasso,grouping effect,lasso,negative least squares,sparsity},
number = {2},
pages = {1--19},
title = {{A Component Lasso}},
url = {http://arxiv.org/abs/1311.4472},
year = {2013}
}
@article{Sch,
archivePrefix = {arXiv},
arxivId = {arXiv:1112.2738v1},
author = {Sch\"{o}lkopf, Bernhard and Janzing, Dominik and Peters, Jonas and Zhang, Kun},
eprint = {arXiv:1112.2738v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sch\"{o}lkopf et al. - Unknown - Robust Learning via Cause-Effect Models.pdf:pdf},
pages = {1--15},
title = {{Robust Learning via Cause-Effect Models}}
}
@inproceedings{Chen2011,
author = {Chen, Minmin and Weinberger, Kilian Q. and Blitzer, John C.},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Chen, Weinberger, Blitzer - 2011 - Co-Training for Domain Adaptation.pdf:pdf},
pages = {2456--2464},
title = {{Co-Training for Domain Adaptation.}},
url = {https://papers.nips.cc/paper/4433-co-training-for-domain-adaptation.pdf},
year = {2011}
}
@article{Dai2006,
author = {Dai, Yu-Hong},
doi = {10.1137/040613305},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Dai - 2006 - Fast Algorithms for Projection on an Ellipsoid.pdf:pdf},
issn = {1052-6234},
journal = {SIAM Journal on Optimization},
keywords = {040613305,1,10,1137,65k05,90c06,ams subject classifications,doi,ellipsoid,hybrid algorithm,introduction,large-scale,linear convergence,on a general convex,projection,set,the problem of projection},
month = jan,
number = {4},
pages = {986--1006},
title = {{Fast Algorithms for Projection on an Ellipsoid}},
url = {http://epubs.siam.org/doi/abs/10.1137/040613305},
volume = {16},
year = {2006}
}
@article{Torgo2014,
archivePrefix = {arXiv},
arxivId = {1412.0436},
author = {Torgo, Luis},
eprint = {1412.0436},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Torgo - 2014 - An Infra-Structure for Performance Estimation and Experimental Comparison of Predictive Models in R.pdf:pdf},
month = dec,
title = {{An Infra-Structure for Performance Estimation and Experimental Comparison of Predictive Models in R}},
url = {http://arxiv.org/abs/1412.0436v1},
year = {2014}
}
@article{Buja2014,
author = {Buja, A and Brown, L},
doi = {10.1214/13-AOS1175},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Buja, Brown - 2014 - Discussion of A significance test for the lasso''.pdf:pdf},
journal = {The Annals of Statistics},
number = {2},
pages = {509--517},
title = {{Discussion of "A significance test for the lasso''}},
volume = {42},
year = {2014}
}
@article{Zeiler2012,
archivePrefix = {arXiv},
arxivId = {arXiv:1311.2901v3},
author = {Zeiler, Matthew D and Fergus, Rob},
eprint = {arXiv:1311.2901v3},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zeiler, Fergus - 2013 - Visualizing and Understanding Convolutional Networks.pdf:pdf},
title = {{Visualizing and Understanding Convolutional Networks}},
year = {2013}
}
@article{Todorovski2003,
author = {Todorovski, Ljupco and D\v{z}eroski, Saso},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Todorovski, D\v{z}eroski - 2003 - Combining classifiers with meta decision trees.pdf:pdf},
journal = {Machine learning},
keywords = {combining classifiers,decision trees,ensembles of classifiers,meta-level learning,stacking},
pages = {223--249},
title = {{Combining classifiers with meta decision trees}},
url = {http://link.springer.com/article/10.1023/A:1021709817809},
year = {2003}
}
@incollection{Wiering,
author = {Wiering, Marco A. and Schomaker, Lambert R.B.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wiering, Schomaker - Unknown - Multi-Layer Support Vector Machines.pdf:pdf},
title = {{Multi-Layer Support Vector Machines}},
url = {http://www.ai.rug.nl/~mwiering/GROUP/ARTICLES/Multi-Layer-SVM.pdf}
}
@article{Kalousis2001,
author = {Kalousis, Alexandros and Hilario, Melanie},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kalousis, Hilario - 2001 - Model selection via meta-learning a comparative study.pdf:pdf},
journal = {International Journal on Artificial Intelligence Tools},
number = {4},
title = {{Model selection via meta-learning: a comparative study}},
url = {http://www.worldscientific.com/doi/abs/10.1142/S0218213001000647},
volume = {10},
year = {2001}
}
@article{Schmidt1996,
author = {Schmidt, Karsten},
doi = {10.1007/BF00046993},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Schmidt - 1996 - A comparison of minimax and least squares estimators in linear regression with polyhedral prior information.pdf:pdf},
issn = {0167-8019},
journal = {Acta Applicandae Mathematicae},
keywords = {1,3,average performance,estimator,inequality restricted least squares,minimax estima-,n vector of,parameter restrictions,polyhedral prior information in,projection estimators,regression model y,the linear regression model,tion,u,we consider the linear,where y is an,x},
month = apr,
number = {1},
pages = {127--138},
title = {{A comparison of minimax and least squares estimators in linear regression with polyhedral prior information}},
url = {http://link.springer.com/10.1007/BF00046993},
volume = {43},
year = {1996}
}
@unpublished{Blum2001,
author = {Blum, Avrim and Chawla, S},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Blum, Chawla - 2001 - Learning from labeled and unlabeled data using graph mincuts.pdf:pdf},
institution = {Carnegie Mellon University, Computer Science Department},
title = {{Learning from labeled and unlabeled data using graph mincuts}},
url = {http://repository.cmu.edu/compsci/163/?utm\_source=repository.cmu.edu\%2Fcompsci\%2F163\&utm\_medium=PDF\&utm\_campaign=PDFCoverPages},
year = {2001}
}
@article{Lockhart2014d,
author = {Lockhart, Richard and Taylor, Jonathan and Tibshirani, Ryan J. and Tibshirani, Robert},
doi = {10.1214/13-AOS1175},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lockhart et al. - 2014 - A significance test for the lasso(3).pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
month = apr,
number = {2},
pages = {413--468},
title = {{A significance test for the lasso}},
url = {http://projecteuclid.org/euclid.aos/1400592161},
volume = {42},
year = {2014}
}
@article{Kawakita2014a,
abstract = {We are interested in developing a safe semi-supervised learning that works in any situation. Semi-supervised learning postulates that n(') unlabeled data are available in addition to n labeled data. However, almost all of the previous semi-supervised methods require additional assumptions (not only unlabeled data) to make improvements on supervised learning. If such assumptions are not met, then the methods possibly perform worse than supervised learning. Sokolovska, Capp\'{e}, and Yvon (2008) proposed a semi-supervised method based on a weighted likelihood approach. They proved that this method asymptotically never performs worse than supervised learning (i.e., it is safe) without any assumption. Their method is attractive because it is easy to implement and is potentially general. Moreover, it is deeply related to a certain statistical paradox. However, the method of Sokolovska et al. (2008) assumes a very limited situation, i.e., classification, discrete covariates, n(')→∞ and a maximum likelihood estimator. In this paper, we extend their method by modifying the weight. We prove that our proposal is safe in a significantly wide range of situations as long as n≤n('). Further, we give a geometrical interpretation of the proof of safety through the relationship with the above-mentioned statistical paradox. Finally, we show that the above proposal is asymptotically safe even when n(')<n by modifying the weight. Numerical experiments illustrate the performance of these methods.},
author = {Kawakita, Masanori and Takeuchi, Jun'ichi},
doi = {10.1016/j.neunet.2014.01.016},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kawakita, Takeuchi - 2014 - Safe semi-supervised learning based on weighted likelihood(2).pdf:pdf},
issn = {1879-2782},
journal = {Neural Networks},
keywords = {semi-supervised learning},
month = may,
pages = {146--64},
publisher = {Elsevier Ltd},
title = {{Safe semi-supervised learning based on weighted likelihood}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24632000},
volume = {53},
year = {2014}
}
@techreport{Seeger2001,
author = {Seeger, Matthias},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Seeger - 2001 - Learning with labeled and unlabeled data.pdf:pdf},
pages = {1--62},
title = {{Learning with labeled and unlabeled data}},
year = {2001}
}
@article{Krahenbuhl2011a,
abstract = {Most state-of-the-art techniques for multi-class image segmentation and labeling use conditional random fields defined over pixels or image regions. While regionlevel models often feature dense pairwise connectivity, pixel-level models are considerably larger and have only permitted sparse graph structures. In this paper, we consider fully connected CRF models defined on the complete set of pixels in an image. The resulting graphs have billions of edges, making traditional inference algorithms impractical. Our main contribution is a highly efficient approximate inference algorithm for fully connected CRF models in which the pairwise edge potentials are defined by a linear combination of Gaussian kernels. Our experiments demonstrate that dense connectivity at the pixel level substantially improves segmentation and labeling accuracy.},
archivePrefix = {arXiv},
arxivId = {1210.5644},
author = {Kr¨ahenb¨uhl, Philipp and Koltun, Vladlen and Krahenbuhl, P},
eprint = {1210.5644},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kr¨ahenb¨uhl, Koltun, Krahenbuhl - 2011 - Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials.pdf:pdf},
isbn = {9781618395993},
journal = {Advances in Neural Information Processing Systems 24 (Proceedings of NIPS)},
keywords = {conditional random field,filtering,message passing,sampling,segmentation},
number = {4},
pages = {1--9},
title = {{Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials}},
year = {2011}
}
@inproceedings{Sindhwani2006,
address = {New York, New York, USA},
author = {Sindhwani, Vikas and Keerthi, S. S.},
booktitle = {Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sindhwani, Keerthi - 2006 - Large scale semi-supervised linear SVMs.pdf:pdf},
isbn = {1595933697},
keywords = {global optimiza-,support vector machines,text categorization,tion,unlabeled data},
pages = {477},
publisher = {ACM Press},
title = {{Large scale semi-supervised linear SVMs}},
year = {2006}
}
@article{Shore1980,
author = {Shore, John E. and Johnson, Rodney W.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Shore, Johnson - 1980 - Axiomatic derivation of the principle of maximum entropy and the principle of minimum cross-entropy.pdf:pdf},
journal = {IEEE Transactions on Information Theory},
number = {1},
pages = {26--37},
title = {{Axiomatic derivation of the principle of maximum entropy and the principle of minimum cross-entropy}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1056144},
volume = {26},
year = {1980}
}
@unpublished{Scholkopf2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1501.06794v1},
author = {Sch\"{o}lkopf, Bernhard and Muandet, Krikamol and Fukumizu, Kenji and Peters, Jonas},
eprint = {arXiv:1501.06794v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sch\"{o}lkopf et al. - 2015 - Computing Functions of Random Variables via Reproducing Kernel Hilbert Space Representations.pdf:pdf},
pages = {1--20},
title = {{Computing Functions of Random Variables via Reproducing Kernel Hilbert Space Representations}},
year = {2015}
}
@inproceedings{Arthur2007,
abstract = {The k-means method is a widely used clustering technique that seeks to minimize the average squared distance between points in the same cluster. Although it offers no accuracy guarantees, its simplicity and speed are very appealing in practice. By augmenting k-means with a very simple, ran- domized seeding technique, we obtain an algorithm that is $\Theta$(log k)-competitive with the optimal clustering. Prelim- inary experiments show that our augmentation improves both the speed and the accuracy of k-means, often quite dramatically.},
author = {Arthur, David and Vassilvitskii, Sergei},
booktitle = {Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Arthur, Vassilvitskii - 2007 - k-means The Advantages of Careful Seeding.pdf:pdf},
pages = {1027--1035},
title = {{k-means ++ : The Advantages of Careful Seeding}},
year = {2007}
}
@article{Rooyen,
archivePrefix = {arXiv},
arxivId = {arXiv:1504.00083v1},
author = {Rooyen, Brendan Van and Williamson, Robert C},
eprint = {arXiv:1504.00083v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Rooyen, Williamson - Unknown - A Theory of Feature Learning.pdf:pdf},
title = {{A Theory of Feature Learning}}
}
@article{Trappenberg,
author = {Trappenberg, Thomas},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Trappenberg - Unknown - A brief introduction to probabilistic machine learning with neuroscientific relations.pdf:pdf},
title = {{A brief introduction to probabilistic machine learning with neuroscientific relations}}
}
@article{Rice2010,
author = {Rice, Kenneth},
doi = {10.1198/tast.2010.09060},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Rice - 2010 - A Decision-Theoretic Formulation of Fisher’s Approach to Testing.pdf:pdf},
issn = {0003-1305},
journal = {The American Statistician},
month = nov,
number = {4},
pages = {345--349},
title = {{A Decision-Theoretic Formulation of Fisher’s Approach to Testing}},
url = {http://www.tandfonline.com/doi/abs/10.1198/tast.2010.09060},
volume = {64},
year = {2010}
}
@inproceedings{Cortes1993,
author = {Cortes, Corinna and Jackel, L.D.},
booktitle = {Advances in Neural Information Processing Systems 6},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cortes, Jackel - 1993 - Learning Cuves Asymptotic Values and Rate of Convergence.pdf:pdf},
pages = {327--334},
title = {{Learning Cuves: Asymptotic Values and Rate of Convergence}},
url = {http://scholar.google.com/scholar?hl=en\&btnG=Search\&q=intitle:Learning+Cuves:+Asymptotic+Values+and+Rate+of+Convergence\#0},
year = {1993}
}
@inproceedings{Rao1995,
author = {Rao, R. Bharat and Gordon, Diana and Spears, William},
booktitle = {Proceedings of the 12th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Rao, Gordon, Spears - 1995 - For Every Generalization Action, Is there really an Equal and Opposite Reaction Analysis of the conservatio.pdf:pdf},
pages = {471--479},
title = {{For Every Generalization Action, Is there really an Equal and Opposite Reaction? Analysis of the conservation Law for Generalization Performance}},
url = {http://www.researchgate.net/publication/2516136\_For\_Every\_Generalization\_Action\_Is\_There\_Really\_An\_Equal\_And\_Opposite\_Reaction\_Analysis\_of\_the\_Conservation\_Law\_for\_Generalization\_Performance/file/79e4150b866697f897.pdf},
year = {1995}
}
@article{Gao2015,
author = {Gao, Chao and Zhou, Harrison H.},
doi = {10.1214/14-AOS1268},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gao, Zhou - 2015 - Rate-optimal posterior contraction for sparse PCA.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
number = {2},
pages = {785--818},
title = {{Rate-optimal posterior contraction for sparse PCA}},
url = {http://projecteuclid.org/euclid.aos/1427115287},
volume = {43},
year = {2015}
}
@article{Wilson1997,
author = {Wilson, D Randall and Martinez, Tony R},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wilson, Martinez - 1997 - Improved Heterogeneous Distance Functions.pdf:pdf},
pages = {1--34},
title = {{Improved Heterogeneous Distance Functions}},
volume = {6},
year = {1997}
}
@article{Poggio2004,
abstract = {Developing theoretical foundations for learning is a key step towards understanding intelligence. 'Learning from examples' is a paradigm in which systems (natural or artificial) learn a functional relationship from a training set of examples. Within this paradigm, a learning algorithm is a map from the space of training sets to the hypothesis space of possible functional solutions. A central question for the theory is to determine conditions under which a learning algorithm will generalize from its finite training set to novel examples. A milestone in learning theory was a characterization of conditions on the hypothesis space that ensure generalization for the natural class of empirical risk minimization (ERM) learning algorithms that are based on minimizing the error on the training set. Here we provide conditions for generalization in terms of a precise stability property of the learning process: when the training set is perturbed by deleting one example, the learned hypothesis does not change much. This stability property stipulates conditions on the learning map rather than on the hypothesis space, subsumes the classical theory for ERM algorithms, and is applicable to more general algorithms. The surprising connection between stability and predictivity has implications for the foundations of learning theory and for the design of novel algorithms, and provides insights into problems as diverse as language learning and inverse problems in physics and engineering.},
author = {Poggio, Tomaso and Rifkin, Ryan and Mukherjee, Sayan and Niyogi, Partha},
doi = {10.1038/nature02341},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Poggio et al. - 2004 - General conditions for predictivity in learning theory.pdf:pdf},
issn = {1476-4687},
journal = {Nature},
keywords = {Algorithms,Intelligence,Language,Learning,Learning: physiology,Models, Theoretical,Probability,Research Design},
month = mar,
number = {6981},
pages = {419--22},
pmid = {15042089},
title = {{General conditions for predictivity in learning theory.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15042089},
volume = {428},
year = {2004}
}
@article{Berthet2013,
author = {Berthet, Quentin and Rigollet, Philippe},
doi = {10.1214/13-AOS1127},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Berthet, Rigollet - 2013 - Optimal detection of sparse principal components in high dimension.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
month = aug,
number = {4},
pages = {1780--1815},
title = {{Optimal detection of sparse principal components in high dimension}},
url = {http://projecteuclid.org/euclid.aos/1378386239},
volume = {41},
year = {2013}
}
@inproceedings{Walt2007,
author = {Walt, Christiaan Van Der and Barnard, Etienne},
booktitle = {18th Annual Symposium of the Pattern Recognition Association of South Africa},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Walt, Barnard - 2007 - Measures for the characterisation of pattern-recognition data sets.pdf:pdf},
title = {{Measures for the characterisation of pattern-recognition data sets}},
url = {http://researchspace.csir.co.za/dspace/handle/10204/1979},
year = {2007}
}
@article{Li2006,
author = {Li, Tao and Zhu, Shenghuo and Ogihara, Mitsunori},
doi = {10.1007/s10115-006-0013-y},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Li, Zhu, Ogihara - 2006 - Using discriminant analysis for multi-class classification an experimental investigation.pdf:pdf},
issn = {0219-1377},
journal = {Knowledge and Information Systems},
keywords = {discriminant analysis,multi-class classification},
month = mar,
number = {4},
pages = {453--472},
title = {{Using discriminant analysis for multi-class classification: an experimental investigation}},
url = {http://www.springerlink.com/index/10.1007/s10115-006-0013-y},
volume = {10},
year = {2006}
}
@inproceedings{Kopf2000,
author = {K\"{o}pf, Christian and Taylor, Charles and Keller, Jorg},
booktitle = {Proceedings of the PKDD-00 workshop on data mining, decision support, meta-learning and ILP},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/K\"{o}pf, Taylor, Keller - 2000 - Meta-analysis from data characterisation for meta-learning to meta-regression.pdf:pdf},
number = {Ml},
title = {{Meta-analysis: from data characterisation for meta-learning to meta-regression}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.26.8159},
year = {2000}
}
@article{Narisetty2014,
author = {Narisetty, Naveen Naidu and He, Xuming},
doi = {10.1214/14-AOS1207},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Narisetty, He - 2014 - Bayesian variable selection with shrinking and diffusing priors.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
month = apr,
number = {2},
pages = {789--817},
title = {{Bayesian variable selection with shrinking and diffusing priors}},
url = {http://projecteuclid.org/euclid.aos/1400592178},
volume = {42},
year = {2014}
}
@article{Muja2014,
author = {Muja, Marius and Lowe, David G.},
doi = {10.1109/TPAMI.2014.2321376},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Muja, Lowe - 2014 - Scalable Nearest Neighbor Algorithms for High Dimensional Data.pdf:pdf},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
month = nov,
number = {11},
pages = {2227--2240},
title = {{Scalable Nearest Neighbor Algorithms for High Dimensional Data}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6809191},
volume = {36},
year = {2014}
}
@book{Lehmann1986,
author = {Lehmann, E.L.},
edition = {Second},
publisher = {Wiley},
title = {{Testing Statistical Hyptoheses}},
year = {1986}
}
@inproceedings{Li2009,
author = {Li, Yu-Feng and Kwok, James T. and Zhou, Zhi-hua},
booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Li, Kwok, Zhou - 2009 - Semi-supervised learning using label mean.pdf:pdf},
pages = {633--640},
title = {{Semi-supervised learning using label mean}},
url = {http://dl.acm.org/citation.cfm?id=1553456},
year = {2009}
}
@article{Xue2015,
author = {Xue, Jing-hao and Hall, Peter},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Xue, Hall - 2015 - Why Does Rebalancing Class-Unbalanced Data Improve AUC for Linear Discriminant Analysis.pdf:pdf},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {5},
pages = {1109--1112},
title = {{Why Does Rebalancing Class-Unbalanced Data Improve AUC for Linear Discriminant Analysis?}},
volume = {37},
year = {2015}
}
@inproceedings{Bernad2004,
author = {Bernad\'{o}-Mansilla, Ester and Ho, Tin Kam},
booktitle = {Proceedings of the 17th International Conference on Pattern Recognition},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bernad\'{o}-Mansilla, Ho - 2004 - On classifier domains of competence.pdf:pdf},
title = {{On classifier domains of competence}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1334026},
year = {2004}
}
@article{Hand2014,
author = {Hand, David J.},
doi = {10.1214/13-STS446},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hand - 2014 - Wonderful Examples, but Let’s not Close Our Eyes.pdf:pdf},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {Frequentist, likelihood inference, Neyman-Pearson,and phrases,frequentist,hypothesis testing,informative and thought-provoking,likelihood inference,making specific comments,neyman,on each of these,pearson,schools of inference,space prohibits me from},
month = feb,
number = {1},
pages = {98--100},
title = {{Wonderful Examples, but Let’s not Close Our Eyes}},
url = {http://projecteuclid.org/euclid.ss/1399645735},
volume = {29},
year = {2014}
}
@article{Platanios2014,
author = {Platanios, Emmanouil Antonios and Blum, Avrim and Mitchell, Tom},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Platanios, Blum, Mitchell - 2014 - Estimating Accuracy from Unlabeled Data.pdf:pdf},
isbn = {9780974903910},
journal = {30th Conference on Uncertainty in Artificial Intelligence},
title = {{Estimating Accuracy from Unlabeled Data}},
year = {2014}
}
@article{Harville2013,
author = {Approach, Nondenominational Model-based and Harville, David A},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Approach, Harville - 2013 - The Need for More Emphasis on Prediction a “Nondenominational” Model-Based Approach.pdf:pdf},
journal = {The American Statistician},
month = sep,
number = {September},
title = {{The Need for More Emphasis on Prediction: a “Nondenominational” Model-Based Approach}},
year = {2013}
}
@article{Wang2009b,
author = {Wang, Junhui and Shen, Xiaotong and Pan, Wei},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wang, Shen, Pan - 2009 - On efficient large margin semisupervised learning Method and theory.pdf:pdf},
journal = {The Journal of Machine Learning Research},
keywords = {classification,difference convex programming,nonconvex minimization,regulariza-,support vectors,tion},
pages = {719--742},
title = {{On efficient large margin semisupervised learning: Method and theory}},
url = {http://dl.acm.org/citation.cfm?id=1577094},
volume = {10},
year = {2009}
}
@inproceedings{Yu2012,
address = {Edinburgh, Scotland},
author = {Yu, AW and Su, Hao and Fei-Fei, L},
booktitle = {International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Yu, Su, Fei-Fei - 2012 - Efficient euclidean projections onto the intersection of norm balls.pdf:pdf},
pages = {433--440},
title = {{Efficient euclidean projections onto the intersection of norm balls}},
url = {http://arxiv.org/abs/1206.4638},
year = {2012}
}
@article{Tibshirani1996,
author = {Tibshirani, Robert},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Tibshirani - 1996 - Regression shrinkage and selection via the lasso.pdf:pdf},
journal = {Journal of the Royal Statistical Society. Series B},
keywords = {quadratic programming,regression,shrinkage,subset selection},
number = {1},
pages = {267--288},
title = {{Regression shrinkage and selection via the lasso}},
volume = {58},
year = {1996}
}
@article{Kawano2012,
archivePrefix = {arXiv},
arxivId = {arXiv:1108.5244v3},
author = {Kawano, Shuichi},
eprint = {arXiv:1108.5244v3},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kawano - 2012 - Semi-supervised logistic discrimination via labeled data and unlabeled data from different sampling distributions.pdf:pdf},
journal = {arXiv preprint},
keywords = {and phrases,covariate shift,em algorithm,model selection,reg-,semi-supervised learning,ularization},
pages = {1--19},
title = {{Semi-supervised logistic discrimination via labeled data and unlabeled data from different sampling distributions}},
year = {2012}
}
@article{Jones2012,
author = {Jones, Emrys A and Deininger, S\"{o}ren-oliver and Hogendoorn, Pancras C W and Deelder, Andr\'{e} M and Mcdonnell, Liam A},
doi = {10.1016/j.jprot.2012.06.014},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Jones et al. - 2012 - Imaging mass spectrometry statistical analysis.pdf:pdf},
issn = {1874-3919},
journal = {Journal of Proteomics},
keywords = {Biomarker discovery,Data analysis,Molecular histology,imaging mass spectrometry},
number = {16},
pages = {4962--4989},
publisher = {Elsevier B.V.},
title = {{Imaging mass spectrometry statistical analysis}},
url = {http://dx.doi.org/10.1016/j.jprot.2012.06.014},
volume = {75},
year = {2012}
}
@book{Seeger2004,
abstract = {Gaussian processes (GPs) are natural generalisations of multivariate Gaussian random variables to infinite (countably or continuous) index sets. GPs have been applied in a large number of fields to a diverse range of ends, and very many deep theoretical analyses of various properties are available. This paper gives an introduction to Gaussian processes on a fairly elementary level with special emphasis on characteristics relevant in machine learning. It draws explicit connections to branches such as spline smoothing models and support vector machines in which similar ideas have been investigated. Gaussian process models are routinely used to solve hard machine learning problems. They are attractive because of their flexible non-parametric nature and computational simplicity. Treated within a Bayesian framework, very powerful statistical methods can be implemented which offer valid estimates of uncertainties in our predictions and generic model selection procedures cast as nonlinear optimization problems. Their main drawback of heavy computational scaling has recently been alleviated by the introduction of generic sparse approximations.13,78,31 The mathematical literature on GPs is large and often uses deep concepts which are not required to fully understand most machine learning applications. In this tutorial paper, we aim to present characteristics of GPs relevant to machine learning and to show up precise connections to other "kernel machines" popular in the community. Our focus is on a simple presentation, but references to more detailed sources are provided.},
author = {Seeger, Matthias},
booktitle = {International journal of neural systems},
doi = {10.1142/S0129065704001899},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Seeger - 2004 - Gaussian processes for machine learning.pdf:pdf},
isbn = {026218253X},
issn = {0129-0657},
keywords = {Algorithms,Artificial Intelligence,Bayes Theorem,Entropy,Linear Models,Models, Statistical,Normal Distribution,Regression Analysis,Statistics, Nonparametric},
month = apr,
number = {2},
pages = {69--106},
pmid = {15112367},
title = {{Gaussian processes for machine learning.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15112367},
volume = {14},
year = {2004}
}
@article{Hollenbach2014,
abstract = {Gold-standard approaches to missing data imputation are complicated and computationally expensive. We present a principled solution to this situation, using copula distributions from which missing data may be quickly drawn. We compare this approach to other imputation techniques and show that it performs at least as well as less computationally efficient approaches. Our results demonstrate that most applied researchers can achieve great speed improvements implementing a copula-based imputation approach, while still maintaining the performance of other approaches to multiple imputation. Moreover, this approach can be easily implemented at the point of need in Bayesian analyses.},
archivePrefix = {arXiv},
arxivId = {1411.0647},
author = {Hollenbach, Florian M. and Metternich, Nils W. and Minhas, Shahryar and Ward, Michael D.},
eprint = {1411.0647},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hollenbach et al. - 2014 - Fast \& Easy Imputation of Missing Social Science Data.pdf:pdf},
month = nov,
pages = {1--17},
title = {{Fast \& Easy Imputation of Missing Social Science Data}},
url = {http://arxiv.org/abs/1411.0647},
year = {2014}
}
@article{Homrighausen2014,
author = {Homrighausen, Darren and McDonald, Daniel J.},
doi = {10.1007/s10994-014-5438-z},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Homrighausen, McDonald - 2014 - Leave-one-out cross-validation is risk consistent for lasso.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {persistence,stochastic equicontinuity,uniform convergence},
month = mar,
number = {March 2013},
title = {{Leave-one-out cross-validation is risk consistent for lasso}},
url = {http://link.springer.com/10.1007/s10994-014-5438-z},
year = {2014}
}
@article{Gelman2014,
author = {Gelman, Andrew},
doi = {10.1214/13-STS458},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gelman - 2014 - How Bayesian Analysis Cracked the Red-State, Blue-State Problem.pdf:pdf},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {Multilevel regression and poststratification (MRP),and phrases,mrp,multilevel regression and poststratification,political science,sample surveys,sparse data,voting},
month = feb,
number = {1},
pages = {26--35},
title = {{How Bayesian Analysis Cracked the Red-State, Blue-State Problem}},
url = {http://projecteuclid.org/euclid.ss/1399645725},
volume = {29},
year = {2014}
}
@article{Kelejian1969,
author = {Kelejian, H.H.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kelejian - 1969 - Missing Observations in Multivaraite Regression Efficiency of a First-Order Method.pdf:pdf},
journal = {Journal of the American Statistical Association},
number = {328},
pages = {1609--1616},
title = {{Missing Observations in Multivaraite Regression: Efficiency of a First-Order Method}},
volume = {64},
year = {1969}
}
@article{Gelman2013d,
author = {Gelman, Andrew and Robert, Christian P},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gelman, Robert - 2013 - “Not only defended but also applied” The perceived absurdity of Bayesian inference.pdf:pdf},
journal = {The American Statistician},
keywords = {bayesian,bogosity,doomsdsay argument,foundations,frequentist,laplace law of succession},
number = {1},
title = {{“Not only defended but also applied”: The perceived absurdity of Bayesian inference}},
volume = {67},
year = {2013}
}
@book{Basu2006,
author = {Basu, Mitra and Ho, Tin Kam},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Basu, Ho - 2006 - Data complexity in pattern recognition.pdf:pdf},
isbn = {9781846281716},
title = {{Data complexity in pattern recognition}},
url = {http://books.google.com/books?hl=en\&lr=\&id=GflBKbzym9oC\&oi=fnd\&pg=PR11\&dq=Data+Complexity+in+Pattern+Recognition\&ots=igbI3IXn6d\&sig=-7L3L4iU5lzLaNaCVoEux\_GbVn4},
year = {2006}
}
@article{Finucane2014,
author = {Finucane, Mariel M. and Paciorek, Christopher J. and Danaei, Goodarz and Ezzati, Majid},
doi = {10.1214/13-STS427},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Finucane et al. - 2014 - Bayesian Estimation of Population-Level Trends in Measures of Health Status.pdf:pdf},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {Bayesian inference, hierarchical models, combining,and phrases},
month = feb,
number = {1},
pages = {18--25},
title = {{Bayesian Estimation of Population-Level Trends in Measures of Health Status}},
url = {http://projecteuclid.org/euclid.ss/1399645724},
volume = {29},
year = {2014}
}
@article{Johnson,
archivePrefix = {arXiv},
arxivId = {arXiv:1504.01255v1},
author = {Johnson, Rie},
eprint = {arXiv:1504.01255v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Johnson - Unknown - Semi-Supervised Learning with Multi-View Embedding Theory and Application with Convolutional Neural Networks.pdf:pdf},
pages = {1--16},
title = {{Semi-Supervised Learning with Multi-View Embedding : Theory and Application with Convolutional Neural Networks}}
}
@inproceedings{Loog2012b,
author = {Loog, Marco and Jensen, Are C},
booktitle = {Structural, Syntactic, and Statistical Pattern Recognition},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Loog, Jensen - 2012 - Constrained log-likelihood-based semi-supervised linear discriminant analysis.pdf:pdf},
pages = {327--335},
title = {{Constrained log-likelihood-based semi-supervised linear discriminant analysis}},
url = {http://www.springerlink.com/index/U16X1L3015777162.pdf},
year = {2012}
}
@inproceedings{Grunwald2000,
author = {Gr\"{u}nwald, Peter},
booktitle = {Proceedings of the 16th Conference on Uncertainty in Artificial Intelligence},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gr\"{u}nwald - 2000 - Maximum entropy and the glasses you are looking through.pdf:pdf},
pages = {238--246},
title = {{Maximum entropy and the glasses you are looking through}},
url = {http://dl.acm.org/citation.cfm?id=2073975},
year = {2000}
}
@article{Hong2013,
author = {Hong, Yongmiao and Lee, Yoon-Jin},
doi = {10.1214/13-AOS1099},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hong, Lee - 2013 - A loss function approach to model specification testing and its relative efficiency.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
month = jun,
number = {3},
pages = {1166--1203},
title = {{A loss function approach to model specification testing and its relative efficiency}},
url = {http://projecteuclid.org/euclid.aos/1371150897},
volume = {41},
year = {2013}
}
@article{Hartley1968,
author = {Hartley, H.O. and Rao, J.N.K.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hartley, Rao - 1968 - A new estimation for sample theory surveys.pdf:pdf},
journal = {Biometrika},
number = {3},
pages = {547--557},
title = {{A new estimation for sample theory surveys}},
volume = {55},
year = {1968}
}
@article{Guyon2003,
author = {Guyon, Isabelle and Elisseeff, Andre},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Guyon, Elisseeff - 2003 - An Introduction to Variable and Feature Selection.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {bioinformatics,clustering,computational biology,ery,feature selection,filters,gene expression,genomics,information retrieval,information theory,microarray,model selection,pattern discov-,proteomics,qsar,space dimensionality reduction,statistical testing,support vector machines,text classification,variable selection,wrappers},
pages = {1157--1182},
title = {{An Introduction to Variable and Feature Selection}},
volume = {3},
year = {2003}
}
@inproceedings{Latinne2001,
author = {Latinne, Patrice and Debeir, Olivier and Decaestecker, Christine},
booktitle = {Multiple Classifier Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Latinne, Debeir, Decaestecker - 2001 - Limiting the Number of Trees in Random Forests.pdf:pdf},
title = {{Limiting the Number of Trees in Random Forests}},
year = {2001}
}
@inproceedings{Plessis2012,
author = {du Plessis, Marthinus Christoffel and Sugiyama, Masashi},
booktitle = {Proceedings of the 29th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Plessis, Sugiyama - 2012 - Semi-supervised learning of class balance under class-prior change by distribution matching.pdf:pdf},
title = {{Semi-supervised learning of class balance under class-prior change by distribution matching}},
url = {http://arxiv.org/abs/1206.4677},
year = {2012}
}
@article{Isaksson2008,
author = {Isaksson, Anders and Wallman, M. and Goransson, H. and Gustafsson, Mats G},
doi = {10.1016/j.patrec.2008.06.018},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Isaksson et al. - 2008 - Cross-validation and bootstrapping are unreliable in small sample classification.pdf:pdf},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {performance estimation,supervised classification},
month = oct,
number = {14},
pages = {1960--1965},
title = {{Cross-validation and bootstrapping are unreliable in small sample classification}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167865508002158},
volume = {29},
year = {2008}
}
@article{Shevade2003,
author = {Shevade, S. K. and Keerthi, S. S.},
doi = {10.1093/bioinformatics/btg308},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Shevade, Keerthi - 2003 - A simple and efficient algorithm for gene selection using sparse logistic regression.pdf:pdf},
issn = {1367-4803},
journal = {Bioinformatics},
month = nov,
number = {17},
pages = {2246--2253},
title = {{A simple and efficient algorithm for gene selection using sparse logistic regression}},
url = {http://bioinformatics.oxfordjournals.org/cgi/doi/10.1093/bioinformatics/btg308},
volume = {19},
year = {2003}
}
@article{Ho2002,
author = {Ho, Tin Kam and Basu, Mitra},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ho, Basu - 2002 - Complexity Measures of Supervised Classification Problems.pdf:pdf},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {3},
pages = {289--300},
title = {{Complexity Measures of Supervised Classification Problems}},
volume = {24},
year = {2002}
}
@article{Peltonen2014,
author = {Peltonen, Jaakko and Lin, Ziyuan},
doi = {10.1007/s10994-014-5464-x},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Peltonen, Lin - 2014 - Information retrieval approach to meta-visualization.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {and masashi sugiyama,b,bob williamson,cheng soon ong,editors,j,lin,meta-visualization,neighbor embedding,nonlinear dimensionality reduction,peltonen,tu bao ho,wray buntine,z},
month = oct,
number = {January},
title = {{Information retrieval approach to meta-visualization}},
url = {http://link.springer.com/10.1007/s10994-014-5464-x},
year = {2014}
}
@inproceedings{Ben-David2006,
author = {Ben-David, Shai and Luxburg, Ulrike Von and P\'{a}l, David},
booktitle = {Proceedings of the 19th Annual Conference on Learning Theory},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ben-David, Luxburg, P\'{a}l - 2006 - A sober look at clustering stability.pdf:pdf},
number = {2002},
pages = {5--19},
title = {{A sober look at clustering stability}},
url = {http://link.springer.com/chapter/10.1007/11776420\_4},
year = {2006}
}
@inproceedings{Caticha2006,
archivePrefix = {arXiv},
arxivId = {arXiv:physics/0608185v1},
author = {Caticha, Ariel and Giffin, Adom},
booktitle = {Bayesian Inference and Maximum Entropy Methods In Science and Engineering},
eprint = {0608185v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Caticha, Giffin - 2006 - Updating Probabilities.pdf:pdf},
pages = {31--42},
primaryClass = {arXiv:physics},
title = {{Updating Probabilities}},
url = {http://arxiv.org/abs/physics/0608185},
volume = {872},
year = {2006}
}
@article{Douc2012,
author = {Douc, Randal and Moulines, Eric},
doi = {10.1214/12-AOS1047},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Douc, Moulines - 2012 - Asymptotic properties of the maximum likelihood estimation in misspecified hidden Markov models.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
keywords = {and phrases,hidden markov models,mator,maximum likelihood esti-,misspecified models,state space models,strong consistency},
month = oct,
number = {5},
pages = {2697--2732},
title = {{Asymptotic properties of the maximum likelihood estimation in misspecified hidden Markov models}},
url = {http://projecteuclid.org/euclid.aos/1359987535},
volume = {40},
year = {2012}
}
@article{Xiao2012,
author = {Xiao, Yuanhui and Song, Ruiguang and Chen, Mi and Hall, H. Irene},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Xiao et al. - 2012 - Direct and Unbiased Multiple Imputation Methods for Missing Values of Categorical Variables.pdf:pdf},
journal = {Journal of Data Science},
keywords = {bias,categorical variable,hiv,missing values,multiple impu-},
pages = {465--481},
title = {{Direct and Unbiased Multiple Imputation Methods for Missing Values of Categorical Variables}},
url = {http://www.jdsruc.org/upload/7(2012-07-01170715).pdf},
volume = {10},
year = {2012}
}
@inproceedings{Niu2012,
author = {Niu, Gang and Dai, Bo and Yamada, Makoto and Sugiyama, Masashi},
booktitle = {Proceedings of the 29th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Niu et al. - 2012 - Information-theoretic Semi-supervised Metric Learning via Entropy Regularization.pdf:pdf},
number = {c},
title = {{Information-theoretic Semi-supervised Metric Learning via Entropy Regularization}},
url = {http://arxiv.org/abs/1206.4614},
year = {2012}
}
@article{Leek2007,
abstract = {It has unambiguously been shown that genetic, environmental, demographic, and technical factors may have substantial effects on gene expression levels. In addition to the measured variable(s) of interest, there will tend to be sources of signal due to factors that are unknown, unmeasured, or too complicated to capture through simple models. We show that failing to incorporate these sources of heterogeneity into an analysis can have widespread and detrimental effects on the study. Not only can this reduce power or induce unwanted dependence across genes, but it can also introduce sources of spurious signal to many genes. This phenomenon is true even for well-designed, randomized studies. We introduce "surrogate variable analysis" (SVA) to overcome the problems caused by heterogeneity in expression studies. SVA can be applied in conjunction with standard analysis techniques to accurately capture the relationship between expression and any modeled variables of interest. We apply SVA to disease class, time course, and genetics of gene expression studies. We show that SVA increases the biological accuracy and reproducibility of analyses in genome-wide expression studies.},
author = {Leek, Jeffrey T. and Storey, John D.},
doi = {10.1371/journal.pgen.0030161},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Leek, Storey - 2007 - Capturing heterogeneity in gene expression studies by surrogate variable analysis.pdf:pdf},
isbn = {1553-7404 (Electronic)$\backslash$n1553-7390 (Linking)},
issn = {15537390},
journal = {PLoS Genetics},
number = {9},
pages = {1724--1735},
pmid = {17907809},
title = {{Capturing heterogeneity in gene expression studies by surrogate variable analysis}},
volume = {3},
year = {2007}
}
@inproceedings{Duin2010,
author = {Duin, Robert P.W. and Loog, Marco and Pȩkalska, Elzbieta and Tax, David M.J.},
booktitle = {Proceedings of the 20th International Conference on Pattern Recognition},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Duin et al. - 2010 - Feature-based dissimilarity space classification.pdf:pdf},
pages = {46--55},
title = {{Feature-based dissimilarity space classification}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-17711-8\_5},
year = {2010}
}
@article{Norton2003,
author = {Norton, John D},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Norton - 2003 - A Material Theory of Induction.pdf:pdf},
journal = {Philosophy of Science},
number = {October},
pages = {647--670},
title = {{A Material Theory of Induction}},
volume = {70},
year = {2003}
}
@article{Ridgway2014,
archivePrefix = {arXiv},
arxivId = {1410.1771},
author = {Ridgway, James and Alquier, Pierre and Chopin, Nicolas and Liang, Feng},
eprint = {1410.1771},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ridgway et al. - 2014 - PAC-Bayesian AUC classification and scoring.pdf:pdf},
month = oct,
pages = {1--18},
title = {{PAC-Bayesian AUC classification and scoring}},
url = {http://arxiv.org/abs/1410.1771v1},
year = {2014}
}
@article{Davies1995,
abstract = {This article attempts to provide a formal framework for a data based inference which explicitly and consistently recognizes the approximate nature of probability models. It is based on the idea that a stochastic model is adequate if samples generated under the model are very much like the sample actually obtained. The formalization is based on the concept of data feature. Examples are given of applying the ideas to different areas of statistics including location-scale models, densities, non-parametric regression, interlaboratory test, auto-regressive processes and the analysis of variance. The four cornerstones of the approach are direct comparison, approximation, weak topologies and parsimony. The approach is contrasted to that of much of conventional statistics many of whose concepts are pathologically discontinuous with respect to the topology of data analysis and common sense.},
author = {Davies, Pl},
doi = {10.1111/j.1467-9574.1995.tb01464.x},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Davies - 1995 - Data features ’.pdf:pdf},
issn = {0039-0402},
journal = {Statistica Neerlandica},
keywords = {adequacy,continuity,data analysis,data features,inference,phrases,smooth functionals,topologies,weak and strong},
number = {2},
pages = {185--245},
title = {{Data features ’}},
volume = {49},
year = {1995}
}
@inproceedings{Joachims2003,
author = {Joachims, Thorsten},
booktitle = {Proceedings of the 20th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Joachims - 2003 - Transductive learning via spectral graph partitioning.pdf:pdf},
pages = {290--297},
title = {{Transductive learning via spectral graph partitioning}},
url = {http://www.aaai.org/Papers/ICML/2003/ICML03-040.pdf},
year = {2003}
}
@article{Doksum2007,
author = {Doksum, Kjell and Ozeki, Akichika and Kim, Jihoon and {Chaibub Neto}, Elias},
doi = {10.1016/j.spl.2007.03.005},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Doksum et al. - 2007 - Thinking outside the box Statistical inference based on Kullback–Leibler empirical projections.pdf:pdf},
issn = {01677152},
journal = {Statistics \& Probability Letters},
keywords = {bootstrap,box-cox transformation,classification,covariate,k-l divergence,klep,outside the box,sandwich formula},
month = jul,
number = {12},
pages = {1201--1213},
title = {{Thinking outside the box: Statistical inference based on Kullback–Leibler empirical projections}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167715207000843},
volume = {77},
year = {2007}
}
@inproceedings{Skurichina2002,
author = {Skurichina, Marina and Kuncheva, Ludmila I and Duin, Robert P.W.},
booktitle = {Multiple Classifier Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Skurichina, Kuncheva, Duin - 2002 - Bagging and Boosting for the Nearest Mean Classifier Effects of Sample Size on Diversity and Accura.pdf:pdf},
pages = {62--71},
publisher = {Springer},
title = {{Bagging and Boosting for the Nearest Mean Classifier : Effects of Sample Size on Diversity and Accuracy}},
year = {2002}
}
@article{Rubin1976,
abstract = {SUMMARY When making sampling distribution inferences about the parameter of the data, $\theta$, it is appropriate to ignore the process that causes missing data if the missing data are ‘missing at random’ and the observed data are ‘observed at random’, but these inferences are generally conditional on the observed pattern of missing data. When making direct-likelihood or Bayesian inferences about $\theta$, it is appropriate to ignore the process that causes missing data if the missing data are missing at random and the parameter of the missing data process is ‘distinct’ from $\theta$. These conditions are the weakest general conditions under which ignoring the process that causes missing data always leads to correct inferences.},
author = {Rubin, Donald B.},
doi = {10.2307/2335739},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Rubin - 1976 - Inference and Missing Data.pdf:pdf},
isbn = {0006344414643510},
issn = {00063444},
journal = {Biometrika},
number = {3},
pages = {581},
pmid = {86},
title = {{Inference and Missing Data}},
url = {http://biomet.oxfordjournals.org.libproxy1.nus.edu.sg/content/63/3/581$\backslash$nhttp://www.jstor.org/stable/2335739?origin=crossref},
volume = {63},
year = {1976}
}
@article{Wang2007,
author = {Wang, Junhui and Shen, Xiaotong},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wang, Shen - 2007 - Large margin semi-supervised learning.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {generalization,grouping,sequential quadratic programming,support vectors},
pages = {1867--1891},
title = {{Large margin semi-supervised learning}},
url = {http://jmlr.csail.mit.edu/papers/volume8/wang07a/wang07a.pdf},
volume = {8},
year = {2007}
}
@article{Agarwal2014,
author = {Agarwal, Alekh and Chapelle, Olivier and Dud\'{\i}k, Miroslav and Langford, John},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Agarwal et al. - 2014 - A reliable effective terascale linear learning system.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {allreduce,distributed machine learning,hadoop,repeated online averaging},
pages = {1111--1133},
title = {{A reliable effective terascale linear learning system}},
volume = {15},
year = {2014}
}
@article{Palazzo2014,
author = {Palazzo, Alexander F. and Gregory, T. Ryan},
doi = {10.1371/journal.pgen.1004351},
editor = {Akey, Joshua M.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Palazzo, Gregory - 2014 - The Case for Junk DNA.pdf:pdf},
issn = {1553-7404},
journal = {PLoS Genetics},
month = may,
number = {5},
pages = {e1004351},
title = {{The Case for Junk DNA}},
url = {http://dx.plos.org/10.1371/journal.pgen.1004351},
volume = {10},
year = {2014}
}
@unpublished{Liu2014,
author = {Liu, Mingxia and Zhang, Daoqiang},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Liu, Zhang - 2014 - CGS A Novel Pairwise Constraint-Guided Sparse Feature Selection Method.pdf:pdf},
title = {{CGS: A Novel Pairwise Constraint-Guided Sparse Feature Selection Method}},
year = {2014}
}
@article{Cooper1992,
author = {Cooper, Gregory F. and Herskovits, Edward},
doi = {10.1007/BF00994110},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cooper, Herskovits - 1992 - A Bayesian method for the induction of probabilistic networks from data.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {bayesian belief networks,induction,machine learning,probabilistic networks},
month = oct,
number = {4},
pages = {309--347},
title = {{A Bayesian method for the induction of probabilistic networks from data}},
url = {http://link.springer.com/10.1007/BF00994110},
volume = {9},
year = {1992}
}
@book{Rothenberg1973,
author = {Rothenberg, Thomas J.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Rothenberg - 1973 - Efficient Estimation with A Priori Information.pdf:pdf},
publisher = {Yale University Press},
title = {{Efficient Estimation with A Priori Information}},
url = {http://www.getcited.org/pub/101421013},
year = {1973}
}
@article{Wang2007a,
author = {Wang, Junhui and Shen, Xiaotong and Pan, Wei},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wang, Shen, Pan - 2007 - On transductive support vector machines.pdf:pdf},
journal = {Contemporary Mathematics},
pages = {7--19},
title = {{On transductive support vector machines}},
url = {http://www.stat.umn.edu/~xshen/paper/tsvm.pdf http://books.google.com/books?hl=en\&lr=\&id=hXi\_rG3NELsC\&oi=fnd\&pg=PA7\&dq=On+transductive+support+vector+machines\&ots=GZGFVmVuKj\&sig=MgTezchQc8K2plQ7hXlKNmr4Itw},
volume = {443},
year = {2007}
}
@article{Foo2013,
archivePrefix = {arXiv},
arxivId = {0000.0000},
author = {Foo, Jasmine and Leder, Kevin},
doi = {10.1214/12-AAP876},
eprint = {0000.0000},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Foo, Leder - 2013 - Dynamics of cancer recurrence.pdf:pdf},
issn = {10505164},
journal = {Annals of Applied Probability},
keywords = {Branching processes,Cancer,Population genetics},
pages = {1437--1468},
title = {{Dynamics of cancer recurrence}},
volume = {23},
year = {2013}
}
@phdthesis{Mika2002,
author = {Mika, Sebastian},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Mika - 2002 - Kernel fisher discriminants.pdf:pdf},
title = {{Kernel fisher discriminants}},
url = {http://opus.kobv.de/tuberlin/volltexte/2003/477/},
year = {2002}
}
@article{Lee2004,
author = {Lee, Yoonkyung and Lin, Yi and Wahba, Grace},
doi = {10.1198/016214504000000098},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lee, Lin, Wahba - 2004 - Multicategory Support Vector Machines.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {generalized approximate cross-validation,method,nonparametric classi cation method,quadratic programming,regularization,reproducing kernel hilbert space},
month = mar,
number = {465},
pages = {67--81},
title = {{Multicategory Support Vector Machines}},
url = {http://www.tandfonline.com/doi/abs/10.1198/016214504000000098},
volume = {99},
year = {2004}
}
@article{Henmi2004,
author = {Henmi, Masayuki and Eguchi, Shinto},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Henmi, Eguchi - 2004 - A paradox concerning nuisance parameters and projected estimating functions.pdf:pdf},
journal = {Biometrika},
number = {4},
pages = {929--941},
title = {{A paradox concerning nuisance parameters and projected estimating functions}},
volume = {91},
year = {2004}
}
@techreport{Welling,
author = {Welling, Max},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Welling - Unknown - Kernel ridge Regression.pdf:pdf},
number = {3},
pages = {3--5},
title = {{Kernel ridge Regression}}
}
@book{Barber2012,
author = {Barber, David},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Barber - 2012 - Bayesian reasoning and machine learning.pdf:pdf},
title = {{Bayesian reasoning and machine learning}},
year = {2012}
}
@article{Lehmann1993,
author = {Lehmann, EL},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lehmann - 1993 - The Fisher, Neyman-Pearson theories of testing hypotheses One theory or two.pdf:pdf},
journal = {Journal of the American Statistical Association},
number = {424},
pages = {1242--1249},
title = {{The Fisher, Neyman-Pearson theories of testing hypotheses: One theory or two?}},
url = {http://www.jstor.org/stable/10.2307/2291263},
volume = {88},
year = {1993}
}
@article{Zhang2004,
author = {Zhang, P. and Peng, J.},
doi = {10.1109/ICPR.2004.1334050},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhang, Peng - 2004 - SVM vs regularized least squares classification.pdf:pdf},
isbn = {0-7695-2128-2},
journal = {Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004.},
number = {3},
pages = {176--179 Vol.1},
publisher = {Ieee},
title = {{SVM vs regularized least squares classification}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1334050},
year = {2004}
}
@unpublished{Berisha2014,
abstract = {Information divergence functions play a critical role in statistics and information theory. In this paper we introduce a divergence function between distributions and describe a number of properties that make it appealing for classification applications. Based on an extension of a multivariate two-sample test, we identify a nonparametric estimator of the divergence that does not impose strong assumptions on the data distribution. Furthermore, we show that this measure bounds the minimum binary classification error for the case when the training and test data are drawn from the same distribution and for the case where there exists some mismatch between training and test distributions. We confirm the theoretical results by designing feature selection algorithms using the criteria from these bounds and evaluating the algorithms on a series of pathological speech classification tasks.},
archivePrefix = {arXiv},
arxivId = {1412.6534},
author = {Berisha, Visar and Wisler, Alan and Hero, Alfred O. and Spanias, Andreas},
eprint = {1412.6534},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Berisha et al. - 2014 - Empirically Estimable Classification Bounds Based on a New Divergence Measure.pdf:pdf},
pages = {12},
title = {{Empirically Estimable Classification Bounds Based on a New Divergence Measure}},
url = {http://arxiv.org/abs/1412.6534},
year = {2014}
}
@inproceedings{Szummer2002,
author = {Szummer, Martin and Jaakkola, Tommi S.},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Szummer, Jaakkola - 2002 - Information regularization with partially labeled data.pdf:pdf},
pages = {1025--1032},
title = {{Information regularization with partially labeled data}},
url = {http://machinelearning.wustl.edu/mlpapers/paper\_files/AA69.pdf},
year = {2002}
}
@book{Kuncheva2004,
author = {Kuncheva, Ludmila I},
booktitle = {Methods and Algorithms. Wiley, Chichester},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kuncheva - 2004 - Combining Pattern Classifers.pdf:pdf},
isbn = {9786468600},
title = {{Combining Pattern Classifers}},
url = {http://www.tandfonline.com/doi/abs/10.1198/tech.2005.s320 http://scholar.google.com/scholar?hl=en\&btnG=Search\&q=intitle:Combining+Pattern+Classifiers\#3},
year = {2004}
}
@unpublished{Tan2013,
author = {Tan, Yimin and Zhu, X},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Tan, Zhu - 2013 - Dragging Density-Ratio Bagging.pdf:pdf},
pages = {1--10},
title = {{Dragging: Density-Ratio Bagging}},
url = {https://minds.wisconsin.edu/bitstream/handle/1793/65831/TR1795.pdf?sequence=1},
year = {2013}
}
@article{Fabius2014,
abstract = {In this paper we combine the strengths of RNNs and SGVB, creating the Variational Recurrent Auto-Encoder (VRAE). Such a model can be used for efficient, large scale unsupervised learning on time series data, mapping the time series data to a latent vector representation. The model is generative, such that data can be generated from samples of the latent space. An important contribution of this work is that the model can make use of unlabeled data in order to facilitate supervised training of RNNs by initialising the weights and network state.},
archivePrefix = {arXiv},
arxivId = {1412.6581},
author = {Fabius, Otto and van Amersfoort, Joost R. and Kingma, Diederik P.},
eprint = {1412.6581},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Fabius, van Amersfoort, Kingma - 2014 - Variational Recurrent Auto-Encoders.pdf:pdf},
month = dec,
number = {2013},
pages = {2013--2016},
title = {{Variational Recurrent Auto-Encoders}},
url = {http://arxiv.org/abs/1412.6581},
year = {2014}
}
@article{Yuille2003,
abstract = {The concave-convex procedure (CCCP) is a way to construct discrete-time iterative dynamical systems that are guaranteed to decrease global optimization and energy functions monotonically. This procedure can be applied to almost any optimization problem, and many existing algorithms can be interpreted in terms of it. In particular, we prove that all expectation-maximization algorithms and classes of Legendre minimization and variational bounding algorithms can be reexpressed in terms of CCCP. We show that many existing neural network and mean-field theory algorithms are also examples of CCCP. The generalized iterative scaling algorithm and Sinkhorn's algorithm can also be expressed as CCCP by changing variables. CCCP can be used both as a new way to understand, and prove the convergence of, existing optimization algorithms and as a procedure for generating new algorithms.},
author = {Yuille, a L and Rangarajan, Anand},
doi = {10.1162/08997660360581958},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Yuille, Rangarajan - 2003 - The concave-convex procedure.pdf:pdf},
issn = {0899-7667},
journal = {Neural computation},
keywords = {Algorithms,Energy Metabolism,Neural Networks (Computer)},
month = apr,
number = {4},
pages = {915--36},
pmid = {12689392},
title = {{The concave-convex procedure.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/12689392},
volume = {15},
year = {2003}
}
@article{Bollen,
archivePrefix = {arXiv},
arxivId = {arXiv:1010.3003v1},
author = {Bollen, Johan and Mao, Huina and Zeng, Xiao-jun},
eprint = {arXiv:1010.3003v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bollen, Mao, Zeng - Unknown - Twitter mood predicts the stock market .pdf:pdf},
pages = {1--8},
title = {{Twitter mood predicts the stock market .}}
}
@article{Samsudin2010,
author = {Samsudin, Noor a. and Bradley, Andrew P.},
doi = {10.1016/j.patcog.2010.05.010},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Samsudin, Bradley - 2010 - Nearest neighbour group-based classification.pdf:pdf},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Compound classification,Nearest neighbour,group-based classification},
month = oct,
number = {10},
pages = {3458--3467},
publisher = {Elsevier},
title = {{Nearest neighbour group-based classification}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0031320310002116},
volume = {43},
year = {2010}
}
@article{Lockhart2014c,
author = {Wasserman, Larry},
doi = {10.1214/13-AOS1175},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wasserman - 2014 - Discussion “a significance test for the lasso”.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
month = apr,
number = {2},
pages = {501--508},
title = {{Discussion: “a significance test for the lasso”}},
url = {http://projecteuclid.org/euclid.aos/1400592161},
volume = {42},
year = {2014}
}
@book{Brazdil2010,
author = {Brazdil, Pavel B. and Bernstein, Abraham},
editor = {Brazdil, Pavel and Bernstein, Abraham},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Brazdil, Bernstein - 2010 - Proceedings of 3rd Planning to Learn Workshop at ECAI 2010.pdf:pdf},
number = {Ecai},
title = {{Proceedings of 3rd Planning to Learn Workshop at ECAI 2010}},
year = {2010}
}
@article{Tibshirani2004,
abstract = {MOTIVATION: Early cancer detection has always been a major research focus in solid tumor oncology. Early tumor detection can theoretically result in lower stage tumors, more treatable diseases and ultimately higher cure rates with less treatment-related morbidities. Protein mass spectrometry is a potentially powerful tool for early cancer detection. We propose a novel method for sample classification from protein mass spectrometry data. When applied to spectra from both diseased and healthy patients, the 'peak probability contrast' technique provides a list of all common peaks among the spectra, their statistical significance and their relative importance in discriminating between the two groups. We illustrate the method on matrix-assisted laser desorption and ionization mass spectrometry data from a study of ovarian cancers. RESULTS: Compared to other statistical approaches for class prediction, the peak probability contrast method performs as well or better than several methods that require the full spectra, rather than just labelled peaks. It is also much more interpretable biologically. The peak probability contrast method is a potentially useful tool for sample classification from protein mass spectrometry data.},
author = {Tibshirani, Robert and Hastie, Trevor and Narasimhan, Balasubramanian and Soltys, Scott and Shi, Gongyi and Koong, Albert and Le, Quynh-Thu},
doi = {10.1093/bioinformatics/bth357},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Tibshirani et al. - 2004 - Sample classification from protein mass spectrometry, by 'peak probability contrasts'.pdf:pdf},
issn = {1367-4803},
journal = {Bioinformatics (Oxford, England)},
keywords = {Algorithms,Artificial Intelligence,Cluster Analysis,Diagnosis, Computer-Assisted,Diagnosis, Computer-Assisted: methods,Female,Humans,Models, Biological,Models, Statistical,Neoplasm Proteins,Neoplasm Proteins: blood,Neoplasm Proteins: classification,Ovarian Neoplasms,Ovarian Neoplasms: blood,Ovarian Neoplasms: classification,Ovarian Neoplasms: diagnosis,Reproducibility of Results,Sensitivity and Specificity,Spectrometry, Mass, Electrospray Ionization,Spectrometry, Mass, Electrospray Ionization: metho,Spectrometry, Mass, Matrix-Assisted Laser Desorpti,Tumor Markers, Biological,Tumor Markers, Biological: blood,Tumor Markers, Biological: classification},
month = nov,
number = {17},
pages = {3034--44},
pmid = {15226172},
title = {{Sample classification from protein mass spectrometry, by 'peak probability contrasts'.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15226172},
volume = {20},
year = {2004}
}
@article{Halevy2009,
author = {Halevy, Alon and Norvig, Peter and Pereira, Fernando},
doi = {10.1109/MIS.2009.36},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Halevy, Norvig, Pereira - 2009 - The Unreasonable Effectiveness of Data.pdf:pdf},
issn = {1541-1672},
journal = {IEEE Intelligent Systems},
month = mar,
number = {2},
pages = {8--12},
title = {{The Unreasonable Effectiveness of Data}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4804817},
volume = {24},
year = {2009}
}
@unpublished{Chena,
archivePrefix = {arXiv},
arxivId = {arXiv:0000.0000},
author = {Chen, Aiyou and Owen, Art B. and Shi, Minghui},
eprint = {arXiv:0000.0000},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Chen, Owen, Shi - Unknown - Data Enriched Linear Regression.pdf:pdf},
pages = {1--37},
title = {{Data Enriched Linear Regression}}
}
@inproceedings{Goldman2000,
author = {Goldman, Sally and Zhou, Yan},
booktitle = {Proceedings of the 17th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Goldman, Zhou - 2000 - Enhancing supervised learning with unlabeled data.pdf:pdf},
pages = {327--334},
title = {{Enhancing supervised learning with unlabeled data}},
url = {http://scholar.google.com/scholar?hl=en\&btnG=Search\&q=intitle:Enhancing+Supervised+Learning+with+Unlabeled+Data\#0},
volume = {3},
year = {2000}
}
@article{Dimitroff2014,
author = {Dimitroff, Georgi and Georgiev, Georgi and Toloşi, Laura and Popov, Borislav},
doi = {10.1007/s10994-014-5439-y},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Dimitroff et al. - 2014 - Efficient \$\$F\$\$ F measure maximization via weighted maximum likelihood.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
month = apr,
title = {{Efficient \$\$F\$\$ F measure maximization via weighted maximum likelihood}},
url = {http://link.springer.com/10.1007/s10994-014-5439-y},
year = {2014}
}
@article{Datar2004,
address = {New York, New York, USA},
author = {Datar, Mayur and Immorlica, Nicole and Indyk, Piotr and Mirrokni, Vahab S.},
doi = {10.1145/997817.997857},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Datar et al. - 2004 - Locality-sensitive hashing scheme based on p-stable distributions.pdf:pdf},
isbn = {1581138857},
journal = {Proceedings of the twentieth annual symposium on Computational geometry - SCG '04},
keywords = {-stable distributions,approximate nearest neighbor,locally sen-,sitive hashing,sublinear algorithm},
pages = {253},
publisher = {ACM Press},
title = {{Locality-sensitive hashing scheme based on p-stable distributions}},
url = {http://portal.acm.org/citation.cfm?doid=997817.997857},
year = {2004}
}
@article{Zhang2014,
author = {Zhang, Kai and Lan, Liang and Kwok, James T. and Vucetic, Slobodan and Parvin, Bahram},
doi = {10.1109/TNNLS.2014.2315526},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhang et al. - 2014 - Scaling Up Graph-Based Semisupervised Learning via Prototype Vector Machines.pdf:pdf},
issn = {2162-237X},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
pages = {1--1},
title = {{Scaling Up Graph-Based Semisupervised Learning via Prototype Vector Machines}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6803073},
year = {2014}
}
@inproceedings{Suzuki2008,
author = {Suzuki, Jun and Isozaki, Hideki},
booktitle = {ACL},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Suzuki, Isozaki - 2008 - Semi-Supervised Sequential Labeling and Segmentation Using Giga-Word Scale Unlabeled Data.pdf:pdf},
pages = {665--673},
title = {{Semi-Supervised Sequential Labeling and Segmentation Using Giga-Word Scale Unlabeled Data.}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.164.5597\&rep=rep1\&type=pdf},
year = {2008}
}
@book{Zhu2009,
author = {Zhu, Xiaojin and Goldberg, Andrew B.},
booktitle = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
doi = {10.2200/S00196ED1V01Y200906AIM006},
editor = {Brachman, Ronald J. and Dietterich, Thomas},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhu, Goldberg - 2009 - Introduction to Semi-Supervised Learning.pdf:pdf},
isbn = {9781598295474},
issn = {1939-4608},
number = {1},
pages = {1--130},
publisher = {Morgan \& Claypool},
title = {{Introduction to Semi-Supervised Learning}},
volume = {3},
year = {2009}
}
@article{Shah2013,
archivePrefix = {arXiv},
arxivId = {1308.1269},
author = {Shah, Rajen D. and Meinshausen, Nicolai},
eprint = {1308.1269},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Shah, Meinshausen - 2013 - Min-wise hashing for large-scale regression and classification with sparse data.pdf:pdf},
pages = {1--36},
title = {{Min-wise hashing for large-scale regression and classification with sparse data}},
url = {http://arxiv.org/abs/1308.1269},
year = {2013}
}
@article{Ye2007a,
address = {New York, New York, USA},
author = {Ye, Jieping},
doi = {10.1145/1273496.1273633},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ye - 2007 - Least squares linear discriminant analysis.pdf:pdf},
isbn = {9781595937933},
journal = {Proceedings of the 24th International Conference on Machine Learning},
keywords = {18,3,8,are linear combinations of,class separability,derived features in lda,dimension reduction,least squares,linear discriminant anal-,linear regression,the,the data achieves maximum,the orig-,ysis},
pages = {1087--1093},
publisher = {ACM Press},
title = {{Least squares linear discriminant analysis}},
url = {http://portal.acm.org/citation.cfm?doid=1273496.1273633},
year = {2007}
}
@article{Zhou2005,
author = {Zhou, Zhi-hua and Li, Ming},
doi = {10.1109/TKDE.2005.186},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhou, Li - 2005 - Tri-training exploiting unlabeled data using three classifiers.pdf:pdf},
issn = {1041-4347},
journal = {IEEE Transactions on Knowledge and Data Engineering},
month = nov,
number = {11},
pages = {1529--1541},
title = {{Tri-training: exploiting unlabeled data using three classifiers}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1512038},
volume = {17},
year = {2005}
}
@inproceedings{Ho1996,
author = {Ho, Tin Kam and Kleinberg, Eeugene M.},
booktitle = {International Conference on Pattern Recognition},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ho, Kleinberg - 1996 - Building projectable classifiers of arbitrary complexity.pdf:pdf},
pages = {880--885},
title = {{Building projectable classifiers of arbitrary complexity}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=547202},
volume = {2},
year = {1996}
}
@article{Gelman2013a,
author = {Gelman, Andrew},
doi = {10.1097/EDE.0b013e31827886f7},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gelman - 2013 - P values and statistical practice.pdf:pdf},
issn = {1531-5487},
journal = {Epidemiology},
month = jan,
number = {1},
pages = {69--72},
pmid = {23232612},
title = {{P values and statistical practice.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23232612},
volume = {24},
year = {2013}
}
@article{Fumera,
author = {Fumera, Giorgio and Roli, Fabio and Serrau, Alessandra},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Fumera, Roli, Serrau - Unknown - A Theoretical Analysis of Bagging as a Linear Combination of Classifiers.pdf:pdf},
pages = {1--19},
title = {{A Theoretical Analysis of Bagging as a Linear Combination of Classifiers}}
}
@article{Amini2002,
author = {Amini, Massih-Reza and Gallinari, Patrick},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Amini, Gallinari - 2002 - Semi-supervised logistic regression.pdf:pdf},
journal = {15th European Conference on Artificial Intelligence},
pages = {390--394},
title = {{Semi-supervised logistic regression}},
year = {2002}
}
@article{Witten2011,
abstract = {We consider the supervised classification setting, in which the data consist of p features measured on n observations, each of which belongs to one of K classes. Linear discriminant analysis (LDA) is a classical method for this problem. However, in the high-dimensional setting where p ≫ n, LDA is not appropriate for two reasons. First, the standard estimate for the within-class covariance matrix is singular, and so the usual discriminant rule cannot be applied. Second, when p is large, it is difficult to interpret the classification rule obtained from LDA, since it involves all p features. We propose penalized LDA, a general approach for penalizing the discriminant vectors in Fisher's discriminant problem in a way that leads to greater interpretability. The discriminant problem is not convex, so we use a minorization-maximization approach in order to efficiently optimize it when convex penalties are applied to the discriminant vectors. In particular, we consider the use of L(1) and fused lasso penalties. Our proposal is equivalent to recasting Fisher's discriminant problem as a biconvex problem. We evaluate the performances of the resulting methods on a simulation study, and on three gene expression data sets. We also survey past methods for extending LDA to the high-dimensional setting, and explore their relationships with our proposal.},
author = {Witten, Daniela M. and Tibshirani, Robert},
doi = {10.1111/j.1467-9868.2011.00783.x},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Witten, Tibshirani - 2011 - Penalized classification using Fisher's linear discriminant.pdf:pdf},
issn = {1369-7412},
journal = {Journal of the Royal Statistical Society. Series B, Statistical methodology},
keywords = {classification,discriminant analysis,feature selection,high dimensional problems,lasso,linear,supervised learning},
month = nov,
number = {5},
pages = {753--772},
pmid = {22323898},
title = {{Penalized classification using Fisher's linear discriminant.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3272679\&tool=pmcentrez\&rendertype=abstract},
volume = {73},
year = {2011}
}
@article{Weston2006,
address = {New York, New York, USA},
author = {Weston, Jason and Collobert, Ronan and Sinz, Fabian and Bottou, L\'{e}on and Vapnik, Vladimir},
doi = {10.1145/1143844.1143971},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Weston et al. - 2006 - Inference with the Universum.pdf:pdf},
isbn = {1595933832},
journal = {Proceedings of the 23rd international conference on Machine learning - ICML '06},
pages = {1009--1016},
publisher = {ACM Press},
title = {{Inference with the Universum}},
url = {http://portal.acm.org/citation.cfm?doid=1143844.1143971},
year = {2006}
}
@techreport{Bensusan2000,
author = {Bensusan, H. and Giraud-carrier, Christophe and Kennedy, C.J.},
booktitle = {ILP Work-in-progress \ldots},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bensusan, Giraud-carrier, Kennedy - 2000 - A Higher-order Approach to Meta-learning.pdf:pdf},
institution = {University of Bristol},
title = {{A Higher-order Approach to Meta-learning}},
url = {http://137.222.102.8/Publications/Papers/1000471.pdf},
year = {2000}
}
@article{Kuncheva2002,
author = {Kuncheva, Ludmila I},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kuncheva - 2002 - A Theoretical Study on Six Classifier Fusion Strategies.pdf:pdf},
journal = {IEEE transactions on pattern analysis and machine intelligence},
number = {2},
pages = {281--286},
title = {{A Theoretical Study on Six Classifier Fusion Strategies}},
volume = {24},
year = {2002}
}
@book{Kawakita2014,
author = {Kawakita, Masanori and Takeuchi, Jun’ichi},
booktitle = {Neural Networks},
doi = {10.1016/j.neunet.2014.01.016},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kawakita, Takeuchi - 2014 - Safe semi-supervised learning based on weighted likelihood.pdf:pdf},
isbn = {8192802361},
issn = {08936080},
month = jan,
publisher = {Elsevier Ltd},
title = {{Safe semi-supervised learning based on weighted likelihood}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0893608014000288},
year = {2014}
}
@article{Lockhart2014f,
author = {Lockhart, Richard and Taylor, Jonathan and Tibshirani, Ryan J. and Tibshirani, Robert},
doi = {10.1214/13-AOS1175},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lockhart et al. - 2014 - Rejoinder A significance test for the lasso.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
month = apr,
number = {2},
pages = {518--531},
title = {{Rejoinder: A significance test for the lasso}},
url = {http://projecteuclid.org/euclid.aos/1400592161},
volume = {42},
year = {2014}
}
@inproceedings{Druck2007,
author = {Druck, Gregory and Pal, Chris and McCallum, Andrew Kachites and Zhu, Xiaojin},
booktitle = {Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Druck et al. - 2007 - Semi-supervised classification with hybrid generativediscriminative methods.pdf:pdf},
isbn = {9781595936097},
keywords = {discriminative,hybrid generative,methods,semi-supervised learning,text classification},
pages = {280--289},
title = {{Semi-supervised classification with hybrid generative/discriminative methods}},
url = {http://dl.acm.org/citation.cfm?id=1281225},
year = {2007}
}
@article{Zhang2013,
author = {Zhang, Li},
doi = {10.1214/13-AOS1141},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhang - 2013 - Nearly optimal minimax estimator for high-dimensional sparse linear regression.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
keywords = {62J05, 62G20, 62C20, Minimax estimation, linear re},
month = aug,
number = {4},
pages = {2149--2175},
title = {{Nearly optimal minimax estimator for high-dimensional sparse linear regression}},
url = {http://projecteuclid.org/euclid.aos/1382547516},
volume = {41},
year = {2013}
}
@article{VonHippel2007,
author = {von Hippel, Paul T.},
doi = {10.1111/j.1467-9531.2007.00180.x},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/von Hippel - 2007 - Regression With Missing Ys an Improved Strategy for Analyzing Multiply Imputed Data.pdf:pdf},
issn = {0081-1750},
journal = {Sociological Methodology},
month = dec,
number = {1},
pages = {83--117},
title = {{Regression With Missing Ys: an Improved Strategy for Analyzing Multiply Imputed Data}},
url = {http://smx.sagepub.com/lookup/doi/10.1111/j.1467-9531.2007.00180.x},
volume = {37},
year = {2007}
}
@article{Lad2011,
abstract = {This article resolves a longstanding question in the axiomatisation of entropy as proposed by Shannon and highlighted in renewed concerns expressed by Jaynes. We introduce a companion measure of a probability distribution that we suggest be called the extropy of the distribution. The entropy and the extropy of an event distribution are identical. However, this identical measure bifurcates into distinct measures for any quantity that is not merely an event indicator. As for entropy, the maximum extropy distribution is also the uniform distribution. We display several theoretical and geometrical properties of the proposed extropy measure, discussing in detail the difference between its assessment of a refined probability distribution and the axiom that characterises the Shannon entropy in this regard. This is what resolves the concerns of Shannon and Jaynes. In a discrete context, the extropy measure is approximated by a variant of Gini's index of heterogeneity when the maximum probability mass is small. This is related to the "repeat rate" of a mass function as studied by Turing and Good. The continuous analogue of extropy turns out to equal the negative integral of the square of the density function. We conclude with a consideration of a rescaled measure of extropy which identifies it as the dual of entropy. The structure of the duality suggests a general theory of complementary distributions.},
archivePrefix = {arXiv},
arxivId = {arXiv:1109.6440v1},
author = {Lad, Frank and Sanfilippo, Giuseppe and Agr, Gianna},
eprint = {arXiv:1109.6440v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lad, Sanfilippo, Agr - 2011 - Extropy a complementary dual of entropy.pdf:pdf},
keywords = {duality,entropy,extropy,gini index of heterogeneity,proper scoring rules,repeat rate},
pages = {1--17},
title = {{Extropy : a complementary dual of entropy}},
year = {2011}
}
@inproceedings{Corduneanu2002,
author = {Corduneanu, Adrian and Jaakkola, Tommi},
booktitle = {Proceedings of the 19th conference on Uncertainty in Artificial Intelligence},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Corduneanu, Jaakkola - 2002 - On information regularization.pdf:pdf},
pages = {151--158},
title = {{On information regularization}},
url = {http://dl.acm.org/citation.cfm?id=2100602},
year = {2002}
}
@article{Loogc,
author = {Loog, M. and van Ginneken, B.},
doi = {10.1109/ICPR.2002.1048456},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Loog, van Ginneken - Unknown - Supervised segmentation by iterated contextual pixel classification.pdf:pdf},
isbn = {0-7695-1695-X},
journal = {Object recognition supported by user interaction for service robots},
pages = {925--928},
publisher = {IEEE Comput. Soc},
title = {{Supervised segmentation by iterated contextual pixel classification}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1048456},
volume = {2}
}
@inproceedings{Brefeld2006,
author = {Brefeld, Ulf and G\"{a}rtner, Thomas and Scheffer, Tobias and Wrobel, Stefan},
booktitle = {Proceedings of the 23rd International Conference on Machine learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Brefeld et al. - 2006 - Efficient co-regularised least squares regression.pdf:pdf},
pages = {137--144},
title = {{Efficient co-regularised least squares regression}},
url = {http://dl.acm.org/citation.cfm?id=1143862},
year = {2006}
}
@inproceedings{Carroll2007,
author = {Carroll, James L. and Seppi, Kevin D.},
booktitle = {IJCNN Workshop on Meta-Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Carroll, Seppi - 2007 - No-free-lunch and Bayesian optimality.pdf:pdf},
title = {{No-free-lunch and Bayesian optimality}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.142.7564\&rep=rep1\&type=pdf},
year = {2007}
}
@article{Wasserman,
author = {Wasserman, Larry},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wasserman - Unknown - Rise of the machines.pdf:pdf},
issn = {1743-9159},
month = aug,
title = {{Rise of the machines.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22226582}
}
@article{Gilovich1985,
author = {Gilovich, Thomas and Vallone, Robert and Tversky, Amos},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gilovich, Vallone, Tversky - 1985 - The Hot Hand in Basketball On the Misperception of Random Sequences.pdf:pdf},
journal = {Cognitive Psychology},
pages = {295--314},
title = {{The Hot Hand in Basketball: On the Misperception of Random Sequences}},
volume = {17},
year = {1985}
}
@article{Bartlett2003a,
author = {Bartlett, Peter L and Mendelson, S},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bartlett, Mendelson - 2003 - Rademacher and Gaussian complexities Risk bounds and structural results.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {data-dependent complexity,error bounds,maxi-,rademacher averages},
pages = {463--482},
title = {{Rademacher and Gaussian complexities: Risk bounds and structural results}},
url = {http://dl.acm.org/citation.cfm?id=944944},
volume = {3},
year = {2003}
}
@article{Shiao2014,
author = {Shiao, Han-Tai and Cherkassky, Vladimir},
doi = {10.1109/IJCNN.2014.6889517},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Shiao, Cherkassky - 2014 - Learning using privileged information (LUPI) for modeling survival data.pdf:pdf},
isbn = {978-1-4799-1484-5},
journal = {2014 International Joint Conference on Neural Networks (IJCNN)},
month = jul,
pages = {1042--1049},
publisher = {Ieee},
title = {{Learning using privileged information (LUPI) for modeling survival data}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6889517},
year = {2014}
}
@article{Shipp2001,
author = {Shipp, Catherine A. and Kuncheva, Ludmila I},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Shipp, Kuncheva - 2001 - Four Measures of Data Complexity for Bootstrapping, Splitting and Feature Sampling.pdf:pdf},
isbn = {0000000000000},
journal = {Proc. CIMA},
title = {{Four Measures of Data Complexity for Bootstrapping, Splitting and Feature Sampling}},
url = {http://www.bangor.ac.uk/~mas00a/papers/cslkAIDA01.pdf},
year = {2001}
}
@inproceedings{Widrow1960,
author = {Widrow, Bernard and Hoff, Marcian E.},
booktitle = {IRE WESCON Convention Record 4},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Widrow, Hoff - 1960 - Adaptive switching circuits.pdf:pdf},
pages = {96--104},
title = {{Adaptive switching circuits.}},
year = {1960}
}
@article{Browner1987,
abstract = {Just as diagnostic tests are most helpful in light of the clinical presentation, statistical tests are most useful in the context of scientific knowledge. Knowing the specificity and sensitivity of a diagnostic test is necessary, but insufficient: the clinician must also estimate the prior probability of the disease. In the same way, knowing the P value and power, or the confidence interval, for the results of a research study is necessary but insufficient: the reader must estimate the prior probability that the research hypothesis is true. Just as a positive diagnostic test does not mean that a patient has the disease, especially if the clinical picture suggests otherwise, a significant P value does not mean that a research hypothesis is correct, especially if it is inconsistent with current knowledge. Powerful studies are like sensitive tests in that they can be especially useful when the results are negative. Very low P values are like very specific tests; both result in few false-positive results due to chance. This Bayesian approach can clarify much of the confusion surrounding the use and interpretation of statistical tests.},
author = {Browner, W S and Newman, T B},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Browner, Newman - 1987 - Are all significant P values created equal The analogy between diagnostic tests and clinical research.pdf:pdf},
issn = {0098-7484},
journal = {JAMA : the journal of the American Medical Association},
keywords = {Bayes Theorem,Predictive Value of Tests,Research,Statistics as Topic},
month = may,
number = {18},
pages = {2459--63},
pmid = {3573245},
title = {{Are all significant P values created equal? The analogy between diagnostic tests and clinical research.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/3573245},
volume = {257},
year = {1987}
}
@article{Dai2014,
abstract = {We propose the supervised hierarchical Dirichlet process (sHDP), a nonparametric generative model for the joint distribution of a group of observations and a response variable directly associated with that whole group. We compare the sHDP with another leading method for regression on grouped data, the supervised latent Dirichlet allocation (sLDA) model. We evaluate our method on two real-world classification problems and two real-world regression problems. Bayesian nonparametric regression models based on the Dirichlet process, such as the Dirichlet process-generalised linear models (DP-GLM) have previously been explored; these models allow flexibility in modelling nonlinear relationships. However, until now, Hierarchical Dirichlet Process (HDP) mixtures have not seen significant use in supervised problems with grouped data since a straightforward application of the HDP on the grouped data results in learnt clusters that are not predictive of the responses. The sHDP solves this problem by allowing for clusters to be learnt jointly from the group structure and from the label assigned to each group.},
author = {Dai, Andrew and Storkey, Amos J.},
doi = {10.1109/TPAMI.2014.2315802},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Dai, Storkey - 2014 - The supervised hierarchical Dirichlet process.pdf:pdf},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Adaptation models,Analytical models,Bayes methods,Data models,Predictive models,Resource management,Vocabulary},
number = {2},
pages = {1--1},
title = {{The supervised hierarchical Dirichlet process}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6784083},
volume = {PP},
year = {2014}
}
@inproceedings{Sokolovska2008,
address = {Helsinki, Finland},
author = {Sokolovska, Nataliya and Capp\'{e}, Olivier and Yvon, Francois},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
editor = {Cohen, William W. and McCallum, Andrew and Roweis, Sam T.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sokolovska, Capp\'{e}, Yvon - 2008 - The asymptotics of semi-supervised learning in discriminative probabilistic models.pdf:pdf},
pages = {984--991},
publisher = {ACM Press},
title = {{The asymptotics of semi-supervised learning in discriminative probabilistic models}},
url = {http://dl.acm.org/citation.cfm?id=1390280},
year = {2008}
}
@article{Schafer2002,
author = {Schafer, Joseph L. and Graham, John W.},
doi = {10.1037//1082-989X.7.2.147},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Schafer, Graham - 2002 - Missing data Our view of the state of the art.pdf:pdf},
issn = {1082-989X},
journal = {Psychological Methods},
number = {2},
pages = {147--177},
title = {{Missing data: Our view of the state of the art.}},
url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/1082-989X.7.2.147},
volume = {7},
year = {2002}
}
@article{Green1992,
archivePrefix = {arXiv},
arxivId = {arXiv:1502.01148v1},
author = {Green, Peter J and Latuszy, Krzysztof and Pereyra, Marcelo and Robert, Christian P and Feb, C O},
eprint = {arXiv:1502.01148v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Green et al. - 1992 - Sampling Backwards and Forwards.pdf:pdf},
keywords = {1,abc techniques,and,at the university of,bayesian analysis,bristol,d063485,ep,epsrc grant,i-,mcmc algorithms,optimisation,supported in part by,sustain},
number = {Metropolis 1987},
title = {{Sampling Backwards and Forwards}},
year = {1992}
}
@inproceedings{Shaffer1994,
author = {Schaffer, Cullen},
booktitle = {International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Schaffer - 1994 - A conservation law for generalization performance.pdf:pdf},
title = {{A conservation law for generalization performance}},
url = {http://dml.cs.byu.edu/~cgc/docs/mldm\_tools/Reading/LCG.pdf},
year = {1994}
}
@article{VanRooden2011,
abstract = {The clinical heterogeneity of Parkinson's disease (PD) may point at the existence of subtypes. Because subtypes likely reflect distinct underlying etiologies, their identification may facilitate future genetic and pharmacotherapeutic studies. Aim of this study was to identify subtypes by a data-driven approach applied to a broad spectrum of motor and nonmotor features of PD. Data of motor and nonmotor PD symptoms were collected in 802 patients in two different European prevalent cohorts. A model-based cluster analysis was conducted on baseline data of 344 patients of a Dutch cohort (PROPARK). Reproducibility of these results was tested in data of the second annual assessment of the same cohort and validated in an independent Spanish cohort (ELEP) of 357 patients. The subtypes were subsequently characterized on clinical and demographic variables. Four similar PD subtypes were identified in two different populations and are largely characterized by differences in the severity of nondopaminergic features and motor complications: Subtype 1 was mildly affected in all domains, Subtype 2 was predominantly characterized by severe motor complications, Subtype 3 was affected mainly on nondopaminergic domains without prominent motor complications, while Subtype 4 was severely affected on all domains. The subtypes had largely similar mean disease durations (nonsignificant differences between three clusters) but showed considerable differences with respect to their association with demographic and clinical variables. In prevalent disease, PD subtypes are largely characterized by the severity of nondopaminergic features and motor complications and likely reflect complex interactions between disease mechanisms, treatment, aging, and gender.},
author = {van Rooden, Stephanie M and Colas, Fabrice P. R. and Mart\'{\i}nez-Mart\'{\i}n, Pablo and Visser, Martine and Verbaan, Dagmar and Marinus, Johan and Chaudhuri, Ray K and Kok, Joost N and van Hilten, Jacobus J},
doi = {10.1002/mds.23346},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/van Rooden et al. - 2011 - Clinical subtypes of Parkinson's disease.pdf:pdf},
issn = {1531-8257},
journal = {Movement disorders : official journal of the Movement Disorder Society},
keywords = {Aged,Cluster Analysis,Cohort Studies,Disease Progression,Female,Germany,Humans,Male,Middle Aged,Neurologic Examination,Parkinson Disease,Parkinson Disease: classification,Parkinson Disease: physiopathology,Reproducibility of Results,Spain,Time Factors},
month = jan,
number = {1},
pages = {51--8},
pmid = {21322019},
title = {{Clinical subtypes of Parkinson's disease.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21322019},
volume = {26},
year = {2011}
}
@article{Friedman2001,
author = {Friedman, Jerome H.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Friedman - 2001 - Greedy Function Approximation A Gradient Boosting Machine.pdf:pdf},
journal = {The Annals of Statistics},
number = {5},
pages = {1189--1232},
title = {{Greedy Function Approximation: A Gradient Boosting Machine}},
url = {http://home.olemiss.edu/~xdang/676/Greedy\_function\_approximation\_a\_gradient\_bossting\_machine.pdf http://scholar.google.com/scholar?hl=en\&btnG=Search\&q=intitle:Greedy+Function+Approximation:+A+Gradient+Boosting+Machine\#3},
volume = {29},
year = {2001}
}
@article{Boyd2013,
author = {Boyd, Kendrick and Eng, Kevin H. and Page, C. David},
doi = {10.1007/978-3-642-40994-3\_29},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Boyd, Eng, Page - 2013 - Area under the precision-recall curve Point estimates and confidence intervals.pdf:pdf},
isbn = {9783642409936},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {451--466},
title = {{Area under the precision-recall curve: Point estimates and confidence intervals}},
volume = {8190 LNAI},
year = {2013}
}
@article{Meding2012,
abstract = {In clinical diagnostics, it is of outmost importance to correctly identify the source of a metastatic tumor, especially if no apparent primary tumor is present. Tissue-based proteomics might allow correct tumor classification. As a result, we performed MALDI imaging to generate proteomic signatures for different tumors. These signatures were used to classify common cancer types. At first, a cohort comprised of tissue samples from six adenocarcinoma entities located at different organ sites (esophagus, breast, colon, liver, stomach, thyroid gland, n = 171) was classified using two algorithms for a training and test set. For the test set, Support Vector Machine and Random Forest yielded overall accuracies of 82.74 and 81.18\%, respectively. Then, colon cancer liver metastasis samples (n = 19) were introduced into the classification. The liver metastasis samples could be discriminated with high accuracy from primary tumors of colon cancer and hepatocellular carcinoma. Additionally, colon cancer liver metastasis samples could be successfully classified by using colon cancer primary tumor samples for the training of the classifier. These findings demonstrate that MALDI imaging-derived proteomic classifiers can discriminate between different tumor types at different organ sites and in the same site.},
author = {Meding, Stephan and Nitsche, Ulrich and Balluff, Benjamin and Elsner, Mareike and Rauser, Sandra and Sch\"{o}ne, C\'{e}drik and Nipp, Martin and Maak, Matthias and Feith, Marcus and Ebert, Matthias P and Friess, Helmut and Langer, Rupert and H\"{o}fler, Heinz and Zitzelsberger, Horst and Rosenberg, Robert and Walch, Axel},
doi = {10.1021/pr200784p},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Meding et al. - 2012 - Tumor classification of six common cancer types based on proteomic profiling by MALDI imaging.pdf:pdf},
issn = {1535-3907},
journal = {Journal of proteome research},
keywords = {Adenocarcinoma,Adenocarcinoma: metabolism,Adenocarcinoma: secondary,Algorithms,Humans,Neoplasms,Neoplasms: diagnosis,Neoplasms: metabolism,Neoplasms: pathology,Proteome,Proteome: metabolism,Proteomics,Sensitivity and Specificity,Spectrometry, Mass, Matrix-Assisted Laser Desorpti,Support Vector Machines},
month = mar,
number = {3},
pages = {1996--2003},
pmid = {22224404},
title = {{Tumor classification of six common cancer types based on proteomic profiling by MALDI imaging.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22224404},
volume = {11},
year = {2012}
}
@article{Li2013,
author = {Li, YF and Tsang, IW and Kwok, JT and Zhou, ZH},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Li et al. - 2013 - Convex and Scalable Weakly Labeled SVMs.pdf:pdf},
journal = {Journal of Machine Learning Research},
pages = {2151--2188},
title = {{Convex and Scalable Weakly Labeled SVMs}},
url = {http://arxiv.org/abs/1303.1271},
volume = {14},
year = {2013}
}
@book{Quinonero-Candela2009,
author = {Quinonero-Candela, Joaquin and Sugiyama, Masashi and Schwaighofer, Anton and Lawrence, Neil D.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Quinonero-Candela et al. - 2009 - Dataset shift in machine learning.pdf:pdf},
isbn = {9780262170055},
title = {{Dataset shift in machine learning}},
url = {http://dl.acm.org/citation.cfm?id=1462129},
year = {2009}
}
@book{Lehmann1998,
author = {Lehmann, E. L. and Casella, G.},
publisher = {Springer-Verlag},
title = {{Theory of Point Estimation}},
year = {1998}
}
@article{Dietterich1997a,
abstract = {The multiple instance problem arises in tasks where the training examples are ambiguous: a single example object may have many alternative feature vectors (instances) that describe it, and yet only one of those feature vectors may be responsible for the observed classification of the object. This paper describes and compares three kinds of algorithms that learn axis-parallel rectangles to solve the multiple instance problem. Algorithms that ignore the multiple instance problem perform very poorly. An algorithm that directly confronts the multiple instance problem (by attempting to identify which feature vectors are responsible for the observed classifications) performs best, giving 89\% correct predictions on a musk odor prediction task. The paper also illustrates the use of artificial data to debug and compare these algorithms.},
archivePrefix = {arXiv},
arxivId = {10.1016/S0004},
author = {Dietterich, T},
doi = {10.1016/S0004-3702(96)00034-3},
eprint = {S0004},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Dietterich - 1997 - Solving the multiple instance problem with axis-parallel rectangles.pdf:pdf},
isbn = {0004-3702},
issn = {00043702},
journal = {Artificial Intelligence},
keywords = {machine},
number = {1-2},
pages = {31--71},
primaryClass = {10.1016},
title = {{Solving the multiple instance problem with axis-parallel rectangles}},
volume = {89},
year = {1997}
}
@article{Henmi2007,
author = {Henmi, Masayuki and Yoshida, Ryo and Eguchi, Shinto},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Henmi, Yoshida, Eguchi - 2007 - Importance sampling via the estimated sampler.pdf:pdf},
journal = {Biometrika},
number = {4},
pages = {985--991},
title = {{Importance sampling via the estimated sampler}},
volume = {94},
year = {2007}
}
@unpublished{Looga,
author = {Loog, Marco and Jensen, Are C},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Loog, Jensen - Unknown - A Constrained Log-Likelihood Formulation for Semi-Supervised Nearest Mean Classification.pdf:pdf},
keywords = {constrained estimation,log-likelihood,nearest mean classifier,semi-supervised learning},
number = {1},
title = {{A Constrained Log-Likelihood Formulation for Semi-Supervised Nearest Mean Classification}},
volume = {1}
}
@article{Buhlmann2014a,
author = {Buhlmann, Peter and Meier, Lukas and van de Geer, Sara},
doi = {10.1214/13-AOS1175},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Buhlmann, Meier, van de Geer - 2014 - Discussion “a significance test for the lasso”.pdf:pdf},
journal = {The Annals of Statistics},
number = {2},
pages = {469--477},
title = {{Discussion: “a significance test for the lasso”}},
volume = {42},
year = {2014}
}
@inproceedings{Ogawa2013,
author = {Ogawa, Kohei and Imamura, Motoki and Takeuchi, Ichiro and Sugiyama, Masashi},
booktitle = {Proceedings of the 30th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ogawa et al. - 2013 - Infinitesimal Annealing for Training Semi-Supervised Support Vector Machines.pdf:pdf},
pages = {897--905},
title = {{Infinitesimal Annealing for Training Semi-Supervised Support Vector Machines}},
url = {http://sugiyama-www.cs.titech.ac.jp/~sugi/2013/ICML2013b.pdf},
year = {2013}
}
@article{Rifkin2003,
author = {Rifkin, Ryan and Yeo, Gene and Poggio, Tomaso},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Rifkin, Yeo, Poggio - 2003 - Regularized least-squares classification.pdf:pdf},
journal = {Nato Science Series Sub Series III Computer and Systems Sciences 190},
title = {{Regularized least-squares classification}},
year = {2003}
}
@article{Poggio2003,
author = {Poggio, Tomaso and Smale, Steve},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Poggio, Smale - 2003 - The Mathematics of Learning Dealing with Data.pdf:pdf},
journal = {Notices of the AMS},
pages = {537--544},
title = {{The Mathematics of Learning: Dealing with Data}},
year = {2003}
}
@unpublished{Savov,
author = {Savov, Ivan},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Savov - Unknown - Linear algebra explained in four pages.pdf:pdf},
pages = {1--4},
title = {{Linear algebra explained in four pages}}
}
@article{Zhang2013a,
author = {Zhang, Q. and Qian, P. Z. G.},
doi = {10.1093/biomet/ast034},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhang, Qian - 2013 - Designs for crossvalidating approximation models.pdf:pdf},
issn = {0006-3444},
journal = {Biometrika},
month = sep,
number = {4},
pages = {997--1004},
title = {{Designs for crossvalidating approximation models}},
url = {http://biomet.oxfordjournals.org/cgi/doi/10.1093/biomet/ast034},
volume = {100},
year = {2013}
}
@inproceedings{Graepel2013,
author = {Graepel, Thore and Lauter, Kristin and Naehrig, Michael},
booktitle = {Information Security and Cryptology},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Graepel, Lauter, Naehrig - 2013 - ML confidential Machine learning on encrypted data.pdf:pdf},
title = {{ML confidential: Machine learning on encrypted data}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-37682-5\_1},
year = {2013}
}
@article{Cheplygina2010,
author = {Cheplygina, Veronika},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cheplygina - 2010 - Random Subspace Method for One-Class Classifiers.pdf:pdf},
title = {{Random Subspace Method for One-Class Classifiers}},
year = {2010}
}
@article{Snijders1991,
author = {Snijders, Tom A.B.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Snijders - 1991 - Enumeration and simulation methods for 0–1 matrices with given marginals.pdf:pdf},
journal = {Psychometrika},
keywords = {0-1 matrices with given,adjacency matrices,an important type of,ecology,in the study of,introduction,marginals,monte carlo methods,networks,observation is relational data,random digraphs,reciprocity,social networks,unequal probability sampling},
number = {3},
pages = {397--417},
title = {{Enumeration and simulation methods for 0–1 matrices with given marginals}},
url = {http://www.springerlink.com/index/B3224G6274141M68.pdf},
volume = {56},
year = {1991}
}
@inproceedings{Jordan2002,
abstract = {We compare discriminative and generative learning as typified by logistic regression and naive Bayes. We show, contrary to a widely held belief that discriminative classifiers are almost always to be preferred, that there can often be two distinct regimes of performance as the training set size is increased, one in which each algorithm does better. This stems from the observation - which is borne out in repeated experiments - that while discriminative learning has lower asymptotic error, a generative classifier may also approach its (higher) asymptotic error much faster.},
author = {Jordan, Michael I. and Ng, Andrew Y},
booktitle = {Advances in neural information processing systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Jordan, Ng - 2002 - On Discriminative vs. Generative classifiers comparison of logistic regression and naive Bayes.pdf:pdf},
number = {14},
pages = {841--848},
title = {{On Discriminative vs. Generative classifiers: comparison of logistic regression and naive Bayes}},
url = {http://books.google.com/books?hl=en\&lr=\&id=GbC8cqxGR7YC\&oi=fnd\&pg=PA841\&dq=On+Discriminative+vs.+Generative+classifiers:+comparison+of+logistic+regression+and+naive+Bayes\&ots=ZvO0F2\_vx9\&sig=0nMLd-CWMsb8-jyrI6YetIH6ZZU},
volume = {2},
year = {2002}
}
@article{Wasserman2009,
author = {Wasserman, Larry and Roeder, Kathryn},
doi = {10.1214/08-AOS646},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wasserman, Roeder - 2009 - High dimensional variable selection.pdf:pdf},
journal = {Annals of statistics},
number = {5},
pages = {2178--2201},
title = {{High dimensional variable selection}},
url = {http://www.ncbi.nlm.nih.gov/pmc/articles/pmc2752029/},
volume = {37},
year = {2009}
}
@article{Saeys2007,
abstract = {Feature selection techniques have become an apparent need in many bioinformatics applications. In addition to the large pool of techniques that have already been developed in the machine learning and data mining fields, specific applications in bioinformatics have led to a wealth of newly proposed techniques. In this article, we make the interested reader aware of the possibilities of feature selection, providing a basic taxonomy of feature selection techniques, and discussing their use, variety and potential in a number of both common as well as upcoming bioinformatics applications.},
author = {Saeys, Yvan and Inza, I\~{n}aki and Larra\~{n}aga, Pedro},
doi = {10.1093/bioinformatics/btm344},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Saeys, Inza, Larra\~{n}aga - 2007 - A review of feature selection techniques in bioinformatics.pdf:pdf},
isbn = {1367-4811 (Electronic)$\backslash$r1367-4803 (Linking)},
issn = {13674803},
journal = {Bioinformatics},
number = {19},
pages = {2507--2517},
pmid = {17720704},
title = {{A review of feature selection techniques in bioinformatics}},
volume = {23},
year = {2007}
}
@inproceedings{Lafferty2007,
author = {Lafferty, John D. and Wasserman, Larry},
booktitle = {Advances in Neural Information Processing Systems 20},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lafferty, Wasserman - 2007 - Statistical analysis of semi-supervised regression.pdf:pdf},
pages = {801----808},
title = {{Statistical analysis of semi-supervised regression}},
year = {2007}
}
@article{Sotoca2006,
author = {Sotoca, J M and Mollineda, R A and Sanchez, J.S.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sotoca, Mollineda, Sanchez - 2006 - A meta-learning framework for pattern classification by means of data complexity measures.pdf:pdf},
journal = {Inteligencia artificial: Revista Iberoamericana de Inteligencia Artificial},
keywords = {classification,data complexity,feature selection,meta-learning,prototype selection},
number = {29},
pages = {31--38},
title = {{A meta-learning framework for pattern classification by means of data complexity measures}},
volume = {10},
year = {2006}
}
@inproceedings{Sørensen2010,
author = {S\o rensen, Lauge and Loog, Marco and Tax, DMJ},
booktitle = {Structural, Syntactic, and Statistical Pattern Recognition},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/S\o rensen, Loog, Tax - 2010 - Dissimilarity-based multiple instance learning.pdf:pdf},
keywords = {bag,dissimilarity measure,dissimilarity representation,multiple instance learning},
pages = {129--138},
title = {{Dissimilarity-based multiple instance learning}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-14980-1\_12},
year = {2010}
}
@book{Casella2002,
author = {Casella, George and Berger, Roger L.},
edition = {Second},
title = {{Statistical Inference}},
year = {2002}
}
@inproceedings{Sa1994,
author = {Sa, Virginia R De},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sa - 1994 - Learning Classification with Unlabeled Data.pdf:pdf},
pages = {112--112},
title = {{Learning Classification with Unlabeled Data}},
year = {1994}
}
@article{Hanselmann2013,
abstract = {Digital staining for the automated annotation of mass spectrometry imaging (MSI) data has previously been achieved using state-of-the-art classifiers such as random forests or support vector machines (SVMs). However, the training of such classifiers requires an expert to label exemplary data in advance. This process is time-consuming and hence costly, especially if the tissue is heterogeneous. In theory, it may be sufficient to only label a few highly representative pixels of an MS image, but it is not known a priori which pixels to select. This motivates active learning strategies in which the algorithm itself queries the expert by automatically suggesting promising candidate pixels of an MS image for labeling. Given a suitable querying strategy, the number of required training labels can be significantly reduced while maintaining classification accuracy. In this work, we propose active learning for convenient annotation of MSI data. We generalize a recently proposed active learning method to the multiclass case and combine it with the random forest classifier. Its superior performance over random sampling is demonstrated on secondary ion mass spectrometry data, making it an interesting approach for the classification of MS images.},
author = {Hanselmann, Michael and R\"{o}der, Jens and K\"{o}the, Ullrich and Renard, Bernhard Y and Heeren, Ron M.A. and Hamprecht, Fred A.},
doi = {10.1021/ac3023313},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hanselmann et al. - 2013 - Active learning for convenient annotation and classification of secondary ion mass spectrometry images.pdf:pdf},
issn = {1520-6882},
journal = {Analytical Chemistry},
month = jan,
number = {1},
pages = {147--55},
pmid = {23157438},
title = {{Active learning for convenient annotation and classification of secondary ion mass spectrometry images.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23157438},
volume = {85},
year = {2013}
}
@article{Bickel2009,
author = {Bickel, Steffen and Br\"{u}ckner, Michael and Scheffer, Tobias},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bickel, Br\"{u}ckner, Scheffer - 2009 - Discriminative learning under covariate shift.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {covariate shift,discriminative learning,transfer learning},
pages = {2137--2155},
title = {{Discriminative learning under covariate shift}},
url = {http://dl.acm.org/citation.cfm?id=1755858},
volume = {10},
year = {2009}
}
@article{Kleinberg2000,
author = {Kleinberg, E.M.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kleinberg - 2000 - On the algorithmic implementation of stochastic discrimination.pdf:pdf},
journal = {IEEE transactions on pattern analysis and machine intelligence},
number = {5},
pages = {473--490},
publisher = {IEEE},
title = {{On the algorithmic implementation of stochastic discrimination}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=857004},
volume = {22},
year = {2000}
}
@article{Uh2012,
author = {Uh, H and Mertens, B J A and Beekman, M.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Uh, Mertens, Beekman - 2012 - Search for multiple biomarkers predicting longevity in families weighted penalized logistic regression.pdf:pdf},
pages = {1--22},
title = {{Search for multiple biomarkers predicting longevity in families : weighted penalized logistic regression}},
year = {2012}
}
@article{McLachlan1975,
author = {McLachlan, Geoffrey J.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/McLachlan - 1975 - Iterative Reclassification Procedure for Constructing an Asymptotically Optimal Rule of Allocation in Discriminant An.pdf:pdf},
journal = {Journal of the American Statistical Association},
number = {350},
pages = {365--369},
title = {{Iterative Reclassification Procedure for Constructing an Asymptotically Optimal Rule of Allocation in Discriminant Analysis}},
volume = {70},
year = {1975}
}
@article{Claassen2010,
author = {Claassen, Tom and Heskes, Tom},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Claassen, Heskes - 2010 - Learning causal network structure from multiple (in) dependence models.pdf:pdf},
journal = {Proceedings of the Fifth European Workshop on Probabilistic Graphical Models},
title = {{Learning causal network structure from multiple (in) dependence models}},
year = {2010}
}
@article{Zach2014,
abstract = {In this work, we present a unified view on Markov random fields (MRFs) and recently proposed continuous tight convex relaxations for multilabel assignment in the image plane. These relaxations are far less biased toward the grid geometry than Markov random fields on grids. It turns out that the continuous methods are nonlinear extensions of the well-established local polytope MRF relaxation. In view of this result, a better understanding of these tight convex relaxations in the discrete setting is obtained. Further, a wider range of optimization methods is now applicable to find a minimizer of the tight formulation. We propose two methods to improve the efficiency of minimization. One uses a weaker, but more efficient continuously inspired approach as initialization and gradually refines the energy where it is necessary. The other one reformulates the dual energy enabling smooth approximations to be used for efficient optimization. We demonstrate the utility of our proposed minimization schemes in numerical experiments. Finally, we generalize the underlying energy formulation from isotropic metric smoothness costs to arbitrary nonmetric and orientation dependent smoothness terms.},
author = {Zach, Christopher and H\"{a}ne, Christian and Pollefeys, Marc},
doi = {10.1109/TPAMI.2013.105},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zach, H\"{a}ne, Pollefeys - 2014 - What Is Optimized in Convex Relaxations for Multilabel Problems Connecting Discrete and Continuously Ins.pdf:pdf},
issn = {1939-3539},
journal = {IEEE transactions on pattern analysis and machine intelligence},
month = jan,
number = {1},
pages = {157--70},
pmid = {24231873},
title = {{What Is Optimized in Convex Relaxations for Multilabel Problems: Connecting Discrete and Continuously Inspired MAP Inference.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24231873},
volume = {36},
year = {2014}
}
@article{Zhou2012,
archivePrefix = {arXiv},
arxivId = {arXiv:1201.3571v1},
author = {Zhou, Hua and Wu, Yichao},
eprint = {arXiv:1201.3571v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhou, Wu - 2012 - A Generic Path Algorithm for Regularized Statistical Estimation.pdf:pdf},
keywords = {gaussian graphical model,generalized linear model,lasso,log-concave density estimation,ordinary differential equations,quasi-likelihoods,regularization,shape restricted regression,solution path},
number = {1},
pages = {1--28},
title = {{A Generic Path Algorithm for Regularized Statistical Estimation}},
year = {2012}
}
@article{Ioannidis2005,
abstract = {There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.},
author = {Ioannidis, John P.A.},
doi = {10.1371/journal.pmed.0020124},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ioannidis - 2005 - Why most published research findings are false.pdf:pdf},
issn = {1549-1676},
journal = {PLoS medicine},
keywords = {Bias (Epidemiology),Data Interpretation,Likelihood Functions,Meta-Analysis as Topic,Odds Ratio,Publishing,Reproducibility of Results,Research Design,Sample Size,Statistical},
month = aug,
number = {8},
pages = {e124},
pmid = {16060722},
title = {{Why most published research findings are false.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=1182327\&tool=pmcentrez\&rendertype=abstract},
volume = {2},
year = {2005}
}
@article{Buhlmann2014,
author = {B\"{u}hlmann, Peter},
doi = {10.1214/13-STS460},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/B\"{u}hlmann - 2014 - Discussion of Big Bayes Stories and BayesBag.pdf:pdf},
issn = {0883-4237},
journal = {Statistical Science},
month = feb,
number = {1},
pages = {91--94},
title = {{Discussion of Big Bayes Stories and BayesBag}},
url = {http://projecteuclid.org/euclid.ss/1399645732},
volume = {29},
year = {2014}
}
@article{Webb1996,
author = {Webb, Geoffrey I.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Webb - 1996 - Further Experimental Evidence against the Utility of Occam's Razor.pdf:pdf},
journal = {Journal of Artificial Intelligence Research},
pages = {397--417},
title = {{Further Experimental Evidence against the Utility of Occam's Razor}},
url = {http://arxiv.org/abs/cs/9605101},
volume = {4},
year = {1996}
}
@article{Bochkina2014,
author = {Bochkina, Natalia a. and Green, Peter J.},
doi = {10.1214/14-AOS1239},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bochkina, Green - 2014 - The Bernstein–von Mises theorem and nonregular models.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
month = oct,
number = {5},
pages = {1850--1878},
title = {{The Bernstein–von Mises theorem and nonregular models}},
url = {http://projecteuclid.org/euclid.aos/1410440627},
volume = {42},
year = {2014}
}
@article{Patel2015,
abstract = {A grand challenge in machine learning is the development of computational al-gorithms that match or outperform humans in perceptual inference tasks such as visual object and speech recognition. The key factor complicating such tasks is the presence of numerous nuisance variables, for instance, the unknown object position, orientation, and scale in object recognition or the unknown voice pronunciation, pitch, and speed in speech recognition. Recently, a new breed of deep learning algorithms have emerged for high-nuisance inference tasks; they are constructed from many layers of alternating linear and nonlin-ear processing units and are trained using large-scale algorithms and massive amounts of training data. The recent success of deep learning systems is im-pressive — they now routinely yield pattern recognition systems with near-or super-human capabilities — but a fundamental question remains: Why do they work? Intuitions abound, but a coherent framework for understanding, analyzing, and synthesizing deep learning architectures has remained elusive. We answer this question by developing a new probabilistic framework for deep learning based on a Bayesian generative probabilistic model that explicitly cap-tures variation due to nuisance variables. The graphical structure of the model enables it to be learned from data using classical expectation-maximization techniques. Furthermore, by relaxing the generative model to a discriminative one, we can recover two of the current leading deep learning systems, deep convolutional neural networks (DCNs) and random decision forests (RDFs), providing insights into their successes and shortcomings as well as a princi-pled route to their improvement.},
archivePrefix = {arXiv},
arxivId = {arXiv:1504.00641v1},
author = {Patel, Ankit B and Nguyen, Tan and Baraniuk, Richard G},
eprint = {arXiv:1504.00641v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Patel, Nguyen, Baraniuk - 2015 - A Probabilistic Theory of Deep Learning.pdf:pdf},
pages = {1--56},
title = {{A Probabilistic Theory of Deep Learning}},
year = {2015}
}
@article{Pitt1988,
author = {Pitt, Leonard and Valiant, Leslie G.},
doi = {10.1145/48014.63140},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Pitt, Valiant - 1988 - Computational limitations on learning from examples.pdf:pdf},
issn = {00045411},
journal = {Journal of the ACM},
month = oct,
number = {4},
pages = {965--984},
title = {{Computational limitations on learning from examples}},
url = {http://portal.acm.org/citation.cfm?doid=48014.63140},
volume = {35},
year = {1988}
}
@inproceedings{Zhang2000a,
author = {Zhang, Tong and Oles, Frank J.},
booktitle = {International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhang, Oles - 2000 - A Probability Analysis on the Value of Unlabeled Data for Classification Problems.pdf:pdf},
pages = {1191--1198},
title = {{A Probability Analysis on the Value of Unlabeled Data for Classification Problems}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.20.6025\&rep=rep1\&type=pdf},
year = {2000}
}
@article{Gretton,
author = {Gretton, Arthur and Sch\"{o}lkopf, Bernhard},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gretton, Sch\"{o}lkopf - Unknown - A Kernel Method for the Two-Sample-Problem.pdf:pdf},
title = {{A Kernel Method for the Two-Sample-Problem}}
}
@misc{Kleijn2012,
author = {Kleijn, Bas and Vaart, Aad Van Der and Zanten, Harry Van},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kleijn, Vaart, Zanten - 2012 - Nonparametric Bayesian Statistics - Intro Bayesian inference.pdf:pdf},
number = {September},
title = {{Nonparametric Bayesian Statistics - Intro Bayesian inference}},
year = {2012}
}
@article{Nadler2008,
author = {Nadler, Boaz},
doi = {10.1214/08-AOS618},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Nadler - 2008 - Finite sample approximation results for principal component analysis A matrix perturbation approach.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
month = dec,
number = {6},
pages = {2791--2817},
title = {{Finite sample approximation results for principal component analysis: A matrix perturbation approach}},
url = {http://projecteuclid.org/euclid.aos/1231165185},
volume = {36},
year = {2008}
}
@unpublished{Lockhart2013,
author = {Lockhart, Richard and Taylor, Jonathan and Tibshirani, Ryan J. and Tibshirani, Robert},
booktitle = {arXiv preprint},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lockhart et al. - 2013 - A significance test for the lasso.pdf:pdf},
keywords = {lasso,least angle regression,p-value,significance test},
title = {{A significance test for the lasso}},
url = {http://repository.cmu.edu/statistics/131/?utm\_source=repository.cmu.edu\%2Fstatistics\%2F131\&utm\_medium=PDF\&utm\_campaign=PDFCoverPages},
year = {2013}
}
@article{Spirtes2010,
author = {Pearl, Judea},
doi = {10.2202/1557-4679.1203},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Pearl - 2010 - Introduction to causal inference.pdf:pdf},
issn = {1557-4679},
journal = {Journal of Machine Learning Research},
month = jan,
pages = {1643--1662},
pmid = {20305706},
title = {{Introduction to causal inference}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2836213\&tool=pmcentrez\&rendertype=abstract},
volume = {11},
year = {2010}
}
@inproceedings{Vandewalle2008,
author = {Vandewalle, Vincent and Biernacki, Christophe and Celeux, Gilles and Govaert, Gerard},
booktitle = {vincent.vandewalle.perso.sfr.fr},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Vandewalle et al. - 2008 - Are unlabeled data useful in semi-supervised model-based classification Combining hypothesis testing and mode.pdf:pdf},
title = {{Are unlabeled data useful in semi-supervised model-based classification? Combining hypothesis testing and model choice}},
url = {http://vincent.vandewalle.perso.sfr.fr/documents/recherche/articles/vbcg.pdf},
year = {2008}
}
@article{Nock2009,
abstract = {Bartlett et al. (2006) recently proved that a ground condition for surrogates, classification calibration, ties up their consistent minimization to that of the classification risk, and left as an important problem the algorithmic questions about their minimization. In this paper, we address this problem for a wide set which lies at the intersection of classification calibrated surrogates and those of Murata et al. (2004). This set coincides with those satisfying three common assumptions about surrogates. Equivalent expressions for the members-sometimes well known-follow for convex and concave surrogates, frequently used in the induction of linear separators and decision trees. Most notably, they share remarkable algorithmic features: for each of these two types of classifiers, we give a minimization algorithm provably converging to the minimum of any such surrogate. While seemingly different, we show that these algorithms are offshoots of the same "master" algorithm. This provides a new and broad unified account of different popular algorithms, including additive regression with the squared loss, the logistic loss, and the top-down induction performed in CART, C4.5. Moreover, we show that the induction enjoys the most popular boosting features, regardless of the surrogate. Experiments are provided on 40 readily available domains.},
author = {Nock, Richard and Nielsen, Frank},
doi = {10.1109/TPAMI.2008.225},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Nock, Nielsen - 2009 - Bregman divergences and surrogates for learning.pdf:pdf},
issn = {1939-3539},
journal = {IEEE transactions on pattern analysis and machine intelligence},
keywords = {Algorithms,Artificial Intelligence,Computer Simulation,Decision Support Techniques,Models, Theoretical,Pattern Recognition, Automated,Pattern Recognition, Automated: methods},
month = nov,
number = {11},
pages = {2048--59},
pmid = {19762930},
title = {{Bregman divergences and surrogates for learning.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19762930},
volume = {31},
year = {2009}
}
@inproceedings{Ben-David2011,
author = {Ben-David, Shai and Srebro, Nathan and Urner, R},
booktitle = {Philosophy and Machine Learning - Workshop at NIPS},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ben-David, Srebro, Urner - 2011 - Universal learning vs. no free lunch results.pdf:pdf},
title = {{Universal learning vs. no free lunch results}},
url = {http://www.dsi.unive.it/PhiMaLe2011/Abstract/Ben-David\_Srebro\_Urner.pdf},
year = {2011}
}
@article{Classifier,
archivePrefix = {arXiv},
arxivId = {arXiv:1503.03148v1},
author = {Classifier, Dimension},
eprint = {arXiv:1503.03148v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Classifier - Unknown - A Neurodynamical System for finding a Minimal VC Dimension Classifier.pdf:pdf},
keywords = {complexity machine,linear programming,minimal,neural network,neurodynamical systems,vc dimension},
title = {{A Neurodynamical System for finding a Minimal VC Dimension Classifier}}
}
@article{Varma2006,
abstract = {Cross-validation (CV) is an effective method for estimating the prediction error of a classifier. Some recent articles have proposed methods for optimizing classifiers by choosing classifier parameter values that minimize the CV error estimate. We have evaluated the validity of using the CV error estimate of the optimized classifier as an estimate of the true error expected on independent data.},
author = {Varma, Sudhir and Simon, Richard},
doi = {10.1186/1471-2105-7-91},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Varma, Simon - 2006 - Bias in error estimation when using cross-validation for model selection.pdf:pdf},
issn = {1471-2105},
journal = {BMC bioinformatics},
keywords = {Algorithms,Artificial Intelligence,Bias (Epidemiology),Computer Simulation,Data Interpretation, Statistical,Gene Expression Profiling,Gene Expression Profiling: methods,Models, Genetic,Models, Statistical,Oligonucleotide Array Sequence Analysis,Oligonucleotide Array Sequence Analysis: methods,Pattern Recognition, Automated,Pattern Recognition, Automated: methods,Reproducibility of Results,Sensitivity and Specificity},
month = jan,
pages = {91},
pmid = {16504092},
title = {{Bias in error estimation when using cross-validation for model selection.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=1397873\&tool=pmcentrez\&rendertype=abstract},
volume = {7},
year = {2006}
}
@article{Fithian2014,
author = {Fithian, William and Hastie, Trevor},
doi = {10.1214/14-AOS1220},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Fithian, Hastie - 2014 - Local case-control sampling Efficient subsampling in imbalanced data sets.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
month = oct,
number = {5},
pages = {1693--1724},
title = {{Local case-control sampling: Efficient subsampling in imbalanced data sets}},
url = {http://projecteuclid.org/euclid.aos/1410440622},
volume = {42},
year = {2014}
}
@article{Hsu2011,
archivePrefix = {arXiv},
arxivId = {arXiv:1106.2363v1},
author = {Hsu, Daniel and Kakade, Sham M. and Zhang, Tong},
eprint = {arXiv:1106.2363v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hsu, Kakade, Zhang - 2011 - An analysis of random design linear regression.pdf:pdf},
journal = {arXiv preprint},
title = {{An analysis of random design linear regression}},
url = {http://arxiv.org/abs/1106.2363},
year = {2011}
}
@article{He2010,
abstract = {Feature selection techniques have been used as the workhorse in biomarker discovery applications for a long time. Surprisingly, the stability of feature selection with respect to sampling variations has long been under-considered. It is only until recently that this issue has received more and more attention. In this article, we review existing stable feature selection methods for biomarker discovery using a generic hierarchical framework. We have two objectives: (1) providing an overview on this new yet fast growing topic for a convenient reference; (2) categorizing existing methods under an expandable framework for future research and development.},
author = {He, Zengyou and Yu, Weichuan},
doi = {10.1016/j.compbiolchem.2010.07.002},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/He, Yu - 2010 - Stable feature selection for biomarker discovery.pdf:pdf},
issn = {1476-928X},
journal = {Computational biology and chemistry},
keywords = {Animals,Artificial Intelligence,Biological Markers,Biological Markers: analysis,Biological Markers: metabolism,Computational Biology,Computational Biology: methods,Humans},
month = aug,
number = {4},
pages = {215--25},
pmid = {20702140},
publisher = {Elsevier Ltd},
title = {{Stable feature selection for biomarker discovery.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20702140},
volume = {34},
year = {2010}
}
@unpublished{Hennig2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1502.02555v1},
author = {Hennig, Christian},
eprint = {arXiv:1502.02555v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hennig - 2015 - What are the true clusters.pdf:pdf},
keywords = {active scientific realism,cat-,comparison of clustering methods,constructivism,egorization,mixture models,natural kinds,variable},
title = {{What are the true clusters?}},
year = {2015}
}
@article{Bruand2011,
abstract = {Mass Spectrometric Imaging (MSI) is a molecular imaging technique that allows the generation of 2D ion density maps for a large complement of the active molecules present in cells and sectioned tissues. Automatic segmentation of such maps according to patterns of co-expression of individual molecules can be used for discovery of novel molecular signatures (molecules that are specifically expressed in particular spatial regions). However, current segmentation techniques are biased toward the discovery of higher abundance molecules and large segments; they allow limited opportunity for user interaction, and validation is usually performed by similarity to known anatomical features. We describe here a novel method, AMASS (Algorithm for MSI Analysis by Semi-supervised Segmentation). AMASS relies on the discriminating power of a molecular signal instead of its intensity as a key feature, uses an internal consistency measure for validation, and allows significant user interaction and supervision as options. An automated segmentation of entire leech embryo data images resulted in segmentation domains congruent with many known organs, including heart, CNS ganglia, nephridia, nephridiopores, and lateral and ventral regions, each with a distinct molecular signature. Likewise, segmentation of a rat brain MSI slice data set yielded known brain features and provided interesting examples of co-expression between distinct brain regions. AMASS represents a new approach for the discovery of peptide masses with distinct spatial features of expression. Software source code and installation and usage guide are available at http://bix.ucsd.edu/AMASS/ .},
author = {Bruand, Jocelyne and Alexandrov, Theodore and Sistla, Srinivas and Wisztorski, Maxence and Meriaux, C\'{e}line and Becker, Michael and Salzet, Michel and Fournier, Isabelle and Macagno, Eduardo and Bafna, Vineet},
doi = {10.1021/pr2005378},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bruand et al. - 2011 - AMASS algorithm for MSI analysis by semi-supervised segmentation.pdf:pdf},
issn = {1535-3907},
journal = {Journal of proteome research},
keywords = {Algorithms,Animals,Automatic Data Processing,Brain,Brain: metabolism,Cluster Analysis,Computational Biology,Computational Biology: methods,Gene Expression Regulation,Gene Expression Regulation, Developmental,Image Processing, Computer-Assisted,Image Processing, Computer-Assisted: methods,Leeches,Mass Spectrometry,Mass Spectrometry: methods,Peptides,Peptides: chemistry,Rats,Spectrometry, Mass, Matrix-Assisted Laser Desorpti},
month = oct,
number = {10},
pages = {4734--43},
pmid = {21800894},
title = {{AMASS: algorithm for MSI analysis by semi-supervised segmentation.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3190602\&tool=pmcentrez\&rendertype=abstract},
volume = {10},
year = {2011}
}
@article{Efron1977,
author = {Efron, Bradley and Morris, Carl},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Efron, Morris - 1977 - Stein's paradox in statistics.pdf:pdf},
journal = {Scientific American},
pages = {119--127},
title = {{Stein's paradox in statistics}},
url = {https://www.cs.nyu.edu/~roweis/csc2515-2006/readings/stein\_sciam.pdf},
year = {1977}
}
@article{Gelly2007,
author = {Gelly, Sylvain and Kocsis, Levente and Schoenauer, Marc and Sebag, Michele and Silver, David and Szepesvari, Csaba and Teytaud, Olivier},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gelly et al. - 2007 - The Grand Challenge of Computer Go Monte Carlo Tree Search and Extensions.pdf:pdf},
title = {{The Grand Challenge of Computer Go : Monte Carlo Tree Search and Extensions}},
year = {2007}
}
@article{Castelli1994,
author = {Castelli, Vittorio and Cover, Thomas M.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Castelli, Cover - 1995 - On the exponential value of labeled samples.pdf:pdf},
journal = {Pattern Recognition Letters},
pages = {105--111},
title = {{On the exponential value of labeled samples}},
volume = {16},
year = {1995}
}
@article{Meinshausen2010,
author = {Meinshausen, Nicolai and B\"{u}hlmann, Peter},
doi = {10.1111/j.1467-9868.2010.00740.x},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Meinshausen, B\"{u}hlmann - 2010 - Stability selection.pdf:pdf},
issn = {13697412},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
keywords = {high dimensional data,resampling,stability selection,structure estimation},
month = jul,
number = {4},
pages = {417--473},
title = {{Stability selection}},
url = {http://doi.wiley.com/10.1111/j.1467-9868.2010.00740.x},
volume = {72},
year = {2010}
}
@article{Jones2011,
abstract = {MALDI mass spectrometry can generate profiles that contain hundreds of biomolecular ions directly from tissue. Spatially-correlated analysis, MALDI imaging MS, can simultaneously reveal how each of these biomolecular ions varies in clinical tissue samples. The use of statistical data analysis tools to identify regions containing correlated mass spectrometry profiles is referred to as imaging MS-based molecular histology because of its ability to annotate tissues solely on the basis of the imaging MS data. Several reports have indicated that imaging MS-based molecular histology may be able to complement established histological and histochemical techniques by distinguishing between pathologies with overlapping/identical morphologies and revealing biomolecular intratumor heterogeneity. A data analysis pipeline that identifies regions of imaging MS datasets with correlated mass spectrometry profiles could lead to the development of novel methods for improved diagnosis (differentiating subgroups within distinct histological groups) and annotating the spatio-chemical makeup of tumors. Here it is demonstrated that highlighting the regions within imaging MS datasets whose mass spectrometry profiles were found to be correlated by five independent multivariate methods provides a consistently accurate summary of the spatio-chemical heterogeneity. The corroboration provided by using multiple multivariate methods, efficiently applied in an automated routine, provides assurance that the identified regions are indeed characterized by distinct mass spectrometry profiles, a crucial requirement for its development as a complementary histological tool. When simultaneously applied to imaging MS datasets from multiple patient samples of intermediate-grade myxofibrosarcoma, a heterogeneous soft tissue sarcoma, nodules with mass spectrometry profiles found to be distinct by five different multivariate methods were detected within morphologically identical regions of all patient tissue samples. To aid the further development of imaging MS based molecular histology as a complementary histological tool the Matlab code of the agreement analysis, instructions and a reduced dataset are included as supporting information.},
author = {Jones, Emrys a and van Remoortere, Alexandra and van Zeijl, Ren\'{e} J M and Hogendoorn, Pancras C W and Bov\'{e}e, Judith V M G and Deelder, Andr\'{e} M and McDonnell, Liam a},
doi = {10.1371/journal.pone.0024913},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Jones et al. - 2011 - Multiple statistical analysis techniques corroborate intratumor heterogeneity in imaging mass spectrometry dataset.pdf:pdf},
issn = {1932-6203},
journal = {PloS one},
keywords = {Algorithms,Databases, Factual,Diagnostic Imaging,Diagnostic Imaging: methods,Fibroma,Fibroma: metabolism,Fibrosarcoma,Fibrosarcoma: metabolism,Gene Expression Regulation, Neoplastic,Humans,Ions,Ions: chemistry,Models, Statistical,Molecular Imaging,Molecular Imaging: methods,Multivariate Analysis,Peptides,Peptides: chemistry,Proteins,Proteins: chemistry,Sarcoma,Sarcoma: metabolism,Software,Spectrometry, Mass, Matrix-Assisted Laser Desorpti},
month = jan,
number = {9},
pages = {e24913},
pmid = {21980364},
title = {{Multiple statistical analysis techniques corroborate intratumor heterogeneity in imaging mass spectrometry datasets of myxofibrosarcoma.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3183001\&tool=pmcentrez\&rendertype=abstract},
volume = {6},
year = {2011}
}
@article{Stein1975,
archivePrefix = {arXiv},
arxivId = {arXiv:1503.07990v1},
author = {Statistics, Applied},
eprint = {arXiv:1503.07990v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Statistics - 2007 - ESTIMATION OF A COMMON COVARIANCE MATRIX.pdf:pdf},
keywords = {and phrases,covariance estimation,dis-,integrative analysis,meta-analysis},
title = {{ESTIMATION OF A COMMON COVARIANCE MATRIX}},
year = {2007}
}
@article{Culp2008a,
author = {Culp, Mark and Michailidis, George},
doi = {10.1198/106186008X344748},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Culp, Michailidis - 2008 - An Iterative Algorithm for Extending Learners to a Semi-Supervised Setting(2).pdf:pdf},
issn = {1061-8600},
journal = {Journal of Computational and Graphical Statistics},
keywords = {convergence,iterative algorithm,linear smoothers,semi-supervised learning},
month = sep,
number = {3},
pages = {545--571},
title = {{An Iterative Algorithm for Extending Learners to a Semi-Supervised Setting}},
url = {http://www.tandfonline.com/doi/abs/10.1198/106186008X344748},
volume = {17},
year = {2008}
}
@article{Loog2014a,
author = {Loog, Marco},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Loog - 2014 - Semi-supervised linear discriminant analysis through moment-constraint parameter estimation.pdf:pdf},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {linear discriminant analysis,semi-supervised learning},
month = mar,
pages = {24--31},
publisher = {Elsevier B.V.},
title = {{Semi-supervised linear discriminant analysis through moment-constraint parameter estimation}},
volume = {37},
year = {2014}
}
@phdthesis{Lu2009,
author = {Lu, Tyler},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lu - 2009 - Fundamental Limitations of Semi-Supervised Learning.pdf:pdf},
title = {{Fundamental Limitations of Semi-Supervised Learning}},
year = {2009}
}
@inproceedings{Grandvalet2005,
address = {Cambridge, MA},
author = {Grandvalet, Yves and Bengio, Yoshua},
booktitle = {Advances in Neural Information Processing Systems 17},
editor = {Saul, L. K. and Weiss, Y. and Bottou, L.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Grandvalet, Bengio - 2005 - Semi-supervised learning by entropy minimization.pdf:pdf},
pages = {529--536},
publisher = {MIT Press},
title = {{Semi-supervised learning by entropy minimization}},
year = {2005}
}
@article{Wang,
author = {Wang, Jianyong},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wang - Unknown - FAMER Making Multi-Instance Learning Better and Faster.pdf:pdf},
pages = {594--605},
title = {{FAMER : Making Multi-Instance Learning Better and Faster}}
}
@article{Hennig2012,
author = {Hennig, Philipp and Kiefel, Martin},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hennig, Kiefel - 2012 - Quasi-Newton Methods A New Direction.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {gaussian processes,numerical analysis,optimization,probability},
pages = {843--865},
title = {{Quasi-Newton Methods: A New Direction}},
url = {http://arxiv.org/abs/1206.4602},
volume = {14},
year = {2012}
}
@article{Wand2002,
author = {Wand, M.P.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wand - 2002 - Vector Differential Calculus in Statistics.pdf:pdf},
journal = {The American Statistician},
keywords = {best linear prediction,generalized linear,generalized linear mixed model,information matrix,matrix differential calculus,maximum likelihood estimation,model,penalized quasi-likelihood,score equation},
number = {1},
pages = {1--8},
title = {{Vector Differential Calculus in Statistics}},
volume = {56},
year = {2002}
}
@article{Shi2011,
author = {Shi, Mingguang and Zhang, Bing},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Shi, Zhang - 2011 - Semi-supervised learning improves gene expression-based prediction of cancer recurrence.pdf:pdf},
journal = {Bioinformatics},
number = {21},
pages = {3017--3023},
title = {{Semi-supervised learning improves gene expression-based prediction of cancer recurrence}},
url = {http://bioinformatics.oxfordjournals.org/content/27/21/3017.short},
volume = {27},
year = {2011}
}
@inproceedings{Auger2007,
author = {Auger, Anne and Teytaud, Olivier},
booktitle = {Proceedings of the 9th annual conference on Genetic and evolutionary computation},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Auger, Teytaud - 2007 - Continuous lunches are free!.pdf:pdf},
isbn = {9781595936974},
keywords = {free-lunch,kolmogorov,no-free-lunch,s extension theo-},
pages = {916--922},
title = {{Continuous lunches are free!}},
url = {http://dl.acm.org/citation.cfm?id=1277145},
year = {2007}
}
@article{Hamsici2008,
abstract = {We present an algorithm which provides the one-dimensional subspace where the Bayes error is minimized for the C class problem with homoscedastic Gaussian distributions. Our main result shows that the set of possible one-dimensional spaces v, for which the order of the projected class means is identical, defines a convex region with associated convex Bayes error function g(v). This allows for the minimization of the error function using standard convex optimization algorithms. Our algorithm is then extended to the minimization of the Bayes error in the more general case of heteroscedastic distributions. This is done by means of an appropriate kernel mapping function. This result is further extended to obtain the d-dimensional solution for any given d, by iteratively applying our algorithm to the null space of the (d - 1)-dimensional solution. We also show how this result can be used to improve up on the outcomes provided by existing algorithms, and derive a low-computational cost, linear approximation. Extensive experimental validations are provided to demonstrate the use of these algorithms in classification, data analysis and visualization.},
author = {Hamsici, Onur C and Martinez, Aleix M},
doi = {10.1109/TPAMI.2007.70717},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hamsici, Martinez - 2008 - Bayes optimality in linear discriminant analysis.pdf:pdf},
issn = {0162-8828},
journal = {IEEE transactions on pattern analysis and machine intelligence},
keywords = {Algorithms,Artificial Intelligence,Bayes Theorem,Computer Simulation,Discriminant Analysis,Image Enhancement,Image Enhancement: methods,Image Interpretation, Computer-Assisted,Image Interpretation, Computer-Assisted: methods,Linear Models,Pattern Recognition, Automated,Pattern Recognition, Automated: methods,Reproducibility of Results,Sensitivity and Specificity},
month = apr,
number = {4},
pages = {647--57},
pmid = {18276970},
title = {{Bayes optimality in linear discriminant analysis.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/18276970},
volume = {30},
year = {2008}
}
@incollection{Cozman2006,
author = {Cozman, F and Cohen, Ira},
booktitle = {Semi-Supervised Learning},
chapter = {4},
editor = {Chapelle, Olivier and Sch\"{o}lkopf, Bernhard and Zien, A},
pages = {56--72},
publisher = {MIT press},
title = {{Risks of Semi-Supervised Learning}},
year = {2006}
}
@article{Roweis1999,
abstract = {Factor analysis, principal component analysis, mixtures of gaussian clusters, vector quantization, Kalman filter models, and hidden Markov models can all be unified as variations of unsupervised learning under a single basic generative model. This is achieved by collecting together disparate observations and derivations made by many previous authors and introducing a new way of linking discrete and continuous state models using a simple nonlinearity. Through the use of other nonlinearities, we show how independent component analysis is also a variation of the same basic generative model. We show that factor analysis and mixtures of gaussians can be implemented in autoencoder neural networks and learned using squared error plus the same regularization term. We introduce a new model for static data, known as sensible principal component analysis, as well as a novel concept of spatially adaptive observation noise. We also review some of the literature involving global and local mixtures of the basic models and provide pseudocode for inference and learning for all the basic models.},
author = {Roweis, Sam and Ghahramani, Zoubin},
doi = {10.1162/089976699300016674},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Roweis, Ghahramani - 1999 - A unifying review of linear gaussian models.pdf:pdf},
isbn = {0899766993000},
issn = {0899-7667},
journal = {Neural computation},
number = {1995},
pages = {305--345},
pmid = {9950734},
title = {{A unifying review of linear gaussian models.}},
volume = {11},
year = {1999}
}
@article{Mann2010,
author = {Mann, Gideon S. and McCallum, Andrew Kachites},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Mann, McCallum - 2010 - Generalized expectation criteria for semi-supervised learning with weakly labeled data.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {condi-,generalized expectation criteria,logistic regression,semi-supervised learning,tional random fields},
pages = {955--984},
title = {{Generalized expectation criteria for semi-supervised learning with weakly labeled data}},
url = {http://dl.acm.org/citation.cfm?id=1756038},
volume = {11},
year = {2010}
}
@article{Marchand,
archivePrefix = {arXiv},
arxivId = {arXiv:1503.08329v1},
author = {Marchand, Mario and Roy, Jean-francis},
eprint = {arXiv:1503.08329v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Marchand, Roy - Unknown - PAC-Bayesian Bounds for the Risk of the Majority Vote.pdf:pdf},
keywords = {ensemble methods,learning theory,majority vote,pac-bayesian theory},
number = {2006},
title = {{PAC-Bayesian Bounds for the Risk of the Majority Vote}}
}
@article{VanderMaaten2007,
author = {{Van der Maaten}, Laurens},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Van der Maaten - 2007 - An Introduction to Dimensionality Reduction Using Matlab.pdf:pdf},
number = {July},
title = {{An Introduction to Dimensionality Reduction Using Matlab}},
year = {2007}
}
@inproceedings{Collobert2006a,
author = {Collobert, Ronan and Sinz, Fabian and Weston, Jason and Bottou, L},
booktitle = {International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Collobert et al. - 2006 - Trading convexity for scalability.pdf:pdf},
pages = {201--208},
title = {{Trading convexity for scalability}},
url = {http://dl.acm.org/citation.cfm?id=1143870},
year = {2006}
}
@article{Menon,
author = {Menon, Aditya Krishna},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Menon - Unknown - Large-Scale Support Vector Machines Algorithms and Theory.pdf:pdf},
title = {{Large-Scale Support Vector Machines : Algorithms and Theory}}
}
@article{Raftery2014a,
author = {Raftery, Adrian E. and Alkema, Leontine and Gerland, Patrick},
doi = {10.1214/13-STS419},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Raftery, Alkema, Gerland - 2014 - Bayesian Population Projections for the United Nations.pdf:pdf},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {Bayesian hierarchical model, cohort component proj,and phrases,bayesian hierarchical model,cohort component,double logistic function,leslie matrix,life expectancy,projection method,total fertility rate},
month = feb,
number = {1},
pages = {58--68},
title = {{Bayesian Population Projections for the United Nations}},
url = {http://projecteuclid.org/euclid.ss/1399645729},
volume = {29},
year = {2014}
}
@article{Wang2012a,
abstract = {This paper proposes a method to select a set of genes from a large number of genes with the ability of classifying types of diseases. The proposed gene selection method is designed according to correlation analysis and the concept of 95\% reference range. The method is very simple and uses the information of all genes. We have used the method in leukemia patients and achieved good classification results.},
author = {Wang, Xiaodong and Tian, Jun},
doi = {10.1155/2012/586246},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wang, Tian - 2012 - A gene selection method for cancer classification.pdf:pdf},
issn = {1748-6718},
journal = {Computational and mathematical methods in medicine},
keywords = {array,cancer classification,diagnosis,diagnostic tests,dna micro-,drug discovery,feature selection,gene selection,genomics,proteomics,recursive feature elimination,rna expression,support vector machines},
month = jan,
pages = {586246},
pmid = {23251228},
title = {{A gene selection method for cancer classification.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23289441},
volume = {2012},
year = {2012}
}
@book{Bertsekas1982,
author = {Bertsekas, Dimitri P.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bertsekas - 1982 - Constrained optimization and Lagrange multiplier methods.pdf:pdf},
publisher = {Academic Press},
title = {{Constrained optimization and Lagrange multiplier methods}},
url = {http://adsabs.harvard.edu/abs/1982colm.book.....b},
year = {1982}
}
@inproceedings{Cortes2011,
author = {Cortes, Corinna and Mohri, Mehryar},
booktitle = {Algorithmic Learning Theory},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cortes, Mohri - 2011 - Domain adaptation in regression.pdf:pdf},
title = {{Domain adaptation in regression}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-24412-4\_25},
year = {2011}
}
@article{Barber2014,
abstract = {We explore and compare a variety of definitions for privacy and disclosure limitation in statistical estimation and data analysis, including (approximate) differential privacy, testing-based definitions of privacy, and posterior guarantees on disclosure risk. We give equivalence results between the definitions, shedding light on the relationships between different formalisms for privacy. We also take an inferential perspective, where---building off of these definitions---we provide minimax risk bounds for several estimation problems, including mean estimation, estimation of the support of a distribution, and nonparametric density estimation. These bounds highlight the statistical consequences of different definitions of privacy and provide a second lens for evaluating the advantages and disadvantages of different techniques for disclosure limitation.},
archivePrefix = {arXiv},
arxivId = {1412.4451},
author = {Barber, Rina Foygel and Duchi, John C.},
eprint = {1412.4451},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Barber, Duchi - 2014 - Privacy and Statistical Risk Formalisms and Minimax Bounds.pdf:pdf},
month = dec,
pages = {29},
title = {{Privacy and Statistical Risk: Formalisms and Minimax Bounds}},
url = {http://arxiv.org/abs/1412.4451},
year = {2014}
}
@article{Zhang2014b,
archivePrefix = {arXiv},
arxivId = {1412.1271},
author = {Zhang, Xiao-Lei},
eprint = {1412.1271},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhang - 2014 - Deep Distributed Random Samplings for Supervised Learning An Alternative to Random Forests.pdf:pdf},
keywords = {bootstrap,deep learning,dimensionality reduction,ensemble methods,evo-,kernel methods,lutionary computing,random forests,sparse coding},
month = dec,
pages = {1--4},
title = {{Deep Distributed Random Samplings for Supervised Learning: An Alternative to Random Forests?}},
url = {http://arxiv.org/abs/1412.1271v1},
year = {2014}
}
@inproceedings{Sokolovska2011,
address = {Greece},
author = {Sokolovska, Nataliya},
booktitle = {ECML PKDD},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sokolovska - 2011 - Aspects of semi-supervised and active learning in conditional random fields.pdf:pdf},
keywords = {active learn-,conditional random fields,ing,probability of observations,semi-supervised learning},
title = {{Aspects of semi-supervised and active learning in conditional random fields}},
url = {http://www.springerlink.com/index/3308764R6251J70P.pdf},
year = {2011}
}
@article{Vilalta2002,
author = {Vilalta, Ricardo and Drissi, Youssef},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Vilalta, Drissi - 2002 - A perspective view and survey of meta-learning.pdf:pdf},
journal = {Artificial Intelligence Review},
keywords = {classification,inductive learning,meta-knowledge},
number = {1997},
pages = {77--95},
title = {{A perspective view and survey of meta-learning}},
url = {http://link.springer.com/article/10.1023/A:1019956318069},
year = {2002}
}
@inproceedings{Kuncheva2001,
author = {Kuncheva, Ludmila I and Roli, Fabio and Marcialis, Gian Luca and Shipp, Catherine A.},
booktitle = {Multiple Classifier Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kuncheva et al. - 2001 - Complexity of Data Subsets Generated by the Random Subspace Method An Experimental Investigation.pdf:pdf},
pages = {349--358},
title = {{Complexity of Data Subsets Generated by the Random Subspace Method: An Experimental Investigation}},
year = {2001}
}
@article{Blanchard2010,
author = {Blanchard, Gilles and Lee, Gyemin and Scott, Clayton},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Blanchard, Lee, Scott - 2010 - Semi-supervised novelty detection.pdf:pdf},
journal = {Journal of Machine Learning Research},
pages = {2973--3009},
title = {{Semi-supervised novelty detection}},
url = {http://dl.acm.org/citation.cfm?id=1953028},
volume = {11},
year = {2010}
}
@inproceedings{Cozman2003,
author = {Cozman, Fabio Gagliardi and Cohen, Ira and Cirelo, Marcelo Cesar},
booktitle = {Proceedings of the Twentieth International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cozman, Cohen, Cirelo - 2003 - Semi-Supervised Learning of Mixture Models.pdf:pdf},
title = {{Semi-Supervised Learning of Mixture Models}},
year = {2003}
}
@article{Schuurmans2002,
author = {Schuurmans, Dale and Southey, Finnegan},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Schuurmans, Southey - 2002 - Metric-Based Methods for Adaptive Model Selection and Regularization.pdf:pdf},
journal = {Machine Learning},
keywords = {model selection,regularization,unlabeled examples},
pages = {51--84},
title = {{Metric-Based Methods for Adaptive Model Selection and Regularization}},
volume = {48},
year = {2002}
}
@article{Gonzalez-Covarrubias2013,
abstract = {Middle-aged offspring of nonagenarians, as compared to their spouses (controls), show a favorable lipid metabolism marked by larger LDL particle size in men and lower total triglyceride levels in women. To investigate which specific lipids associate with familial longevity, we explore the plasma lipidome by measuring 128 lipid species using liquid chromatography coupled to mass spectrometry in 1526 offspring of nonagenarians (59 years ± 6.6) and 675 (59 years ± 7.4) controls from the Leiden Longevity Study. In men, no significant differences were observed between offspring and controls. In women, however, 19 lipid species associated with familial longevity. Female offspring showed higher levels of ether phosphocholine (PC) and sphingomyelin (SM) species (3.5-8.7\%) and lower levels of phosphoethanolamine PE (38:6) and long-chain triglycerides (TG) (9.4-12.4\%). The association with familial longevity of two ether PC and four SM species was independent of total triglyceride levels. In addition, the longevity-associated lipid profile was characterized by a higher ratio of monounsaturated (MUFA) over polyunsaturated (PUFA) lipid species, suggesting that female offspring have a plasma lipidome less prone to oxidative stress. Ether PC and SM species were identified as novel longevity markers in females, independent of total triglycerides levels. Several longevity-associated lipids correlated with a lower risk of hypertension and diabetes in the Leiden Longevity Study cohort. This sex-specific lipid signature marks familial longevity and may suggest a plasma lipidome with a better antioxidant capacity, lower lipid peroxidation and inflammatory precursors, and an efficient beta-oxidation function.},
author = {Gonzalez-Covarrubias, Vanessa and Beekman, Marian and Uh, Hae Won and Dane, Adrie and Troost, Jorne and Paliukhovich, Iryna and van der Kloet, Frans M. and Houwing-Duistermaat, Jeanine and Vreeken, Rob J. and Hankemeier, Thomas and Slagboom, Eline P.},
doi = {10.1111/acel.12064},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gonzalez-Covarrubias et al. - 2013 - Lipidomics of familial longevity.pdf:pdf},
isbn = {1474-9726 (Electronic)$\backslash$r1474-9718 (Linking)},
issn = {14749718},
journal = {Aging Cell},
keywords = {Aging,Gender differences,Human,Longevity,Mass spectrometry,Oxidative stress},
number = {3},
pages = {426--434},
pmid = {23451766},
title = {{Lipidomics of familial longevity}},
volume = {12},
year = {2013}
}
@misc{Efron,
author = {Efron, Bradley},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Efron - Unknown - Frequentist Accuracy of Bayesian Estimates.pdf:pdf},
title = {{Frequentist Accuracy of Bayesian Estimates}}
}
@article{Lopez-paz2012,
archivePrefix = {arXiv},
arxivId = {arXiv:1502.02398v1},
author = {Lopez-paz, David and Sch, Bernhard},
eprint = {arXiv:1502.02398v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lopez-paz, Sch - 2012 - Towards a Learning Theory of Causation.pdf:pdf},
title = {{Towards a Learning Theory of Causation}},
year = {2012}
}
@article{Caticha2011,
author = {Caticha, Ariel and Mohammad-Djafari, Ali and Bercher, Jean-François and Bessiére, Pierre},
doi = {10.1063/1.3573619},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Caticha et al. - 2011 - Entropic Inference.pdf:pdf},
isbn = {9780735408609},
keywords = {bayes rule,entropy,information,maximum entropy},
number = {1},
pages = {20--29},
title = {{Entropic Inference}},
url = {http://link.aip.org/link/APCPCS/v1305/i1/p20/s1\&Agg=doi},
volume = {20},
year = {2011}
}
@inproceedings{Elworthy1994,
archivePrefix = {arXiv},
arxivId = {arXiv:cmp-lg/9410012v2},
author = {Elworthy, David},
booktitle = {Proceedings of the fourth conference on Applied natural language processing},
eprint = {9410012v2},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Elworthy - 1994 - Does Baum-Welch re-estimation help taggers.pdf:pdf},
pages = {53--58},
primaryClass = {arXiv:cmp-lg},
title = {{Does Baum-Welch re-estimation help taggers?}},
url = {http://dl.acm.org/citation.cfm?id=974371},
year = {1994}
}
@inproceedings{Blum1998,
author = {Blum, Avrim and Mitchell, Tom},
booktitle = {Proceedings of the 11th Annual Conference on Computational Learning Theory},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Blum, Mitchell - 1998 - Combining labeled and unlabeled data with co-training.pdf:pdf},
pages = {92--100},
title = {{Combining labeled and unlabeled data with co-training}},
url = {http://dl.acm.org/citation.cfm?id=279962},
year = {1998}
}
@article{Skurichina2000,
author = {Skurichina, Marina and Duin, Robert P.W.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Skurichina, Duin - 2000 - The role of combining rules in bagging and boosting.pdf:pdf},
journal = {Advances in Pattern Recognition},
pages = {631--640},
title = {{The role of combining rules in bagging and boosting}},
url = {http://link.springer.com/chapter/10.1007/3-540-44522-6\_65},
year = {2000}
}
@article{Hanselmann2008,
abstract = {Imaging mass spectrometry (IMS) is a promising technology which allows for detailed analysis of spatial distributions of (bio)molecules in organic samples. In many current applications, IMS relies heavily on (semi)automated exploratory data analysis procedures to decompose the data into characteristic component spectra and corresponding abundance maps, visualizing spectral and spatial structure. The most commonly used techniques are principal component analysis (PCA) and independent component analysis (ICA). Both methods operate in an unsupervised manner. However, their decomposition estimates usually feature negative counts and are not amenable to direct physical interpretation. We propose probabilistic latent semantic analysis (pLSA) for non-negative decomposition and the elucidation of interpretable component spectra and abundance maps. We compare this algorithm to PCA, ICA, and non-negative PARAFAC (parallel factors analysis) and show on simulated and real-world data that pLSA and non-negative PARAFAC are superior to PCA or ICA in terms of complementarity of the resulting components and reconstruction accuracy. We further combine pLSA decomposition with a statistical complexity estimation scheme based on the Akaike information criterion (AIC) to automatically estimate the number of components present in a tissue sample data set and show that this results in sensible complexity estimates.},
author = {Hanselmann, Michael and Kirchner, Marc and Renard, Bernhard Y and Amstalden, Erika R and Glunde, Kristine and Heeren, Ron M a and Hamprecht, Fred a},
doi = {10.1021/ac801303x},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hanselmann et al. - 2008 - Concise representation of mass spectrometry images by probabilistic latent semantic analysis.pdf:pdf},
issn = {1520-6882},
journal = {Analytical chemistry},
keywords = {Algorithms,Breast Neoplasms,Breast Neoplasms: pathology,Computer Simulation,Female,Humans,Image Processing, Computer-Assisted,Mass Spectrometry,Principal Component Analysis,Signal Processing, Computer-Assisted},
month = dec,
number = {24},
pages = {9649--58},
pmid = {18989936},
title = {{Concise representation of mass spectrometry images by probabilistic latent semantic analysis.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/18989936},
volume = {80},
year = {2008}
}
@article{Baraniuk2007,
author = {Baraniuk, Richard G.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Baraniuk - 2007 - Compressive sensing.pdf:pdf},
journal = {IEEE Signal Processing Magazine},
number = {July},
pages = {118--121},
title = {{Compressive sensing}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4286571 http://omni.isr.ist.utl.pt/~aguiar/CS\_notes.pdf},
year = {2007}
}
@article{Stigler2013,
author = {Stigler, Stephen M.},
doi = {10.1214/13-STS438},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Stigler - 2013 - The True Title of Bayes’s Essay.pdf:pdf},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {Thomas Bayes, Richard Price, Bayes's theorem, hist,and phrases},
month = aug,
number = {3},
pages = {283--288},
title = {{The True Title of Bayes’s Essay}},
url = {http://projecteuclid.org/euclid.ss/1377696937},
volume = {28},
year = {2013}
}
@article{Abney2004,
author = {Abney, Steven},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Abney - 2004 - Understanding the yarowsky algorithm.pdf:pdf},
journal = {Computational Linguistics},
number = {3},
pages = {365--395},
title = {{Understanding the yarowsky algorithm}},
url = {http://www.mitpressjournals.org/doi/pdf/10.1162/0891201041850876},
volume = {30},
year = {2004}
}
@article{Efron2004,
author = {Efron, Bradley and Hastie, Trevor and Johnstone, Iain and Tibshirani, Robert},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Efron et al. - 2004 - Least Angle Regression.pdf:pdf},
journal = {The Annals of Statistics},
keywords = {and phrases,boosting,coefficient paths,lasso,linear regression,variable selection},
number = {2},
pages = {407--499},
title = {{Least Angle Regression}},
url = {http://projecteuclid.org/euclid.aos/1083178935},
volume = {32},
year = {2004}
}
@unpublished{Graves2014,
archivePrefix = {arXiv},
arxivId = {1410.5401},
author = {Graves, Alex and Wayne, Greg and Danihelka, Ivo},
eprint = {1410.5401},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Graves, Wayne, Danihelka - 2014 - Neural Turing Machines.pdf:pdf},
month = oct,
pages = {1--26},
title = {{Neural Turing Machines}},
url = {http://arxiv.org/abs/1410.5401v1},
year = {2014}
}
@inproceedings{Bottou2011,
author = {Bottou, Leon and Bousquet, Olivier},
booktitle = {Advances in Neural Information Processing Systems 24},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bottou, Bousquet - 2011 - The Tradeoffs of Large-Scale Learning.pdf:pdf},
pages = {In Advances in Neural Information Processing Syste},
title = {{The Tradeoffs of Large-Scale Learning}},
url = {http://books.google.com/books?hl=en\&lr=\&id=JPQx7s2L1A8C\&oi=fnd\&pg=PA351\&dq=The+Tradeoffs+of+Large+Scale+Learning\&ots=vbhayjhcGc\&sig=kWCMo7N51TgoLQSVSv2f\_ILArjo http://books.google.com/books?hl=en\&lr=\&id=JPQx7s2L1A8C\&oi=fnd\&pg=PA351\&dq=The+Tradeoffs+of+Large-Scale+Learning\&ots=vbjaAkg8Fe\&sig=chdz7lCKXTFdUaLPYAgH\_FfgLmA},
year = {2011}
}
@article{Lopez2014,
archivePrefix = {arXiv},
arxivId = {1412.0248},
author = {Lopez, Michael J. and Matthews, Gregory},
eprint = {1412.0248},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lopez, Matthews - 2014 - Building an NCAA mens basketball predictive model and quantifying its success.pdf:pdf},
month = nov,
number = {1996},
title = {{Building an NCAA mens basketball predictive model and quantifying its success}},
url = {http://arxiv.org/abs/1412.0248v1},
year = {2014}
}
@misc{Gelman2013b,
author = {Gelman, Andrew},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gelman - 2013 - Choices in statistical graphics My stories.pdf:pdf},
title = {{Choices in statistical graphics : My stories}},
year = {2013}
}
@article{Yang2014,
abstract = {The fully connected layers of a deep convolutional neural network typically contain over 90\% of the network parameters, and consume the majority of the memory required to store the network parameters. Reducing the number of parameters while preserving essentially the same predictive performance is critically important for operating deep neural networks in memory constrained environments such as GPUs or embedded devices.   In this paper we show how kernel methods, in particular a single Fastfood layer, can be used to replace all fully connected layers in a deep convolutional neural network. This novel Fastfood layer is also end-to-end trainable in conjunction with convolutional layers, allowing us to combine them into a new architecture, named deep fried convolutional networks, which substantially reduces the memory footprint of convolutional networks trained on MNIST and ImageNet with no drop in predictive performance.},
archivePrefix = {arXiv},
arxivId = {1412.7149},
author = {Yang, Zichao and Moczulski, Marcin and Denil, Misha and de Freitas, Nando and Smola, Alex and Song, Le and Wang, Ziyu},
eprint = {1412.7149},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Yang et al. - 2014 - Deep Fried Convnets.pdf:pdf},
month = dec,
number = {2013},
pages = {9},
title = {{Deep Fried Convnets}},
url = {http://arxiv.org/abs/1412.7149},
year = {2014}
}
@article{Wang2012b,
author = {Wang, Hua and Nie, Feiping and Huang, Heng},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wang, Nie, Huang - 2012 - High-Order Multi-Task Feature Learning to Identify Longitudinal Phenotypic Markers for Alzheimer's Disease Pro.pdf:pdf},
journal = {Advances in \ldots},
pages = {1--9},
title = {{High-Order Multi-Task Feature Learning to Identify Longitudinal Phenotypic Markers for Alzheimer's Disease Progression Prediction}},
url = {http://machinelearning.wustl.edu/mlpapers/paper\_files/NIPS2012\_0621.pdf},
year = {2012}
}
@article{Schaffer1993,
author = {Schaffer, Cullen},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Schaffer - 1993 - Selecting a classification method by cross-validation.pdf:pdf},
journal = {Machine Learning},
keywords = {classification,cross-validation,decision trees,neural networks},
pages = {135--143},
title = {{Selecting a classification method by cross-validation}},
url = {http://link.springer.com/article/10.1007/BF00993106},
volume = {13},
year = {1993}
}
@inproceedings{Frome2013,
author = {Frome, Andrea and Corrado, Greg S. and Shlens, Jonathon and Bengio, Samy and Dean, Jeffrey and Ranzato, Marc Aurelio and Mikolov, Tomas},
booktitle = {NIPS},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Frome et al. - 2013 - Devise A deep visual-semantic embedding model.pdf:pdf},
pages = {2121--2129},
title = {{Devise: A deep visual-semantic embedding model}},
url = {http://papers.nips.cc/paper/5204-devise-a-deep-visual-semantic-embedding-model},
year = {2013}
}
@article{Hoffman2013,
abstract = {We develop stochastic variational inference, a scalable algorithm for approximating posterior distributions. We develop this technique for a large class of probabilistic models and we demonstrate it with two probabilistic topic models, latent Dirichlet allocation and the hierarchical Dirichlet process topic model. Using stochastic variational inference, we analyze several large collections of documents: 300K articles from Nature, 1.8M articles from The New York Times, and 3.8M articles from Wikipedia. Stochastic inference can easily handle data sets of this size and outperforms traditional variational inference, which can only handle a smaller subset. (We also show that the Bayesian nonparametric topic model outperforms its parametric counterpart.) Stochastic variational inference lets us apply complex Bayesian models to massive data sets.},
archivePrefix = {arXiv},
arxivId = {1206.7051},
author = {Hoffman, Matthew D. and Blei, David M. and Wang, Chong and Paisley, John},
eprint = {1206.7051},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hoffman et al. - 2013 - Stochastic Variational Inference.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
number = {2},
pages = {1303--1347},
title = {{Stochastic Variational Inference}},
url = {http://jmlr.org/papers/v14/hoffman13a.html$\backslash$nhttp://arxiv.org/abs/1206.7051},
volume = {14},
year = {2013}
}
@inproceedings{Zhou2007a,
author = {Zhou, Zhi-hua and Xu, Jun-Ming},
booktitle = {Proceedings of the 24th International Conference on Machine learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhou, Xu - 2007 - On the relation between multi-instance learning and semi-supervised learning.pdf:pdf},
number = {1997},
title = {{On the relation between multi-instance learning and semi-supervised learning}},
url = {http://dl.acm.org/citation.cfm?id=1273643},
year = {2007}
}
@article{Smola2003,
abstract = {We introduce a family of kernels on graphs based on the notion of regularization operators. This generalizes in a natural way the notion of regularization and Greens functions, as commonly used for real valued functions, to graphs. It turns out that diffusion kernels can be found as a special case of our reasoning. We show that the class of positive, monotonically decreasing functions on the unit interval leads to kernels and corresponding regularization operators.},
author = {Smola, Aj Alexander J and Kondor, Risi},
doi = {10.1007/978-3-540-45167-9\_12},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Smola, Kondor - 2003 - Kernels and Regularization on Graphs.pdf:pdf},
isbn = {3540407200},
issn = {03029743},
journal = {Machine Learning},
pages = {1--15},
title = {{Kernels and Regularization on Graphs}},
url = {http://www.springerlink.com/index/H96KDX90DCMM6FX0.pdf$\backslash$nhttp://link.springer.com/chapter/10.1007/978-3-540-45167-9\_12},
volume = {2777},
year = {2003}
}
@article{Wang2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1502.06309v1},
author = {Wang, Yu-xiang and Lei, Jing and Fienberg, Stephen E and Feb, M L},
eprint = {arXiv:1502.06309v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wang et al. - 2015 - Learning with Differential Privacy Stability , Learnability and the Sufficiency and Necessity of ERM Principle.pdf:pdf},
pages = {1--33},
title = {{Learning with Differential Privacy : Stability , Learnability and the Sufficiency and Necessity of ERM Principle}},
year = {2015}
}
@inproceedings{Jaakkola1999,
author = {Jaakkola, Tommi S. and Haussler, David},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Jaakkola, Haussler - 1999 - Exploiting generative models in discriminative classifiers.pdf:pdf},
pages = {487--493},
title = {{Exploiting generative models in discriminative classifiers}},
url = {http://www.uniroma2.it/didattica/BdDD/deposito/jaakkola98exploiting-haussler.pdf},
year = {1999}
}
@article{Leemis2008,
author = {Leemis, Lawrence M and McQueston, Jacquelyn T},
doi = {10.1198/000313008X270448},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Leemis, McQueston - 2008 - Univariate Distribution Relationships.pdf:pdf},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {asymptotic relationships,distribution proper-,limiting distributions,stochastic parameters,ties,transforma-},
month = feb,
number = {1},
pages = {45--53},
title = {{Univariate Distribution Relationships}},
url = {http://www.tandfonline.com/doi/abs/10.1198/000313008X270448},
volume = {62},
year = {2008}
}
@article{Hilario2006,
abstract = {Among the many applications of mass spectrometry, biomarker pattern discovery from protein mass spectra has aroused considerable interest in the past few years. While research efforts have raised hopes of early and less invasive diagnosis, they have also brought to light the many issues to be tackled before mass-spectra-based proteomic patterns become routine clinical tools. Known issues cover the entire pipeline leading from sample collection through mass spectrometry analytics to biomarker pattern extraction, validation, and interpretation. This study focuses on the data-analytical phase, which takes as input mass spectra of biological specimens and discovers patterns of peak masses and intensities that discriminate between different pathological states. We survey current work and investigate computational issues concerning the different stages of the knowledge discovery process: exploratory analysis, quality control, and diverse transforms of mass spectra, followed by further dimensionality reduction, classification, and model evaluation. We conclude after a brief discussion of the critical biomedical task of analyzing discovered discriminatory patterns to identify their component proteins as well as interpret and validate their biological implications.},
author = {Hilario, Melanie and Kalousis, Alexandros and Pellegrini, Christian and M\"{u}ller, Markus},
doi = {10.1002/mas.20072},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hilario et al. - 2006 - Processing and classification of protein mass spectra.pdf:pdf},
issn = {0277-7037},
journal = {Mass spectrometry reviews},
keywords = {Algorithms,Animals,Biological Markers,Computational Biology,Humans,Mass Spectrometry,Mass Spectrometry: classification,Mass Spectrometry: methods,Models, Chemical,Peptide Mapping,Proteins,Proteins: analysis,Proteomics},
number = {3},
pages = {409--49},
pmid = {16463283},
title = {{Processing and classification of protein mass spectra.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16463283},
volume = {25},
year = {2006}
}
@article{Tu2015,
author = {Tu, Enmei and Yang, Jie and Kasabov, Nicola and Zhang, Yaqian},
doi = {10.1016/j.neucom.2015.01.020},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Tu et al. - 2015 - Posterior Distribution Learning (PDL) A novel supervised learning framework using unlabeled samples to improve classi.pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Posterior distribution learning,Supervised learning,Supervised manifold classification},
pages = {173--186},
publisher = {Elsevier},
title = {{Posterior Distribution Learning (PDL): A novel supervised learning framework using unlabeled samples to improve classification performance}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0925231215000417},
volume = {157},
year = {2015}
}
@inproceedings{Li2011,
author = {Li, Yu-Feng and Zhou, Zhi-hua},
booktitle = {Proceedings of the 28th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Li, Zhou - 2011 - Towards making unlabeled data never hurt.pdf:pdf},
pages = {1081----1088},
title = {{Towards making unlabeled data never hurt}},
year = {2011}
}
@article{Bousquet2004,
author = {Bousquet, Olivier and Boucheron, Stephane and Lugosi, Gabor},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bousquet, Boucheron, Lugosi - 2004 - Introduction to statistical learning theory.pdf:pdf},
journal = {Lecture Notes in Computer Science},
pages = {169--207},
title = {{Introduction to statistical learning theory}},
url = {http://www.springerlink.com/index/CGW0K6W5W1W1WR9B.pdf},
volume = {3176},
year = {2004}
}
@phdthesis{Krijthe2012,
abstract = {In order to choose from the large number of classification methods available for use, cross-validation error estimates are often employed. We present this cross-validation selection strategy in the framework of meta-learning and show that conceptually, meta- learning techniques could provide better classifier selections than traditional cross-validation selection. Using various simulation studies we illustrate and discuss this possibility. Through a collection of datasets resembling real-world data, we investigate whether these improvements could possibly exist in the real-world as well. Although the approach presented here currently requires signifi- cant investment when applied to practical applications, the concept of being able to outperform cross-validation selection opens the door to new classifier selection strategies.},
author = {Krijthe, Jesse Hendrik},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Krijthe - 2012 - Improving Cross-Validation Classifier Selection Accuracy through Meta-Learning.pdf:pdf},
keywords = {Classifier Selection,Error estimation,Meta-Learning},
mendeley-tags = {Classifier Selection,Error estimation,Meta-Learning},
school = {Delft University of Technology},
title = {{Improving Cross-Validation Classifier Selection Accuracy through Meta-Learning}},
year = {2012}
}
@article{Armagan2013,
author = {Armagan, a. and Dunson, D. B. and Lee, J. and Bajwa, W. U. and Strawn, N.},
doi = {10.1093/biomet/ast028},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Armagan et al. - 2013 - Posterior consistency in linear models under shrinkage priors.pdf:pdf},
issn = {0006-3444},
journal = {Biometrika},
month = jul,
number = {4},
pages = {1011--1018},
title = {{Posterior consistency in linear models under shrinkage priors}},
url = {http://biomet.oxfordjournals.org/cgi/doi/10.1093/biomet/ast028},
volume = {100},
year = {2013}
}
@article{Raudys1998,
author = {Raudys, Sarunas and Duin, Robert P.W.},
doi = {10.1016/S0167-8655(98)00016-6},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Raudys, Duin - 1998 - Expected classification error of the Fisher linear classifier with pseudo-inverse covariance matrix.pdf:pdf},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Dimensionality,fisher linear discriminant,generalization error,pseudo-inversion,sample size,scissors effect,statistical classification},
month = apr,
number = {5-6},
pages = {385--392},
publisher = {Elsevier},
title = {{Expected classification error of the Fisher linear classifier with pseudo-inverse covariance matrix}},
volume = {19},
year = {1998}
}
@article{Krahenbuhl2011,
abstract = {Most state-of-the-art techniques for multi-class image segmentation and labeling use conditional random fields defined over pixels or image regions. While regionlevel models often feature dense pairwise connectivity, pixel-level models are considerably larger and have only permitted sparse graph structures. In this paper, we consider fully connected CRF models defined on the complete set of pixels in an image. The resulting graphs have billions of edges, making traditional inference algorithms impractical. Our main contribution is a highly efficient approximate inference algorithm for fully connected CRF models in which the pairwise edge potentials are defined by a linear combination of Gaussian kernels. Our experiments demonstrate that dense connectivity at the pixel level substantially improves segmentation and labeling accuracy.},
archivePrefix = {arXiv},
arxivId = {1210.5644},
author = {Krahenbuhl, Philipp and Koltun, Vladlen},
eprint = {1210.5644},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Krahenbuhl, Koltun - 2011 - Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials.pdf:pdf},
isbn = {9781618395993},
journal = {Advances in Neural Information Processing Systems 24 (Proceedings of NIPS)},
keywords = {conditional random field,filtering,message passing,sampling,segmentation},
pages = {1--9},
title = {{Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials}},
year = {2011}
}
@article{JuliaFlores2013,
author = {{Julia Flores}, M. and G\'{a}mez, Jos\'{e} a. and Mart\'{\i}nez, Ana M.},
doi = {10.1016/j.ins.2013.10.007},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Julia Flores, G\'{a}mez, Mart\'{\i}nez - 2013 - Domains of competence of the semi-naive Bayesian network classifiers.pdf:pdf},
issn = {00200255},
journal = {Information Sciences},
keywords = {domains of competence,semi-naive bayesian network classifiers},
month = oct,
number = {October},
publisher = {Elsevier Inc.},
title = {{Domains of competence of the semi-naive Bayesian network classifiers}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0020025513007226},
year = {2013}
}
@article{Gama1995,
author = {Gama, Joao and Brazdil, Pavel B.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gama, Brazdil - 1995 - Characterization of Classification Algorithms.pdf:pdf},
journal = {Progress in Artificial Intelligence},
pages = {189--200},
title = {{Characterization of Classification Algorithms}},
url = {http://link.springer.com/chapter/10.1007/3-540-60428-6\_16},
year = {1995}
}
@article{Shibagaki2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1502.02344v1},
author = {Shibagaki, Atsushi},
eprint = {arXiv:1502.02344v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Shibagaki - 2015 - Approximately optimal selection of regularization parameters in cross-validation for regularized classifiers.pdf:pdf},
pages = {1--28},
title = {{Approximately optimal selection of regularization parameters in cross-validation for regularized classifiers}},
year = {2015}
}
@article{Yan2013,
author = {Yan, Yan and Rosales, R\'{o}mer and Fung, Glenn and Subramanian, Ramanathan and Dy, Jennifer},
doi = {10.1007/s10994-013-5412-1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Yan et al. - 2013 - Learning from multiple annotators with varying expertise.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
month = oct,
title = {{Learning from multiple annotators with varying expertise}},
url = {http://link.springer.com/10.1007/s10994-013-5412-1},
year = {2013}
}
@article{Wilson,
archivePrefix = {arXiv},
arxivId = {arXiv:1302.4245v2},
author = {Wilson, Andrew Gordon and Adams, Ryan Prescott},
eprint = {arXiv:1302.4245v2},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wilson, Adams - Unknown - Gaussian Process Covariance Kernels for Pattern Discovery and Extrapolation.pdf:pdf},
title = {{Gaussian Process Covariance Kernels for Pattern Discovery and Extrapolation}}
}
@article{Amores2013,
abstract = {Multiple Instance Learning (MIL) has become an important topic in the pattern recognition community, and many solutions to this problem have been proposed until now. Despite this fact, there is a lack of comparative studies that shed light into the characteristics and behavior of the different methods. In this work we provide such an analysis focused on the classification task (i.e., leaving out other learning tasks such as regression). In order to perform our study, we implemented fourteen methods grouped into three different families. We analyze the performance of the approaches across a variety of well-known databases, and we also study their behavior in synthetic scenarios in order to highlight their characteristics. As a result of this analysis, we conclude that methods that extract global bag-level information show a clearly superior performance in general. In this sense, the analysis permits us to understand why some types of methods are more successful than others, and it permits us to establish guidelines in the design of new MIL methods. ?? 2013 Elsevier B.V.},
author = {Amores, Jaume},
doi = {10.1016/j.artint.2013.06.003},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Amores - 2013 - Multiple instance classification Review, taxonomy and comparative study.pdf:pdf},
issn = {00043702},
journal = {Artificial Intelligence},
keywords = {Bag-of-Words,Codebook,Multi-instance learning},
pages = {81--105},
publisher = {Elsevier B.V.},
title = {{Multiple instance classification: Review, taxonomy and comparative study}},
url = {http://dx.doi.org/10.1016/j.artint.2013.06.003},
volume = {201},
year = {2013}
}
@article{Chatterjee2014a,
author = {Chatterjee, Sourav},
doi = {10.1214/14-AOS1254},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Chatterjee - 2014 - A new perspective on least squares under convex constraint.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
keywords = {and phrases,convex constraint,empirical pro-,least squares,maximum likelihood},
month = dec,
number = {6},
pages = {2340--2381},
title = {{A new perspective on least squares under convex constraint}},
url = {http://projecteuclid.org/euclid.aos/1413810730},
volume = {42},
year = {2014}
}
@article{Cucker2007,
abstract = {Abstract.  We describe a setting where convergence to consensus in a population of autonomous agents can be formally addressed and prove$\backslash$nsome general results establishing conditions under which such convergence occurs. Both continuous and discrete time are considered$\backslash$nand a number of particular examples, notably the way in which a population of animals move together, are considered as particular$\backslash$ninstances of our setting.},
archivePrefix = {arXiv},
arxivId = {arXiv:1503.04098v1},
author = {Cucker, Felipe and Smale, Steve},
doi = {10.1007/s11537-007-0647-x},
eprint = {arXiv:1503.04098v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cucker, Smale - 2007 - On the mathematics of emergence.pdf:pdf},
isbn = {0289-2316},
issn = {02892316},
journal = {Japanese Journal of Mathematics},
keywords = {Consensus reaching,Emergence,Flocking},
number = {1},
pages = {197--227},
title = {{On the mathematics of emergence}},
volume = {2},
year = {2007}
}
@article{Zhao2013a,
author = {Zhao, Junlong and Leng, Chenlei and Li, Lexin and Wang, Hansheng},
doi = {10.1214/13-AOS1165},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhao et al. - 2013 - High-dimensional influence measure.pdf:pdf},
journal = {The Annals of Statistics},
number = {5},
pages = {2639--2667},
title = {{High-dimensional influence measure}},
url = {http://projecteuclid.org/euclid.aos/1384871348},
volume = {41},
year = {2013}
}
@inproceedings{Druck2010,
author = {Druck, Gregory and McCallum, Andrew Kachites},
booktitle = {Proceedings of the 27th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Druck, McCallum - 2010 - High-performance semi-supervised learning using discriminatively constrained generative models.pdf:pdf},
pages = {319--326},
title = {{High-performance semi-supervised learning using discriminatively constrained generative models}},
url = {http://www.cs.umass.edu/~gdruck/pubs/druck10high.pdf http://machinelearning.wustl.edu/mlpapers/paper\_files/icml2010\_DruckM10.pdf},
year = {2010}
}
@inproceedings{Matti2006,
author = {Kaariainen, Matti},
booktitle = {International Joint Conference on Neural Networks},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kaariainen - 2006 - Semi-Supervised Model Selection Based on Cross-Validation.pdf:pdf},
number = {510},
title = {{Semi-Supervised Model Selection Based on Cross-Validation}},
year = {2006}
}
@article{Halkidi2001,
author = {Halkidi, Maria},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Halkidi - 2001 - On Clustering Validation Techniques.pdf:pdf},
keywords = {cluster validity,clustering algorithms,unsupervised learning,validity indices},
pages = {107--145},
title = {{On Clustering Validation Techniques}},
year = {2001}
}
@article{Horton2007,
abstract = {Missing data are a recurring problem that can cause bias or lead to inefficient analyses. Development of statistical methods to address missingness have been actively pursued in recent years, including imputation, likelihood and weighting approaches. Each approach is more complicated when there are many patterns of missing values, or when both categorical and continuous random variables are involved. Implementations of routines to incorporate observations with incomplete variables in regression models are now widely available. We review these routines in the context of a motivating example from a large health services research dataset. While there are still limitations to the current implementations, and additional efforts are required of the analyst, it is feasible to incorporate partially observed values, and these methods should be utilized in practice.},
author = {Horton, Nicholas J and Kleinman, Ken P},
doi = {10.1198/000313007X172556},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Horton, Kleinman - 2007 - Much ado about nothing A comparison of missing data methods and software to fit incomplete data regression mod.pdf:pdf},
isbn = {000313007X},
issn = {0003-1305},
journal = {The American statistician},
keywords = {conditional gaussian,health services research,maximum likelihood,multiple imputation,psychiatric epidemi-},
month = feb,
number = {1},
pages = {79--90},
pmid = {17401454},
title = {{Much ado about nothing: A comparison of missing data methods and software to fit incomplete data regression models.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=1839993\&tool=pmcentrez\&rendertype=abstract},
volume = {61},
year = {2007}
}
@article{Rosasco2004,
abstract = {In this letter, we investigate the impact of choosing different loss functions from the viewpoint of statistical learning theory. We introduce a convexity assumption, which is met by all loss functions commonly used in the literature, and study how the bound on the estimation error changes with the loss. We also derive a general result on the minimizer of the expected risk for a convex loss function in the case of classification. The main outcome of our analysis is that for classification, the hinge loss appears to be the loss of choice. Other things being equal, the hinge loss leads to a convergence rate practically indistinguishable from the logistic loss rate and much better than the square loss rate. Furthermore, if the hypothesis space is sufficiently rich, the bounds obtained for the hinge loss are not loosened by the thresholding stage.},
author = {Rosasco, Lorenzo and {De Vito}, Ernesto and Caponnetto, Andrea and Piana, Michele and Verri, Alessandro},
doi = {10.1162/089976604773135104},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Rosasco et al. - 2004 - Are loss functions all the same.pdf:pdf},
issn = {0899-7667},
journal = {Neural computation},
keywords = {Learning,Learning: physiology,Linear Models,Models, Neurological,Statistics as Topic},
month = may,
number = {5},
pages = {1063--76},
pmid = {15070510},
title = {{Are loss functions all the same?}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15070510},
volume = {16},
year = {2004}
}
@book{Hastie2001,
author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome H.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hastie, Tibshirani, Friedman - 2001 - The Elements of Statistical Learning.pdf:pdf},
publisher = {Spinger},
title = {{The Elements of Statistical Learning}},
year = {2001}
}
@article{Hoogerbrugge1983,
author = {Hoogerbrugge, Ronald and Willig, Simon J. and Kistemaker, Piet G.},
doi = {10.1021/ac00261a016},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hoogerbrugge, Willig, Kistemaker - 1983 - Discriminant analysis by double stage principal component analysis.pdf:pdf},
issn = {0003-2700},
journal = {Analytical Chemistry},
month = sep,
number = {11},
pages = {1710--1712},
title = {{Discriminant analysis by double stage principal component analysis}},
url = {http://pubs.acs.org/doi/abs/10.1021/ac00261a016},
volume = {55},
year = {1983}
}
@article{White1982,
author = {White, Halbert},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/White - 1982 - Maximum Likelihood Estimation of Misspecified Models.pdf:pdf},
journal = {Econometrica},
number = {1},
pages = {1--25},
title = {{Maximum Likelihood Estimation of Misspecified Models}},
url = {http://www.jstor.org/stable/10.2307/1912526},
volume = {50},
year = {1982}
}
@article{Balsubramani,
archivePrefix = {arXiv},
arxivId = {arXiv:1503.01811v1},
author = {Balsubramani, Akshay and Freund, Yoav},
eprint = {arXiv:1503.01811v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Balsubramani, Freund - Unknown - Optimally Combining Classifiers Using Unlabeled Data.pdf:pdf},
title = {{Optimally Combining Classifiers Using Unlabeled Data}}
}
@inproceedings{Bennett1998,
author = {Bennett, Kristin P. and Demiriz, Ayhan},
booktitle = {Advances in Neural Information Processing Systems 11},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bennett, Demiriz - 1998 - Semi-supervised support vector machines.pdf:pdf},
pages = {368--374},
title = {{Semi-supervised support vector machines}},
year = {1998}
}
@article{Reid2009a,
abstract = {We unify f-divergences, Bregman divergences, surrogate loss bounds (regret bounds), proper scoring rules, matching losses, cost curves, ROC-curves and information. We do this by systematically studying integral and variational representations of these objects and in so doing identify their primitives which all are related to cost-sensitive binary classification. As well as clarifying relationships between generative and discriminative views of learning, the new machinery leads to tight and more general surrogate loss bounds and generalised Pinsker inequalities relating f-divergences to variational divergence. The new viewpoint illuminates existing algorithms: it provides a new derivation of Support Vector Machines in terms of divergences and relates Maximum Mean Discrepancy to Fisher Linear Discriminants. It also suggests new techniques for estimating f-divergences.},
archivePrefix = {arXiv},
arxivId = {0901.0356},
author = {Reid, Mark D. and Williamson, Robert C.},
eprint = {0901.0356},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Reid, Williamson - 2009 - Information, Divergence and Risk for Binary Experiments.pdf:pdf},
issn = {1532-4435},
keywords = {classification,divergence,loss functions,regret bounds,statistical information},
pages = {89},
title = {{Information, Divergence and Risk for Binary Experiments}},
url = {http://arxiv.org/abs/0901.0356},
volume = {12},
year = {2009}
}
@inproceedings{Duchi2008,
author = {Duchi, John and Shalev-Shwartz, Shai},
booktitle = {Proceedings of the 25th international conference on Machine learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Duchi, Shalev-Shwartz - 2008 - Efficient Projections onto the ℓ1-Ball for Learning in High Dimensions.pdf:pdf},
pages = {272--279},
title = {{Efficient Projections onto the ℓ1-Ball for Learning in High Dimensions}},
url = {http://dl.acm.org/citation.cfm?id=1390191},
year = {2008}
}
@article{Azizyan2013,
author = {Azizyan, Martin and Singh, Aarti and Wasserman, Larry},
doi = {10.1214/13-AOS1092},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Azizyan, Singh, Wasserman - 2013 - Density-sensitive semisupervised inference.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
month = apr,
number = {2},
pages = {751--771},
title = {{Density-sensitive semisupervised inference}},
url = {http://projecteuclid.org/euclid.aos/1368018172},
volume = {41},
year = {2013}
}
@article{Cevher2014,
abstract = {This article reviews recent advances in convex optimization algorithms for Big Data, which aim to reduce the computational, storage, and communications bottlenecks. We provide an overview of this emerging field, describe contemporary approximation techniques like first-order methods and randomization for scalability, and survey the important role of parallel and distributed computation. The new Big Data algorithms are based on surprisingly simple principles and attain staggering accelerations even on classical problems.},
archivePrefix = {arXiv},
arxivId = {1411.0972},
author = {Cevher, Volkan and Becker, Stephen and Schmidt, Mark},
doi = {10.1109/MSP.2014.2329397},
eprint = {1411.0972},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cevher, Becker, Schmidt - 2014 - Convex Optimization for Big Data.pdf:pdf},
month = nov,
number = {September},
pages = {23},
title = {{Convex Optimization for Big Data}},
url = {http://arxiv.org/abs/1411.0972},
year = {2014}
}
@article{Geer2009,
archivePrefix = {arXiv},
arxivId = {arXiv:0910.0722v1},
author = {Geer, Sara Van De and B\"{u}hlmann, Peter},
eprint = {arXiv:0910.0722v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Geer, B\"{u}hlmann - 2009 - On the conditions used to prove oracle results for the Lasso.pdf:pdf},
journal = {Electronic Journal of Statistics},
keywords = {and phrases,coherence,compatibility,irrepresentable condition,lasso,re-,restricted isometry,sparsity,stricted eigenvalue},
pages = {1--33},
title = {{On the conditions used to prove oracle results for the Lasso}},
url = {http://projecteuclid.org/euclid.ejs/1260801227},
year = {2009}
}
@article{Koyamada2014,
abstract = {We present a novel algorithm(Principal Sensitivity Analysis; PSA) to analyze the knowledge of the classifier obtained from supervised machine learning technique. In particular, we define principal sensitivity map (PSM) as the direction on the input space to which the trained classifier is most sensitive, and use analogously defined k-th PSM to define a basis for the input space. We train neural networks with artificial data and real data, and apply the algorithm to the obtained supervised classifiers. We then visualize the PSMs to demonstrate the PSA's ability to decompose the knowledge acquired by the trained classifiers.},
archivePrefix = {arXiv},
arxivId = {1412.6785},
author = {Koyamada, Sotetsu and Koyama, Masanori and Nakae, Ken and Ishii, Shin},
eprint = {1412.6785},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Koyamada et al. - 2014 - Principal Sensitivity Analysis.pdf:pdf},
keywords = {dark knowledge,knowl-,pca,sensitibity analysis,sensitivity map},
month = dec,
pages = {1--13},
title = {{Principal Sensitivity Analysis}},
url = {http://arxiv.org/abs/1412.6785},
year = {2014}
}
@article{Haffari2012,
author = {Haffari, Gholamreza and Sarkar, Anoop},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Haffari, Sarkar - 2012 - Analysis of semi-supervised learning with the yarowsky algorithm.pdf:pdf},
journal = {arXiv preprint},
title = {{Analysis of semi-supervised learning with the yarowsky algorithm}},
url = {http://arxiv.org/abs/1206.5240},
year = {2012}
}
@inproceedings{Brazdil1994,
author = {Brazdil, Pavel B. and Gama, Joao and Henery, Bob},
booktitle = {Proceedings of the European conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Brazdil, Gama, Henery - 1994 - Characterizing the applicability of classification algorithms using meta-level learning.pdf:pdf},
pages = {83--102},
title = {{Characterizing the applicability of classification algorithms using meta-level learning}},
url = {http://link.springer.com/chapter/10.1007/3-540-57868-4\_52},
year = {1994}
}
@inproceedings{Furnkranz2001,
author = {F\"{u}rnkranz, Johannes and Petrak, Johann},
booktitle = {Proceedings of the ECML/PKDD Workshop on Integrating Aspects of Data Mining, Decision Support and Meta-Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/F\"{u}rnkranz, Petrak - 2001 - An Evaluation of Landmarking Variants.pdf:pdf},
pages = {57--68},
title = {{An Evaluation of Landmarking Variants}},
url = {http://ai.ijs.si/branax/iddm-2001-proceedings/paper9.pdf},
year = {2001}
}
@article{Smola2005,
abstract = {We present methods for dealing with missing variables in the context
  of Gaussian Processes and Support Vector Machines. This solves an
  important problem which has largely been ignored by kernel methods:
  How to systematically deal with incomplete data? Our method can also
  be applied to problems with partially observed labels as well as to
  the transductive setting where we view the labels as missing data.
  
  Our approach relies on casting kernel methods as an estimation
  problem in exponential families. Hence, estimation with missing
  variables becomes a problem of computing marginal distributions, and
  finding efficient optimization methods. To that extent we propose an
  optimization scheme which extends the Concave Convex Procedure (CCP)
  of Yuille and Rangarajan, and present a simplified and intuitive
  proof of its convergence. We show how our algorithm can be
  specialized to various cases in order to efficiently solve the
  optimization problems that arise. Encouraging preliminary
  experimental results on the USPS dataset are also presented.},
author = {Smola, Alex and Vishwanathan, S V N and Hoffman, Thomas},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Smola, Vishwanathan, Hoffman - 2005 - Kernel Methods for Missing Variables.pdf:pdf},
isbn = {097273581X},
keywords = {Learning/Statistics \& Optimisation,Theory \& Algorithms},
title = {{Kernel Methods for Missing Variables}},
url = {http://eprints.pascal-network.org/archive/00002053/},
year = {2005}
}
@article{Dean,
archivePrefix = {arXiv},
arxivId = {arXiv:1503.02531v1},
author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
eprint = {arXiv:1503.02531v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hinton, Vinyals, Dean - Unknown - Distilling the Knowledge in a Neural Network.pdf:pdf},
pages = {1--9},
title = {{Distilling the Knowledge in a Neural Network}}
}
@article{Greenland1985,
abstract = {The authors examine some recently proposed criteria for determining when to adjust for covariates related to misclassification, and show these criteria to be incorrect. In particular, they show that when misclassification is present, covariate control can sometimes increase net bias, even when the covariate would have been a confounder under perfect classification, and even if the covariate is a determinant of classification. Thus, bias due to misclassification cannot be adequately dealt with by the methods used for control of confounding. The examples presented also show that the "change-in-estimate" criterion for deciding whether to control a covariate can be systematically misleading when misclassification is present. These results demonstrate that it is necessary to consider the degree of misclassification when deciding whether to control a covariate.},
author = {Greenland, S and Robins, J M},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Greenland, Robins - 1985 - Confounding and misclassification.pdf:pdf},
issn = {0002-9262},
journal = {American journal of epidemiology},
number = {3},
pages = {495--506},
pmid = {4025298},
title = {{Confounding and misclassification.}},
volume = {122},
year = {1985}
}
@article{Adams2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1502.01664v1},
author = {Adams, Niall M},
eprint = {arXiv:1502.01664v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Adams - 2014 - Estimating Optimal Active Learning via Model Retraining.pdf:pdf},
keywords = {active learning,classification,estimation framework,ex-,model retraining improvement,pected loss reduction},
pages = {1--36},
title = {{Estimating Optimal Active Learning via Model Retraining}},
volume = {15},
year = {2014}
}
@unpublished{Amasyali2009,
author = {Amasyali, M Fatih and Ersoy, Okan K.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Amasyali, Ersoy - 2009 - A Study of Meta Learning for Regression.pdf:pdf},
institution = {Purdue University},
title = {{A Study of Meta Learning for Regression}},
year = {2009}
}
@article{Cheng2014,
author = {Cheng, Qiang and Zhou, Hongbo and Cheng, Jie and Li, Huiqing},
doi = {10.1109/TPAMI.2014.2327978},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cheng et al. - 2014 - A Minimax Framework for Classification with Applications to Images and High Dimensional Data.pdf:pdf},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {c},
pages = {1--1},
title = {{A Minimax Framework for Classification with Applications to Images and High Dimensional Data}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6824834},
volume = {8828},
year = {2014}
}
@article{Culp2008b,
abstract = {Graph-based learning provides a useful approach for modeling data in classification problems. In this modeling scenario, the relationship between labeled and unlabeled data impacts the construction and performance of classifiers, and therefore a semi-supervised learning framework is adopted. We propose a graph classifier based on kernel smoothing. A regularization framework is also introduced, and it is shown that the proposed classifier optimizes certain loss functions. Its performance is assessed on several synthetic and real benchmark data sets with good results, especially in settings where only a small fraction of the data are labeled.},
author = {Culp, Mark and Michailidis, George},
doi = {10.1109/TPAMI.2007.70765},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Culp, Michailidis - 2008 - Graph-based semisupervised learning.pdf:pdf},
issn = {0162-8828},
journal = {IEEE transactions on pattern analysis and machine intelligence},
keywords = {Algorithms,Artificial Intelligence,Cluster Analysis,Computer Simulation,Data Interpretation, Statistical,Models, Statistical,Pattern Recognition, Automated,Pattern Recognition, Automated: methods},
month = jan,
number = {1},
pages = {174--9},
pmid = {18000333},
title = {{Graph-based semisupervised learning.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/18000333},
volume = {30},
year = {2008}
}
@article{Rasmussen2005,
author = {Rasmussen, Carl Edward and Williams, Christopher K I},
publisher = {MIT Press},
title = {{Gaussian Processes for Machine Learning}},
year = {2005}
}
@phdthesis{Macia2011,
author = {Macia, Nuria},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Macia - 2011 - Data Complexity in Supervised Learning A Far-Reaching Implication.pdf:pdf},
title = {{Data Complexity in Supervised Learning: A Far-Reaching Implication}},
year = {2011}
}
@article{Wang2013,
author = {Wang, Jun and Jebara, Tony and Chang, Shih-Fu},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wang, Jebara, Chang - 2013 - Semi-Supervised Learning Using Greedy Max-Cut.pdf:pdf},
journal = {Journal of Machine Learning Research},
pages = {771--800},
title = {{Semi-Supervised Learning Using Greedy Max-Cut}},
url = {http://www.ee.columbia.edu/ln/dvmm/publications/13/ggmc\_13.pdf},
volume = {14},
year = {2013}
}
@inproceedings{Hoekstra1996,
author = {Hoekstra, Aarnoud and Duin, Robert P.W.},
booktitle = {Proceedings of the 13th International Conference on Pattern Recognition},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hoekstra, Duin - 1996 - On the nonlinearity of pattern classifiers.pdf:pdf},
pages = {271--275},
title = {{On the nonlinearity of pattern classifiers}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=547429},
year = {1996}
}
@article{Xu2011,
abstract = {We consider two desired properties of learning algorithms: *sparsity* and *algorithmic stability*. Both properties are believed to lead to good generalization ability. We show that these two properties are fundamentally at odds with each other: a sparse algorithm cannot be stable and vice versa. Thus, one has to trade off sparsity and stability in designing a learning algorithm. In particular, our general result implies that \$\backslash ell\_1\$-regularized regression (Lasso) cannot be stable, while \$\backslash ell\_2\$-regularized regression is known to have strong stability properties and is therefore not sparse.},
author = {Xu, Huan and Caramanis, Constantine and Mannor, Shie},
doi = {10.1109/TPAMI.2011.177},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Xu, Caramanis, Mannor - 2011 - Sparse Algorithms are not Stable A No-free-lunch Theorem.pdf:pdf},
issn = {1939-3539},
journal = {IEEE transactions on pattern analysis and machine intelligence},
month = aug,
pages = {1--8},
pmid = {21844627},
title = {{Sparse Algorithms are not Stable: A No-free-lunch Theorem.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21844627},
year = {2011}
}
@inproceedings{Rohrbach2013,
author = {Rohrbach, Marcus and Ebert, Sandra and Schiele, Bernt},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Rohrbach, Ebert, Schiele - 2013 - Transfer Learning in a Transductive Setting.pdf:pdf},
pages = {46--54},
title = {{Transfer Learning in a Transductive Setting}},
url = {http://papers.nips.cc/paper/5209-transfer-learning-in-a-transductive-setting},
year = {2013}
}
@article{Carlo2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1502.07645v1},
author = {Carlo, Monte and Feb, M L},
eprint = {arXiv:1502.07645v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Carlo, Feb - 2015 - Privacy for Free Posterior Sampling and Stochastic Gradient.pdf:pdf},
pages = {1--27},
title = {{Privacy for Free : Posterior Sampling and Stochastic Gradient}},
year = {2015}
}
@article{Jager2013a,
abstract = {The accuracy of published medical research is critical for scientists, physicians and patients who rely on these results. However, the fundamental belief in the medical literature was called into serious question by a paper suggesting that most published medical research is false. Here we adapt estimation methods from the genomics community to the problem of estimating the rate of false discoveries in the medical literature using reported \$P\$-values as the data. We then collect \$P\$-values from the abstracts of all 77 430 papers published in The Lancet, The Journal of the American Medical Association, The New England Journal of Medicine, The British Medical Journal, and The American Journal of Epidemiology between 2000 and 2010. Among these papers, we found 5322 reported \$P\$-values. We estimate that the overall rate of false discoveries among reported results is 14\% (s.d. 1\%), contrary to previous claims. We also found that there is no a significant increase in the estimated rate of reported false discovery results over time (0.5\% more false positives (FP) per year, \$P = 0.18\$) or with respect to journal submissions (0.5\% more FP per 100 submissions, \$P = 0.12\$). Statistical analysis must allow for false discoveries in order to make claims on the basis of noisy data. But our analysis suggests that the medical literature remains a reliable record of scientific progress.},
author = {Jager, Leah R and Leek, Jeffrey T},
doi = {10.1093/biostatistics/kxt007},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Jager, Leek - 2013 - An estimate of the science-wise false discovery rate and application to the top medical literature.pdf:pdf},
issn = {1468-4357},
journal = {Biostatistics (Oxford, England)},
keywords = {false discovery rate,genomics,meta-analysis,multiple testing,science-wise false discovery rate},
month = sep,
pages = {1--12},
pmid = {24068246},
title = {{An estimate of the science-wise false discovery rate and application to the top medical literature.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24068246},
year = {2013}
}
@article{Samworth2012,
author = {Samworth, Richard J.},
doi = {10.1214/12-AOS1049},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Samworth - 2012 - Optimal weighted nearest neighbour classifiers.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
month = oct,
number = {5},
pages = {2733--2763},
title = {{Optimal weighted nearest neighbour classifiers}},
url = {http://projecteuclid.org/euclid.aos/1359987536},
volume = {40},
year = {2012}
}
@article{Hay2008,
abstract = {There is mounting evidence of a gap between Evidence-based Medicine (EBM) and physician clinical practice, in part because EBM is averaged global evidence gathered from exogenous populations which may not be relevant to local circumstances. Local endogenous evidence, collected in particular and 'real world' patient populations may be more relevant, convincing and timely for clinical practice. Evidence Farming (EF) is a concept to provide such local evidence through the systematic collection of clinical experience to guide more effective practice.},
author = {Hay, M Cameron and Weisner, Thomas S and Subramanian, Saskia and Duan, Naihua and Niedzinski, Edmund J and Kravitz, Richard L},
doi = {10.1111/j.1365-2753.2008.01009.x},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hay et al. - 2008 - Harnessing experience exploring the gap between evidence-based medicine and clinical practice.pdf:pdf},
issn = {1365-2753},
journal = {Journal of evaluation in clinical practice},
keywords = {Attitude of Health Personnel,California,Clinical Medicine,Clinical Medicine: education,Clinical Medicine: organization \& administration,Data Collection,Decision Making,Decision Support Techniques,Diffusion of Innovation,Evidence-Based Medicine,Evidence-Based Medicine: education,Evidence-Based Medicine: organization \& administra,Focus Groups,Health Knowledge, Attitudes, Practice,Health Services Needs and Demand,Humans,Information Storage and Retrieval,Outcome Assessment (Health Care),Patient Care Planning,Physicians,Physicians: psychology,Pilot Projects,Practice Guidelines as Topic,Questionnaires,Randomized Controlled Trials as Topic},
month = oct,
number = {5},
pages = {707--13},
pmid = {19018899},
title = {{Harnessing experience: exploring the gap between evidence-based medicine and clinical practice.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19018899},
volume = {14},
year = {2008}
}
@article{Chaudhuri2006,
author = {Chaudhuri, Kamalika and Healy, DG and Schapira, AHV},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Chaudhuri, Healy, Schapira - 2006 - Non-motor symptoms of Parkinson's disease diagnosis and management.pdf:pdf},
journal = {The Lancet Neurology},
pages = {235--245},
title = {{Non-motor symptoms of Parkinson's disease : diagnosis and management}},
url = {http://www.sciencedirect.com/science/article/pii/S1474442206703738},
volume = {5},
year = {2006}
}
@article{Vandewalle2013,
author = {Vandewalle, Vincent and Biernacki, Christophe},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Vandewalle, Biernacki - 2013 - A predictive deviance criterion for selecting a generative model in semi-supervised classification.pdf:pdf},
journal = {Computational Statistics \& Data Analysis},
pages = {220--236},
title = {{A predictive deviance criterion for selecting a generative model in semi-supervised classification}},
url = {http://www.sciencedirect.com/science/article/pii/S0167947313000546},
volume = {64},
year = {2013}
}
@article{Seaman2013,
author = {Seaman, Shaun and Galati, John and Jackson, Dan and Carlin, John},
doi = {10.1214/13-STS415},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Seaman et al. - 2013 - What Is Meant by “Missing at Random”.pdf:pdf},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {Ignorability, direct-likelihood inference, frequen,and phrases,direct-likelihood inference,frequen-,ignorability,missing completely at random,repeated sampling,tist inference},
month = may,
number = {2},
pages = {257--268},
title = {{What Is Meant by “Missing at Random”?}},
url = {http://projecteuclid.org/euclid.ss/1369147915},
volume = {28},
year = {2013}
}
@inproceedings{Abney2002,
author = {Abney, Steven},
booktitle = {Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Abney - 2002 - Bootstrapping.pdf:pdf},
number = {July},
pages = {360--367},
title = {{Bootstrapping}},
year = {2002}
}
@article{Lin2008,
author = {Lin, Chih-jen and Weng, Ruby C},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lin, Weng - 2008 - Trust Region Newton Method for Large-Scale Logistic Regression.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {conjugate gradient,logistic regression,newton method,support,trust region},
pages = {627--650},
title = {{Trust Region Newton Method for Large-Scale Logistic Regression}},
volume = {9},
year = {2008}
}
@article{Belkin2006,
author = {Belkin, Mikhail and Niyogi, Partha and Sindhwani, Vikas},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Belkin, Niyogi, Sindhwani - 2006 - Manifold regularization A geometric framework for learning from labeled and unlabeled examples.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {fold learning,graph transduction,kernel methods,mani-,regularization,semi-supervised learning,spectral graph theory,support vector machines,unlabeled data},
pages = {2399--2434},
title = {{Manifold regularization: A geometric framework for learning from labeled and unlabeled examples}},
url = {http://dl.acm.org/citation.cfm?id=1248632},
volume = {7},
year = {2006}
}
@article{Tsao2014,
author = {Tsao, M. and Wu, F.},
doi = {10.1093/biomet/asu014},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Tsao, Wu - 2014 - Extended empirical likelihood for estimating equations.pdf:pdf},
issn = {0006-3444},
journal = {Biometrika},
month = jun,
number = {3},
pages = {703--710},
title = {{Extended empirical likelihood for estimating equations}},
url = {http://biomet.oxfordjournals.org/cgi/doi/10.1093/biomet/asu014},
volume = {101},
year = {2014}
}
@unpublished{DeDeo2014,
abstract = {A recurring problem with statistical prediction for policy-making is that many useful variables are associated with others on which it would be ethically problematic to base decisions. This problem becomes particularly acute in the Big Data era, when predictions are often made in the absence of strong theories for the underlying causal mechanisms. Given this, we show how to use information theory to construct the distribution closest in predictive power to the full distribution, but in which predictions---and thus policy outcomes, provision of services, and so forth---are not correlated with protected variables.},
archivePrefix = {arXiv},
arxivId = {1412.4643},
author = {DeDeo, Simon},
eprint = {1412.4643},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/DeDeo - 2014 - Wrong side of the tracks Big Data and Protected Categories.pdf:pdf},
month = dec,
pages = {3},
title = {{"Wrong side of the tracks": Big Data and Protected Categories}},
url = {http://arxiv.org/abs/1412.4643},
year = {2014}
}
@article{Heller2012,
author = {Heller, Ruth and Heller, Yair and Gorfine, Malka},
doi = {10.1093/biomet/ass070},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Heller, Heller, Gorfine - 2012 - A consistent multivariate test of association based on ranks of distances.pdf:pdf},
issn = {0006-3444},
journal = {Biometrika},
month = dec,
number = {2},
pages = {503--510},
title = {{A consistent multivariate test of association based on ranks of distances}},
url = {http://biomet.oxfordjournals.org/cgi/doi/10.1093/biomet/ass070},
volume = {100},
year = {2012}
}
@article{Dietterich1998,
author = {Dietterich, Thomas G},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Dietterich - 1998 - Approximate statistical tests for comparing supervised classification learning algorithms.pdf:pdf},
journal = {Neural computation},
title = {{Approximate statistical tests for comparing supervised classification learning algorithms}},
url = {http://www.mitpressjournals.org/doi/abs/10.1162/089976698300017197},
year = {1998}
}
@article{Loog2014b,
author={Loog, M. and Jensen, A.C.}, 
journal={IEEE Transactions on Neural Networks and Learning Systems}, 
title={Semi-Supervised Nearest Mean Classification Through a Constrained Log-Likelihood}, 
year={2015}, 
month={May}, 
volume={26}, 
number={5}, 
pages={995-1006}
}
@inproceedings{Collins1999,
author = {Collins, Michael and Singer, Yoram},
booktitle = {Proceedings of the joint SIGDAT conference on empirical methods in natural language processing and very large corpora},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Collins, Singer - 1999 - Unsupervised models for named entity classification.pdf:pdf},
pages = {189--196},
title = {{Unsupervised models for named entity classification}},
url = {http://acl.ldc.upenn.edu/W/W99/W99-0613.pdf?ref=Sawos.OrgR\%7B.\%EF\%BF\%BD\%EF\%BF\%BD\%EF\%BF\%BD\%EF\%BF\%BD\%C7\%9D\%EF\%BF\%BD\%E2\%80\%A1\%5E\%EF\%BF\%BD\%EF\%BF\%BD\%C3\%A87},
year = {1999}
}
@article{Gelman1999,
abstract = {Maps are frequently used to display spatial distributions of parameters of interest, such as cancer rates or average pollutant concentrations by county. It is well known that plotting observed rates can have serious drawbacks when sample sizes vary by area, since very high (and low) observed rates are found disproportionately in poorly-sampled areas. Unfortunately, adjusting the observed rates to account for the effects of small-sample noise can introduce an opposite effect, in which the highest adjusted rates tend to be found disproportionately in well-sampled areas. In either case, the maps can be difficult to interpret because the display of spatial variation in the underlying parameters of interest is confounded with spatial variation in sample sizes. As a result, spatial patterns occur in adjusted rates even if there is no spatial structure in the underlying parameters of interest, and adjusted rates tend to look too uniform in areas with little data. We introduce two models (normal and Poisson) in which parameters of interest have no spatial patterns, and demonstrate the existence of spatial artefacts in inference from these models. We also discuss spatial models and the extent to which they are subject to the same artefacts. We present examples from Bayesian modelling, but, as we explain, the artefacts occur generally.},
author = {Gelman, Andrew and Price, Phlllip N.},
doi = {10.1002/(SICI)1097-0258(19991215)18:23<3221::AID-SIM312>3.0.CO;2-M},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gelman, Price - 1999 - All maps of parameter estimates are misleading.pdf:pdf},
isbn = {0277-6715},
issn = {02776715},
journal = {Statistics in Medicine},
pages = {3221--3234},
pmid = {10602147},
title = {{All maps of parameter estimates are misleading}},
volume = {18},
year = {1999}
}
@book{Chapelle2006,
author = {Chapelle, Olivier and Sch\"{o}lkopf, Bernhard and Zien, Alexander},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Chapelle, Sch\"{o}lkopf, Zien - 2006 - Semi-supervised learning.pdf:pdf},
isbn = {9780262033589},
publisher = {MIT press},
title = {{Semi-supervised learning}},
year = {2006}
}
@article{Li2013a,
abstract = {In this paper, we study the heterogeneous domain adaptation (HDA) problem, in which the data from the source domain and the target domain are represented by heterogeneous features with different dimensions. By introducing two different projection matrices, we first transform the data from two domains into a common subspace such that the similarity between samples across different domains can be measured. We then develop two new feature mapping functions for two domains, which respectively augments the transformed source and target samples with their original features and padding zeros. Existing supervised learning methods (e.g., SVM and SVR) can be readily employed by incorporating our newly proposed augmented feature representations for supervised HDA. As a showcase, we propose a novel method called Heterogeneous Feature Augmentation (HFA) based on SVM. We show that the proposed formulation can be equivalently derived as a standard Multiple Kernel Learning (MKL) problem, which is convex and thus the global solution can be guaranteed. To additionally utilize the unlabeled data in the target domain, we further propose the semi-supervised HFA (SHFA) which can simultaneously learn the target classifier as well as infer the labels of unlabeled target samples. Comprehensive experiments on three different applications clearly demonstrate that our SHFA and HFA outperform the existing HDA methods.},
author = {Li, Wen and Duan, Lixin and Xu, Dong and Tsang, Ivor W},
doi = {3E685107-2977-47E4-B935-16F0A8864540},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Li et al. - 2013 - Learning with Augmented Features for Supervised and Semi-supervised Heterogeneous Domain Adaptation.pdf:pdf},
issn = {1939-3539},
journal = {IEEE transactions on pattern analysis and machine intelligence},
month = aug,
number = {6},
pages = {1134--1148},
pmid = {23999386},
title = {{Learning with Augmented Features for Supervised and Semi-supervised Heterogeneous Domain Adaptation.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23999386},
volume = {36},
year = {2013}
}
@article{Yamada2013a,
author = {Yamada, Makoto and Sugiyama, Masashi and Sese, Jun},
doi = {10.1007/s10994-013-5423-y},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Yamada, Sugiyama, Sese - 2013 - Least-squares independence regression for non-linear causal inference under non-Gaussian noise.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {causal inference,information,least-squares independence regression,non-gaussian,non-linear,squared-loss mutual},
month = nov,
title = {{Least-squares independence regression for non-linear causal inference under non-Gaussian noise}},
url = {http://link.springer.com/10.1007/s10994-013-5423-y},
year = {2013}
}
@article{Bengio2007,
author = {Bengio, Yoshua and LeCun, Yann},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bengio, LeCun - 2007 - Scaling Learning Algorithms towards AI.pdf:pdf},
journal = {Large-Scale Kernel Machines},
number = {1},
pages = {1--41},
title = {{Scaling Learning Algorithms towards AI}},
url = {http://www.iro.umontreal.ca/~lisa/bib/pub\_subject/language/pointeurs/bengio+lecun-chapter2007.pdf},
year = {2007}
}
@article{Itai1991,
author = {Itai, Alon},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Itai - 1991 - Learnability distributions with respect to fixed.pdf:pdf},
pages = {377--389},
title = {{Learnability distributions with respect to fixed}},
volume = {86},
year = {1991}
}
@inproceedings{Klinkenberg2001,
author = {Klinkenberg, Ralf},
booktitle = {Workshop notes of the IJCAI-01 Workshop on Learning from Temporal and Spatial Data},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Klinkenberg - 2001 - Using labeled and unlabeled data to learn drifting concepts.pdf:pdf},
pages = {16--24},
title = {{Using labeled and unlabeled data to learn drifting concepts}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.23.1798\&rep=rep1\&type=pdf},
year = {2001}
}
@article{Balcan2010,
author = {Balcan, Maria-Florina and Blum, Avrim},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Balcan, Blum - 2010 - A Discriminative Model for Semi-Supervised Learning.pdf:pdf},
journal = {Journal of the ACM (JACM)},
number = {3},
title = {{A Discriminative Model for Semi-Supervised Learning}},
url = {http://dl.acm.org/citation.cfm?id=1706599},
volume = {57},
year = {2010}
}
@article{Collobert2006,
author = {Collobert, Ronan and Sinz, Fabian and Weston, Jason and Bottou, Leon},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Collobert et al. - 2006 - Large scale transductive SVMs.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {cccp,semi-supervised learning,transduction,transductive svms},
pages = {1687--1712},
title = {{Large scale transductive SVMs}},
url = {http://dl.acm.org/citation.cfm?id=1248609},
volume = {7},
year = {2006}
}
@article{Williamson,
author = {Williamson, Robert C},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Williamson - Unknown - Loss Functions.pdf:pdf},
number = {3},
pages = {1--10},
title = {{Loss Functions}}
}
@article{Mooij2014,
archivePrefix = {arXiv},
arxivId = {1412.3773},
author = {Mooij, Joris M. and Peters, Jonas and Janzing, Dominik and Zscheischler, Jakob and Sch\"{o}lkopf, Bernhard},
eprint = {1412.3773},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Mooij et al. - 2014 - Distinguishing cause from effect using observational data methods and benchmarks.pdf:pdf},
month = dec,
title = {{Distinguishing cause from effect using observational data: methods and benchmarks}},
url = {http://arxiv.org/abs/1412.3773v1},
year = {2014}
}
@inproceedings{Fan2008,
author = {Fan, Bin and Lei, Zhen and Li, Stan Z.},
booktitle = {8th IEEE International Conference on Automatic Face \& Gesture Recognition},
doi = {10.1109/AFGR.2008.4813329},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Fan, Lei, Li - 2008 - Normalized LDA for semi-supervised learning.pdf:pdf},
isbn = {978-1-4244-2153-4},
month = sep,
pages = {1--6},
title = {{Normalized LDA for semi-supervised learning}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4813329},
year = {2008}
}
@article{Geyer1994,
author = {Geyer, Charles J.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Geyer - 1994 - On the asymptotics of constrained M-estimation.pdf:pdf},
journal = {The Annals of Statistics},
number = {4},
pages = {1993--2010},
title = {{On the asymptotics of constrained M-estimation}},
url = {http://www.jstor.org/stable/2242495},
volume = {22},
year = {1994}
}
@article{Fayyad1996,
author = {Fayyad, Usama and Piatetsky-shapiro, Gregory and Smyth, Padhraic},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Fayyad, Piatetsky-shapiro, Smyth - 1996 - From Data Mining to Knowledge Discovery in.pdf:pdf},
pages = {37--54},
title = {{From Data Mining to Knowledge Discovery in}},
year = {1996}
}
@article{Celisse2014,
author = {Celisse, Alain},
doi = {10.1214/14-AOS1240},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Celisse - 2014 - Optimal cross-validation in density estimation with the \$L\{2\}\$-loss.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
month = oct,
number = {5},
pages = {1879--1910},
title = {{Optimal cross-validation in density estimation with the \$L\^{}\{2\}\$-loss}},
url = {http://projecteuclid.org/euclid.aos/1410440628},
volume = {42},
year = {2014}
}
@article{Subramanya2011a,
author = {Subramanya, A and Bilmes, Jeff},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Subramanya, Bilmes - 2011 - Semi-supervised learning with measure propagation.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {graph-based semi-supervised learning,large-scale semi-supervised,learning,non-parametric models,transductive inference},
pages = {3311--3370},
title = {{Semi-supervised learning with measure propagation}},
url = {http://dl.acm.org/citation.cfm?id=2078212},
volume = {12},
year = {2011}
}
@inproceedings{Ben-David2007,
author = {Rakhlin, Alexander and Caponnetto, Andrea},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Rakhlin, Caponnetto - Unknown - Stability of k-means clustering.pdf:pdf},
title = {{Stability of k-means clustering}}
}
@article{Sugiyama2009,
author = {Sugiyama, Masashi and Id\'{e}, Tsuyoshi and Nakajima, Shinichi and Sese, Jun},
doi = {10.1007/s10994-009-5125-7},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sugiyama et al. - 2009 - Semi-supervised local Fisher discriminant analysis for dimensionality reduction.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
month = jul,
number = {1-2},
pages = {35--61},
title = {{Semi-supervised local Fisher discriminant analysis for dimensionality reduction}},
url = {http://www.springerlink.com/index/10.1007/s10994-009-5125-7},
volume = {78},
year = {2009}
}
@article{Neal2011,
abstract = {Hamiltonian dynamics can be used to produce distant proposals for the Metropolis algorithm, thereby avoiding the slow exploration of the state space that results from the diffusive behaviour of simple random-walk proposals. Though originating in physics, Hamiltonian dynamics can be applied to most problems with continuous state spaces by simply introducing fictitious "momentum" variables. A key to its usefulness is that Hamiltonian dynamics preserves volume, and its trajectories can thus be used to define complex mappings without the need to account for a hard to compute Jacobian factor - a property that can be exactly maintained even when the dynamics is approximated by discretizing time. In this review, discuss theoretical and practical aspects of Hamiltonian Monte Carlo, and present some of its variations, including using windows of states for approximations, tempering during the course of a trajectory to handle isolated modes, and short-cut methods that prevent useless trajectories form taking much computation time.},
archivePrefix = {arXiv},
arxivId = {arXiv:1206.1901v1},
author = {Neal, Radford M.},
doi = {10.1201/b10905},
eprint = {arXiv:1206.1901v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Neal - 2011 - Handbook of Markov Chain Monte Carlo.pdf:pdf},
isbn = {978-1-4200-7941-8},
issn = {<null>},
journal = {Handbook of Markov Chain Monte Carlo},
keywords = {hamiltonian dynamics,mcmc},
pages = {113--162},
title = {{Handbook of Markov Chain Monte Carlo}},
url = {http://www.crcnetbase.com/doi/book/10.1201/b10905},
volume = {20116022},
year = {2011}
}
@misc{Gelman2012,
author = {Gelman, Andrew},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gelman - 2012 - Little Data How traditional statistical ideas remain relevant in a big-data world.pdf:pdf},
title = {{Little Data : How traditional statistical ideas remain relevant in a big-data world}},
year = {2012}
}
@inproceedings{Zhou2005a,
author = {Zhou, Zhi-hua and Li, Ming},
booktitle = {International Joint Conferences on Artificial Intelligence},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhou, Li - 2005 - Semi-Supervised Regression with Co-Training.pdf:pdf},
title = {{Semi-Supervised Regression with Co-Training.}},
url = {http://ijcai.org/Past Proceedings/IJCAI-05/PDF/0689.pdf},
year = {2005}
}
@article{Peng2002,
author = {Peng, Yonghong and Flach, Peter A. and Soares, Carlos and Brazdil, Pavel B.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Peng et al. - 2002 - Improved dataset characterisation for meta-learning.pdf:pdf},
journal = {Lecture Notes in Computer Science},
pages = {141--152},
title = {{Improved dataset characterisation for meta-learning}},
url = {http://link.springer.com/chapter/10.1007/3-540-36182-0\_14},
volume = {2534},
year = {2002}
}
@article{Williams1998,
author = {Williams, Christopher K. I.},
doi = {10.1162/089976698300017412},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Williams - 1998 - Computation with Infinite Neural Networks.pdf:pdf},
issn = {0899-7667},
journal = {Neural Computation},
month = jul,
number = {5},
pages = {1203--1216},
title = {{Computation with Infinite Neural Networks}},
url = {http://www.mitpressjournals.org/doi/abs/10.1162/089976698300017412},
volume = {10},
year = {1998}
}
@article{Dougherty2001,
abstract = {In order to study the molecular biological differences between normal and diseased tissues, it is desirable to perform classification among diseases and stages of disease using microarray-based gene-expression values. Owing to the limited number of microarrays typically used in these studies, serious issues arise with respect to the design, performance and analysis of classifiers based on microarray data. This paper reviews some fundamental issues facing small-sample classification: classification rules, constrained classifiers, error estimation and feature selection. It discusses both unconstrained and constrained classifier design from sample data, and the contributions to classifier error from constrained optimization and lack of optimality owing to design from sample data. The difficulty with estimating classifier error when confined to small samples is addressed, particularly estimating the error from training data. The impact of small samples on the ability to include more than a few variables as classifier features is explained.},
author = {Dougherty, Edward R.},
doi = {10.1002/cfg.62},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Dougherty - 2001 - Small sample issues for microarray-based classification.pdf:pdf},
issn = {1531-6912},
journal = {Comparative and functional genomics},
month = jan,
number = {1},
pages = {28--34},
pmid = {18628896},
title = {{Small sample issues for microarray-based classification.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2447190\&tool=pmcentrez\&rendertype=abstract},
volume = {2},
year = {2001}
}
@inproceedings{Weinberger2009,
archivePrefix = {arXiv},
arxivId = {arXiv:0902.2206v5},
author = {Weinberger, Kilian and Dasgupta, Anirban and Attenberg, Josh and Langford, John and Smola, Alex},
booktitle = {International Conference on Machine Learning},
eprint = {arXiv:0902.2206v5},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Weinberger et al. - 2009 - Feature hashing for large scale multitask learning.pdf:pdf},
keywords = {classifier personalization,concentration inequalities,document classification,kernels,multitask learning},
number = {Icml},
pages = {1113--1120},
title = {{Feature hashing for large scale multitask learning}},
url = {http://dl.acm.org/citation.cfm?id=1553516},
year = {2009}
}
@article{Buhmann2010,
archivePrefix = {arXiv},
arxivId = {arXiv:1006.0375v1},
author = {Buhmann, Joachim M},
eprint = {arXiv:1006.0375v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Buhmann - 2010 - Information theoretic model validation for clustering.pdf:pdf},
number = {X},
title = {{Information theoretic model validation for clustering}},
volume = {2010},
year = {2010}
}
@article{Keogh2005,
author = {Keogh, Eamonn and Lin, Jessica},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Keogh, Lin - 2005 - Clustering of time-series subsequences is meaningless implications for previous and future research.pdf:pdf},
journal = {Knowledge and information systems},
number = {2},
pages = {154--177},
title = {{Clustering of time-series subsequences is meaningless: implications for previous and future research}},
url = {http://link.springer.com/article/10.1007/s10115-004-0172-7},
volume = {8},
year = {2005}
}
@article{Chiou2007,
author = {Chiou, Jeng-Min and Li, Pai-Ling},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Chiou, Li - 2007 - Functional clustering and identifying substructures of longitudinal data.pdf:pdf},
journal = {Journal of the Royal Statistical Society: Series B},
keywords = {analysis,classification,clustering,functional data,functional principal component,modes of variation,stochastic processes},
pages = {679--699},
title = {{Functional clustering and identifying substructures of longitudinal data}},
url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-9868.2007.00605.x/full},
volume = {69},
year = {2007}
}
@article{Suzuki2013,
author = {Suzuki, Taiji and Sugiyama, Masashi},
doi = {10.1214/13-AOS1095},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Suzuki, Sugiyama - 2013 - Fast learning rate of multiple kernel learning Trade-off between sparsity and smoothness.pdf:pdf},
journal = {The Annals of Statistics},
keywords = {additive model,and phrases,convergence rate,elastic-net,multiple kernel learning,reproducing kernel hilbert spaces,restricted isometry,smoothness,sparse learning},
number = {3},
pages = {1381--1405},
title = {{Fast learning rate of multiple kernel learning: Trade-off between sparsity and smoothness}},
url = {http://projecteuclid.org/euclid.aos/1375362553},
volume = {41},
year = {2013}
}
@article{Domingos2012,
author = {Domingos, Pedro},
doi = {10.1145/2347736.2347755},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Domingos - 2012 - A few useful things to know about machine learning.pdf:pdf},
issn = {00010782},
journal = {Communications of the ACM},
month = oct,
number = {10},
pages = {78},
title = {{A few useful things to know about machine learning}},
url = {http://dl.acm.org/citation.cfm?doid=2347736.2347755},
volume = {55},
year = {2012}
}
@article{Smith-Miles2008,
author = {Smith-Miles, Kate},
doi = {10.1145/1456650.1456656},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Smith-Miles - 2008 - Cross-disciplinary perspectives on meta-learning for algorithm selection.pdf:pdf},
issn = {03600300},
journal = {ACM Computing Surveys},
month = dec,
number = {1},
pages = {1--25},
title = {{Cross-disciplinary perspectives on meta-learning for algorithm selection}},
url = {http://portal.acm.org/citation.cfm?doid=1456650.1456656},
volume = {41},
year = {2008}
}
@phdthesis{Rifkin2002,
author = {Rifkin, Ryan},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Rifkin - 2002 - Everything Old Is New Again A Fresh Look at Historical Approaches in Machine Learning.pdf:pdf},
title = {{Everything Old Is New Again: A Fresh Look at Historical Approaches in Machine Learning}},
year = {2002}
}
@inproceedings{McDowell2012,
author = {McDowell, Luke and Aha, David W.},
booktitle = {Proceedings of the 29th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/McDowell, Aha - 2012 - Semi-supervised collective classification via hybrid label regularization.pdf:pdf},
pages = {975--982},
title = {{Semi-supervised collective classification via hybrid label regularization}},
url = {http://arxiv.org/abs/1206.6467},
year = {2012}
}
@article{Marchand2004,
author = {Marchand, E and Strawderman, WE},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Marchand, Strawderman - 2004 - Estimation in Restricted Parameter Spaces A Review.pdf:pdf},
journal = {Institute of Mathematical Statistics Lecture Notes-Monograph Series},
number = {2004},
pages = {21--44},
title = {{Estimation in Restricted Parameter Spaces: A Review}},
url = {http://www.jstor.org/stable/10.2307/4356296},
volume = {45},
year = {2004}
}
@article{Minka1998,
author = {Minka, Thomas P},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Minka - 1998 - Escpectation-Maximization as lower bound maximization.pdf:pdf},
number = {1977},
pages = {1--8},
title = {{Escpectation-Maximization as lower bound maximization}},
year = {1998}
}
@inproceedings{Loog2010,
author = {Loog, Marco},
booktitle = {Proceedings of the 2010 European Conference on Machine learning and Knowledge Discovery in Databases},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Loog - 2010 - Constrained Parameter Estimation for Semi-Supervised Learning The Case of the Nearest Mean Classifier.pdf:pdf},
pages = {291--304},
title = {{Constrained Parameter Estimation for Semi-Supervised Learning: The Case of the Nearest Mean Classifier}},
year = {2010}
}
@article{Ireland1968a,
author = {Ireland, C.T. and Kullback, S.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ireland, Kullback - 1968 - Contingence tables with given marginals.pdf:pdf},
journal = {Biometrika},
number = {1},
pages = {179--188},
title = {{Contingence tables with given marginals}},
volume = {55},
year = {1968}
}
@inproceedings{Duin2002,
author = {Pekalska, Ella and Duin, Robert P.W. and Skurichina, Marina},
booktitle = {Multiple Classifier Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Pekalska, Duin, Skurichina - 2002 - A discussion on the classifier projection space for classifier combining.pdf:pdf},
pages = {137--148},
title = {{A discussion on the classifier projection space for classifier combining}},
url = {http://www.springerlink.com/index/A98FBKT93AK0YNNE.pdf},
year = {2002}
}
@article{Chapelle2006a,
address = {New York, New York, USA},
author = {Chapelle, Olivier and Chi, Mingmin and Zien, Alexander},
doi = {10.1145/1143844.1143868},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Chapelle, Chi, Zien - 2006 - A continuation method for semi-supervised SVMs.pdf:pdf},
isbn = {1595933832},
journal = {Proceedings of the 23rd international conference on Machine learning - ICML '06},
pages = {185--192},
publisher = {ACM Press},
title = {{A continuation method for semi-supervised SVMs}},
url = {http://portal.acm.org/citation.cfm?doid=1143844.1143868},
year = {2006}
}
@article{Hand2013,
author = {Hand, D.J. and Anagnostopoulos, C.},
doi = {10.1016/j.patrec.2012.12.004},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hand, Anagnostopoulos - 2013 - When is the area under the receiver operating characteristic curve an appropriate measure of classifier p.pdf:pdf},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {area under the curve},
month = apr,
number = {5},
pages = {492--495},
title = {{When is the area under the receiver operating characteristic curve an appropriate measure of classifier performance?}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167865512003923},
volume = {34},
year = {2013}
}
@article{Kuncheva2003,
author = {Kuncheva, Ludmila I and Whitaker, Christopher J.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kuncheva, Whitaker - 2003 - Measures of Diversity in Classifier Ensembles.pdf:pdf},
journal = {Machine Learning},
keywords = {committee of learners,dependency and diversity,multiple classifiers ensemble,pattern recognition},
pages = {181--207},
title = {{Measures of Diversity in Classifier Ensembles}},
volume = {51},
year = {2003}
}
@article{Shen2013,
abstract = {As the molecular marker density grows, there is a strong need in both genome-wide association studies and genomic selection to fit models with a large number of parameters. Here we present a computationally efficient generalized ridge regression (RR) algorithm for situations in which the number of parameters largely exceeds the number of observations. The computationally demanding parts of the method depend mainly on the number of observations and not the number of parameters. The algorithm was implemented in the R package bigRR based on the previously developed package hglm. Using such an approach, a heteroscedastic effects model (HEM) was also developed, implemented, and tested. The efficiency for different data sizes were evaluated via simulation. The method was tested for a bacteria-hypersensitive trait in a publicly available Arabidopsis data set including 84 inbred lines and 216,130 SNPs. The computation of all the SNP effects required <10 sec using a single 2.7-GHz core. The advantage in run time makes permutation test feasible for such a whole-genome model, so that a genome-wide significance threshold can be obtained. HEM was found to be more robust than ordinary RR (a.k.a. SNP-best linear unbiased prediction) in terms of QTL mapping, because SNP-specific shrinkage was applied instead of a common shrinkage. The proposed algorithm was also assessed for genomic evaluation and was shown to give better predictions than ordinary RR.},
author = {Shen, Xia and Alam, Moudud and Fikse, Freddy and R\"{o}nneg\aa rd, Lars},
doi = {10.1534/genetics.112.146720},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Shen et al. - 2013 - A novel generalized ridge regression method for quantitative genetics.pdf:pdf},
issn = {1943-2631},
journal = {Genetics},
keywords = {Algorithms,Arabidopsis,Arabidopsis: genetics,Genetics, Population,Genetics, Population: methods,Genome, Plant,Genome-Wide Association Study,Genome-Wide Association Study: methods,Models, Genetic,Polymorphism, Single Nucleotide,Quantitative Trait Loci},
month = apr,
number = {4},
pages = {1255--68},
pmid = {23335338},
title = {{A novel generalized ridge regression method for quantitative genetics.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23335338},
volume = {193},
year = {2013}
}
@article{Nigam2000,
author = {Nigam, Kamal and McCallum, Andrew Kachites and Thrun, Sebastian and Mitchell, Tom},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Nigam et al. - 2000 - Text classification from labeled and unlabeled documents using EM.pdf:pdf},
journal = {Machine learning},
keywords = {bayesian learning,combining labeled and unlabeled,data,expectation-maximization,integrating supervised and unsuper-,text classification,vised learning},
pages = {1--34},
title = {{Text classification from labeled and unlabeled documents using EM}},
volume = {34},
year = {2000}
}
@article{Liang2007,
archivePrefix = {arXiv},
arxivId = {arXiv:0710.4618v1},
author = {Liang, Feng and Mukherjee, Sayan and West, Mike},
doi = {10.1214/088342307000000032},
eprint = {arXiv:0710.4618v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Liang, Mukherjee, West - 2007 - The Use of Unlabeled Data in Predictive Modeling.pdf:pdf},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {and phrases,bayesian analysis,bayesian kernel regression,latent factor models,mixture models,pervised learning,predictive distribution,semisu-,unlabeled data},
month = may,
number = {2},
pages = {189--205},
title = {{The Use of Unlabeled Data in Predictive Modeling}},
url = {http://projecteuclid.org/euclid.ss/1190905518},
volume = {22},
year = {2007}
}
@techreport{Krijthe2013,
author = {Krijthe, Jesse H and Loog, Marco},
booktitle = {Under review},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Krijthe, Loog - 2013 - Implicitly Constrained Semi-Supervised Least Squares Classification.pdf:pdf},
keywords = {constrained,least squares classification,semi-supervised learning},
title = {{Implicitly Constrained Semi-Supervised Least Squares Classification}},
url = {www.jessekrijthe.com/papers/krijthe2013.pdf},
year = {2013}
}
@article{Sejdinovic2013,
author = {Sejdinovic, Dino and Sriperumbudur, Bharath and Gretton, Arthur and Fukumizu, Kenji},
doi = {10.1214/13-AOS1140},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sejdinovic et al. - 2013 - Equivalence of distance-based and RKHS-based statistics in hypothesis testing.pdf:pdf},
journal = {The Annals of Statistics},
number = {5},
pages = {2263--2291},
title = {{Equivalence of distance-based and RKHS-based statistics in hypothesis testing}},
url = {http://projecteuclid.org/euclid.aos/1383661264},
volume = {41},
year = {2013}
}
@inproceedings{Huang2006,
author = {Huang, Jiayuan and Smola, Alex and Gretton, Arthur and Borgwardt, Karsten M. and Sch\"{o}lkopf, Bernhard},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Huang et al. - 2006 - Correcting sample selection bias by unlabeled data.pdf:pdf},
pages = {601--608},
title = {{Correcting sample selection bias by unlabeled data}},
url = {http://machinelearning.wustl.edu/mlpapers/paper\_files/NIPS2006\_915.pdf},
year = {2006}
}
@article{Huopaniemi2010,
abstract = {Analysis of variance (ANOVA)-type methods are the default tool for the analysis of data with multiple covariates. These tools have been generalized to the multivariate analysis of high-throughput biological datasets, where the main challenge is the problem of small sample size and high dimensionality. However, the existing multi-way analysis methods are not designed for the currently increasingly important experiments where data is obtained from multiple sources. Common examples of such settings include integrated analysis of metabolic and gene expression profiles, or metabolic profiles from several tissues in our case, in a controlled multi-way experimental setup where disease status, medical treatment, gender and time-series are usual covariates.},
author = {Huopaniemi, Ilkka and Suvitaival, Tommi and Nikkil\"{a}, Janne and Oresic, Matej and Kaski, Samuel},
doi = {10.1093/bioinformatics/btq174},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Huopaniemi et al. - 2010 - Multivariate multi-way analysis of multi-source data.pdf:pdf},
issn = {1367-4811},
journal = {Bioinformatics (Oxford, England)},
keywords = {Algorithms,Analysis of Variance,Data Collection,Gene Expression Profiling,Gene Expression Profiling: methods,Multivariate Analysis},
month = jun,
number = {12},
pages = {i391--8},
pmid = {20529933},
title = {{Multivariate multi-way analysis of multi-source data.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2881359\&tool=pmcentrez\&rendertype=abstract},
volume = {26},
year = {2010}
}
@unpublished{Loog2013,
author = {Loog, Marco},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Loog - 2013 - Conservative Transductive and Semi-Supervised Empirical Risk Minimization.pdf:pdf},
pages = {1--9},
title = {{Conservative Transductive and Semi-Supervised Empirical Risk Minimization}},
year = {2013}
}
@inproceedings{Ji2012,
author = {Ji, Ming and Yang, Tianbao and Lin, Binbin and Jin, Rong and Han, Jiawei},
booktitle = {Proceedings of the 29th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ji et al. - 2012 - A simple algorithm for semi-supervised learning with improved generalization error bound.pdf:pdf},
number = {2},
title = {{A simple algorithm for semi-supervised learning with improved generalization error bound}},
url = {http://arxiv.org/abs/1206.6412},
year = {2012}
}
@article{Iosifidis2014b,
author = {Iosifidis, Alexandros and Tefas, Anastastios and Pitas, Ioannis},
doi = {10.1016/j.patrec.2014.12.003},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Iosifidis, Tefas, Pitas - 2014 - On the Kernel Extreme Learning Machine Classifier.pdf:pdf},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Extreme Learning Machine,Infinite Networks,Single-hidden Layer Networks,extreme learning machine,single-hidden layer networks},
month = dec,
publisher = {Elsevier Ltd.},
title = {{On the Kernel Extreme Learning Machine Classifier}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167865514003705},
year = {2014}
}
@inproceedings{Ratsaby1995,
author = {Ratsaby, Joel and Venkatesht, Santosh S.},
booktitle = {Proceedings of the 8th Annual conference on Computational learning theory},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Ratsaby, Venkatesht - 1995 - Learning from a mixture of labeled and unlabeled examples with parametric side information.pdf:pdf},
pages = {412--417},
title = {{Learning from a mixture of labeled and unlabeled examples with parametric side information}},
url = {http://dl.acm.org/citation.cfm?id=225348},
year = {1995}
}
@article{Lv2013,
author = {Lv, Jinchi},
doi = {10.1214/13-AOS1149},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lv - 2013 - Impacts of high dimensionality in finite samples.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
month = aug,
number = {4},
pages = {2236--2262},
title = {{Impacts of high dimensionality in finite samples}},
url = {http://projecteuclid.org/euclid.aos/1382547520},
volume = {41},
year = {2013}
}
@article{Sabato2013,
author = {Sabato, Sivan and Srebro, Nathan and Tishby, Naftali},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Sabato, Srebro, Tishby - 2013 - Distribution-Dependent Sample Complexity of Large Margin Learning.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {distribution-dependence,linear classifiers,sample complexity,supervised learning},
pages = {2119--2149},
title = {{Distribution-Dependent Sample Complexity of Large Margin Learning}},
volume = {14},
year = {2013}
}
@article{Byrd1995,
author = {Byrd, Richard H. and Lu, Peihuang and Nocedal, Jorge and Zhu, Ciyou},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Byrd et al. - 1995 - A limited memory algorithm for bound constrained optimization.pdf:pdf},
journal = {SIAM Journal on Scientific Computing},
number = {5},
pages = {1190--1208},
title = {{A limited memory algorithm for bound constrained optimization}},
volume = {16},
year = {1995}
}
@inproceedings{Krijthe2014,
address = {Stockholm},
author = {Krijthe, Jesse H. and Loog, Marco},
booktitle = {International Conference on Pattern Recognition},
pages = {3762--3767},
title = {{Implicitly Constrained Semi-Supervised Linear Discriminant Analysis}},
year = {2014}
}
@article{Loog2012a,
author = {Loog, Marco},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Loog - 2012 - Semi-supervised linear discriminant analysis using moment constraints.pdf:pdf},
journal = {Partially Supervised Learning, LNCS},
pages = {32--41},
title = {{Semi-supervised linear discriminant analysis using moment constraints}},
url = {http://www.springerlink.com/index/A3T1U8542T092156.pdf},
volume = {7081},
year = {2012}
}
@article{Dempster1977,
author = {Dempster, AP and Laird, NM and Rubin, DB},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Dempster, Laird, Rubin - 1977 - Maximum likelihood from incomplete data via the EM algorithm.pdf:pdf},
isbn = {0000000779},
journal = {Journal of the Royal Statistical Society. Series B},
keywords = {incomplete,likelihood,maximum},
number = {1},
pages = {1--38},
title = {{Maximum likelihood from incomplete data via the EM algorithm}},
url = {http://www.jstor.org/stable/10.2307/2984875},
volume = {39},
year = {1977}
}
@article{Wang2004,
author = {Wang, Duolao and Murphy, Michael},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wang, Murphy - 2004 - Estimating optimal transformations for multiple regression using the ACE algorithm.pdf:pdf},
journal = {Journal of data science},
keywords = {ace,algorithm,alternating conditional expectation,non-,parametric regression,transformation},
pages = {329--346},
title = {{Estimating optimal transformations for multiple regression using the ACE algorithm}},
url = {http://www.jds-online.com/file\_download/56/JDS-156.pdf},
volume = {2},
year = {2004}
}
@article{Zhang2004a,
author = {Zhang, Tong},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhang - 2004 - Statistical Behavior and Consistency of Classification Methods Based on Convex Risk Minimization.pdf:pdf},
journal = {The Annals of Statistics},
number = {1},
pages = {56--134},
title = {{Statistical Behavior and Consistency of Classification Methods Based on Convex Risk Minimization}},
url = {http://www.jstor.org/stable/10.2307/3448494},
volume = {32},
year = {2004}
}
@article{Gelman2011a,
author = {Gelman, Andrew},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gelman - 2011 - Ethics and statistics Open data and open methods.pdf:pdf},
journal = {Chance},
pages = {51--53},
title = {{Ethics and statistics: Open data and open methods}},
url = {http://scholar.google.com/scholar?hl=en\&btnG=Search\&q=intitle:Ethics+and+Statistics+Open+Data+and+Open+Methods\#3},
year = {2011}
}
@article{Wickenberg-Bolin2006,
abstract = {Supervised learning for classification of cancer employs a set of design examples to learn how to discriminate between tumors. In practice it is crucial to confirm that the classifier is robust with good generalization performance to new examples, or at least that it performs better than random guessing. A suggested alternative is to obtain a confidence interval of the error rate using repeated design and test sets selected from available examples. However, it is known that even in the ideal situation of repeated designs and tests with completely novel samples in each cycle, a small test set size leads to a large bias in the estimate of the true variance between design sets. Therefore different methods for small sample performance estimation such as a recently proposed procedure called Repeated Random Sampling (RSS) is also expected to result in heavily biased estimates, which in turn translates into biased confidence intervals. Here we explore such biases and develop a refined algorithm called Repeated Independent Design and Test (RIDT).},
author = {Wickenberg-Bolin, Ulrika and G\"{o}ransson, Hanna and Frykn\"{a}s, M\aa rten and Gustafsson, Mats G and Isaksson, Anders},
doi = {10.1186/1471-2105-7-127},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wickenberg-Bolin et al. - 2006 - Improved variance estimation of classification performance via reduction of bias caused by small sample.pdf:pdf},
issn = {1471-2105},
journal = {BMC bioinformatics},
keywords = {Analysis of Variance,Artificial Intelligence,Bias (Epidemiology),Diagnosis, Computer-Assisted,Diagnosis, Computer-Assisted: methods,Gene Expression Profiling,Gene Expression Profiling: methods,Humans,Models, Biological,Models, Statistical,Neoplasm Proteins,Neoplasm Proteins: analysis,Neoplasms,Neoplasms: diagnosis,Neoplasms: metabolism,Oligonucleotide Array Sequence Analysis,Oligonucleotide Array Sequence Analysis: methods,Pattern Recognition, Automated,Pattern Recognition, Automated: methods,Reproducibility of Results,Sample Size,Sensitivity and Specificity,Tumor Markers, Biological,Tumor Markers, Biological: analysis},
month = jan,
pages = {127},
pmid = {16533392},
title = {{Improved variance estimation of classification performance via reduction of bias caused by small sample size.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=1435937\&tool=pmcentrez\&rendertype=abstract},
volume = {7},
year = {2006}
}
@article{Dietterich1997,
author = {Dietterich, Thomas G},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Dietterich - 1997 - Machine-Learning Research.pdf:pdf},
number = {4},
pages = {97--136},
title = {{Machine-Learning Research}},
volume = {18},
year = {1997}
}
@inproceedings{Jaakkola2002,
author = {Jaakkola, MST and Szummer, Martin},
booktitle = {Advances in Neural Information Processing Systems 14},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Jaakkola, Szummer - 2002 - Partially labeled classification with Markov random walks.pdf:pdf},
pages = {945--952},
title = {{Partially labeled classification with Markov random walks}},
url = {http://books.google.com/books?hl=en\&lr=\&id=GbC8cqxGR7YC\&oi=fnd\&pg=PA945\&dq=Partially+labeled+classification+with+Markov+random+walks\&ots=ZvP5J\_YBx6\&sig=dk27TWzUdp9G-e9OyvfYcGR14ro},
year = {2002}
}
@misc{Tibshirani,
author = {Tibshirani, Robert},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Tibshirani - Unknown - Machine Learning vs. Statistics.pdf:pdf},
title = {{Machine Learning vs. Statistics}}
}
@article{Cule2011,
abstract = {BACKGROUND: Technological developments have increased the feasibility of large scale genetic association studies. Densely typed genetic markers are obtained using SNP arrays, next-generation sequencing technologies and imputation. However, SNPs typed using these methods can be highly correlated due to linkage disequilibrium among them, and standard multiple regression techniques fail with these data sets due to their high dimensionality and correlation structure. There has been increasing interest in using penalised regression in the analysis of high dimensional data. Ridge regression is one such penalised regression technique which does not perform variable selection, instead estimating a regression coefficient for each predictor variable. It is therefore desirable to obtain an estimate of the significance of each ridge regression coefficient.$\backslash$n$\backslash$nRESULTS: We develop and evaluate a test of significance for ridge regression coefficients. Using simulation studies, we demonstrate that the performance of the test is comparable to that of a permutation test, with the advantage of a much-reduced computational cost. We introduce the p-value trace, a plot of the negative logarithm of the p-values of ridge regression coefficients with increasing shrinkage parameter, which enables the visualisation of the change in p-value of the regression coefficients with increasing penalisation. We apply the proposed method to a lung cancer case-control data set from EPIC, the European Prospective Investigation into Cancer and Nutrition.$\backslash$n$\backslash$nCONCLUSIONS: The proposed test is a useful alternative to a permutation test for the estimation of the significance of ridge regression coefficients, at a much-reduced computational cost. The p-value trace is an informative graphical tool for evaluating the results of a test of significance of ridge regression coefficients as the shrinkage parameter increases, and the proposed test makes its production computationally feasible.},
author = {Cule, Erika and Vineis, Paolo and {De Iorio}, Maria},
doi = {10.1186/1471-2105-12-372},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cule, Vineis, De Iorio - 2011 - Significance testing in ridge regression for genetic data.pdf:pdf},
issn = {1471-2105},
journal = {BMC Bioinformatics},
number = {1},
pages = {372},
pmid = {21929786},
publisher = {BioMed Central Ltd},
title = {{Significance testing in ridge regression for genetic data}},
url = {http://www.biomedcentral.com/1471-2105/12/372},
volume = {12},
year = {2011}
}
@article{Kall2007,
abstract = {Shotgun proteomics uses liquid chromatography-tandem mass spectrometry to identify proteins in complex biological samples. We describe an algorithm, called Percolator, for improving the rate of confident peptide identifications from a collection of tandem mass spectra. Percolator uses semi-supervised machine learning to discriminate between correct and decoy spectrum identifications, correctly assigning peptides to 17\% more spectra from a tryptic Saccharomyces cerevisiae dataset, and up to 77\% more spectra from non-tryptic digests, relative to a fully supervised approach.},
author = {K\"{a}ll, Lukas and Canterbury, Jesse D and Weston, Jason and Noble, William Stafford and MacCoss, Michael J},
doi = {10.1038/nmeth1113},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/K\"{a}ll et al. - 2007 - Semi-supervised learning for peptide identification from shotgun proteomics datasets.pdf:pdf},
isbn = {1548-7091},
issn = {1548-7091},
journal = {Nature methods},
number = {11},
pages = {923--925},
pmid = {17952086},
title = {{Semi-supervised learning for peptide identification from shotgun proteomics datasets.}},
volume = {4},
year = {2007}
}
@article{Germain2013,
archivePrefix = {arXiv},
arxivId = {arXiv:1503.06944v1},
author = {Germain, Pascal},
eprint = {arXiv:1503.06944v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Germain - 2013 - PAC-Bayesian Theorems for Domain Adaptation with Specialization to Linear Classifiers.pdf:pdf},
title = {{PAC-Bayesian Theorems for Domain Adaptation with Specialization to Linear Classifiers}},
year = {2013}
}
@inproceedings{Cai2007,
author = {Cai, Deng and He, Xiaofei and Han, Jiawei},
booktitle = {IEEE 11th International Conference on Computer Vision},
doi = {10.1109/ICCV.2007.4408856},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cai, He, Han - 2007 - Semi-supervised Discriminant Analysis.pdf:pdf},
isbn = {978-1-4244-1630-1},
pages = {1--7},
title = {{Semi-supervised Discriminant Analysis}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4408856},
year = {2007}
}
@article{Molinaro2005,
abstract = {In genomic studies, thousands of features are collected on relatively few samples. One of the goals of these studies is to build classifiers to predict the outcome of future observations. There are three inherent steps to this process: feature selection, model selection and prediction assessment. With a focus on prediction assessment, we compare several methods for estimating the 'true' prediction error of a prediction model in the presence of feature selection.},
author = {Molinaro, Annette M and Simon, Richard and Pfeiffer, Ruth M},
doi = {10.1093/bioinformatics/bti499},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Molinaro, Simon, Pfeiffer - 2005 - Prediction error estimation a comparison of resampling methods.pdf:pdf},
issn = {1367-4803},
journal = {Bioinformatics (Oxford, England)},
keywords = {Algorithms,Computer Simulation,Data Interpretation, Statistical,Gene Expression Profiling,Gene Expression Profiling: methods,Models, Genetic,Models, Statistical,Oligonucleotide Array Sequence Analysis,Oligonucleotide Array Sequence Analysis: methods,Reproducibility of Results,Sample Size,Sensitivity and Specificity,Software},
month = aug,
number = {15},
pages = {3301--7},
pmid = {15905277},
title = {{Prediction error estimation: a comparison of resampling methods.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15905277},
volume = {21},
year = {2005}
}
@article{Lattimore2011,
archivePrefix = {arXiv},
arxivId = {1111.3846},
author = {Lattimore, Tor and Hutter, Marcus},
eprint = {1111.3846},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lattimore, Hutter - 2011 - No Free Lunch versus Occam's Razor in Supervised Learning.pdf:pdf},
journal = {arXiv preprint},
keywords = {kolmogorov complexity,no free lunch,occam,s razor,supervised learning},
title = {{No Free Lunch versus Occam's Razor in Supervised Learning}},
url = {http://arxiv.org/abs/1111.3846},
year = {2011}
}
@article{Mann2007,
author = {Mann, Gideon S. and McCallum, Andrew Kachites},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Mann, McCallum - 2007 - Efficient computation of entropy gradient for semi-supervised conditional random fields.pdf:pdf},
journal = {Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics},
title = {{Efficient computation of entropy gradient for semi-supervised conditional random fields}},
url = {http://dl.acm.org/citation.cfm?id=1614136},
year = {2007}
}
@inproceedings{Cotter2013,
author = {Cotter, Andrew and Shalev-Shwartz, Shai and Srebro, Nathan},
booktitle = {Proceedings of the 30th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Cotter, Shalev-Shwartz, Srebro - 2013 - Learning optimally sparse support vector machines.pdf:pdf},
pages = {266--274},
title = {{Learning optimally sparse support vector machines}},
url = {http://machinelearning.wustl.edu/mlpapers/papers/ICML2013\_cotter13},
year = {2013}
}
@inproceedings{Weiss2008,
author = {Weiss, Yair and Torralba, Antonio and Fergus, Rob},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Weiss, Torralba, Fergus - 2008 - Spectral Hashing.pdf:pdf},
pages = {1753--1760},
title = {{Spectral Hashing.}},
url = {https://papers.nips.cc/paper/3383-spectral-hashing.pdf},
year = {2008}
}
@inproceedings{Roli2002,
author = {Roli, Fabio and Raudys, \v{S}arūnas and Marcialis, Gian Luca},
booktitle = {Multiple Classifier Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Roli, Raudys, Marcialis - 2002 - An experimental comparison of fixed and trained fusion rules for crisp classifier outputs.pdf:pdf},
title = {{An experimental comparison of fixed and trained fusion rules for crisp classifier outputs}},
url = {http://link.springer.com/chapter/10.1007/3-540-45428-4\_23},
year = {2002}
}
@article{Tolstikhin2014,
archivePrefix = {arXiv},
arxivId = {1411.7200},
author = {Tolstikhin, Ilya and Blanchard, Gilles and Kloft, Marius},
eprint = {1411.7200},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Tolstikhin, Blanchard, Kloft - 2014 - Localized Complexities for Transductive Learning.pdf:pdf},
keywords = {centration inequalities,con-,empirical processes,fast rates,kernel classes,localized complexities,statistical learning,transductive learning},
month = nov,
pages = {1--28},
title = {{Localized Complexities for Transductive Learning}},
url = {http://arxiv.org/abs/1411.7200v1},
volume = {35},
year = {2014}
}
@article{Kullback1968,
author = {Kullback, S.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kullback - 1968 - Probability Densities with Given Marginals.pdf:pdf},
journal = {The Annals of Mathematical Statistics},
number = {4},
pages = {1236--1243},
title = {{Probability Densities with Given Marginals}},
url = {http://www.jstor.org/stable/10.2307/2239692},
volume = {39},
year = {1968}
}
@article{Dy2004a,
author = {Dy, Jennifer G. and Brodley, Carla E.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Dy, Brodley - 2004 - Feature selection for unsupervised learning.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {clustering,expectation-maximization,feature selection,unsupervised learning},
pages = {845--889},
title = {{Feature selection for unsupervised learning}},
url = {http://dl.acm.org/citation.cfm?id=1016787},
volume = {5},
year = {2004}
}
@inproceedings{White2012,
author = {White, Martha and Schuurmans, Dale},
booktitle = {International Conference on Artificial Intelligence and Statistics},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/White, Schuurmans - 2012 - Generalized optimal reverse prediction.pdf:pdf},
pages = {1305--1313},
title = {{Generalized optimal reverse prediction}},
url = {http://machinelearning.wustl.edu/mlpapers/paper\_files/AISTATS2012\_WhiteS12.pdf},
year = {2012}
}
@inproceedings{Chapelle2002,
author = {Chapelle, Olivier and Weston, Jason and Sch\"{o}lkopf, Bernhard},
booktitle = {Advances in Neural Information Processing Systems 14},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Chapelle, Weston, Sch\"{o}lkopf - 2002 - Cluster kernels for semi-supervised learning.pdf:pdf},
pages = {585--592},
title = {{Cluster kernels for semi-supervised learning}},
url = {http://machinelearning.wustl.edu/mlpapers/paper\_files/AA13.pdf},
year = {2002}
}
@article{Reid2011,
author = {Reid, Mark D. and Williamson, Robert C.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Reid, Williamson - 2011 - Information, divergence and risk for binary experiments.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {classification,divergence,loss functions,regret bounds,statistical information},
pages = {731--817},
title = {{Information, divergence and risk for binary experiments}},
url = {http://dl.acm.org/citation.cfm?id=2021029},
volume = {12},
year = {2011}
}
@article{Jain1999a,
author = {Jain, A.K. and Murty, M.N. and Flynn, P.J.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Jain, Murty, Flynn - 1999 - Data clustering a review.pdf:pdf},
journal = {ACM computing surveys (CSUR)},
number = {3},
title = {{Data clustering: a review}},
url = {http://dl.acm.org/citation.cfm?id=331504},
volume = {31},
year = {1999}
}
@article{Bottou2012,
author = {Bottou, L\'{e}on},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Bottou - 2012 - Stochastic Gradient Descent Tricks.pdf:pdf},
journal = {Neural Networks: Tricks of the Trade},
number = {1},
pages = {1--16},
title = {{Stochastic Gradient Descent Tricks}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-35289-8\_25},
year = {2012}
}
@misc{Klein2004,
author = {Klein, Dan},
booktitle = {University of California at Berkeley, Computer Science \ldots},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Klein - 2004 - Lagrange Multipliers without Permanent Scarring.pdf:pdf},
title = {{Lagrange Multipliers without Permanent Scarring}},
url = {http://www.ee.columbia.edu/~vittorio/LagrangeMultipliers-Klein.pdf},
year = {2004}
}
@inproceedings{Shindler2011,
author = {Shindler, Michael and Wong, Alex and Meyerson, Adam},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Shindler, Wong, Meyerson - 2011 - Fast and accurate k-means for large datasets.pdf:pdf},
pages = {2375--2383},
title = {{Fast and accurate k-means for large datasets}},
url = {http://web.engr.oregonstate.edu/~shindler/papers/FastKMeans\_nips11.pdf http://machinelearning.wustl.edu/mlpapers/paper\_files/NIPS2011\_1271.pdf},
year = {2011}
}
@article{Piironen2012,
archivePrefix = {arXiv},
arxivId = {arXiv:1503.08650v1},
author = {Piironen, Juho and Vehtari, Aki},
eprint = {arXiv:1503.08650v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Piironen, Vehtari - 2012 - Comparison of Bayesian predictive methods for model selection.pdf:pdf},
keywords = {bayesian model selection,cross-validation,map,median,overfitting,probability model,projection,reference model,selection bias,waic},
title = {{Comparison of Bayesian predictive methods for model selection}},
year = {2012}
}
@article{Reid2009,
abstract = {We present tight surrogate regret bounds for the
class of proper (i.e., Fisher consistent) losses.
The bounds generalise the margin-based bounds
due to Bartlett et al. (2006). The proof uses Taylor’s
theorem and leads to new representations
for loss and regret and a simple proof of the integral
representation of proper losses. We also
present a different formulation of a duality result
of Bregman divergences which leads to a simple
demonstration of the convexity of composite
losses using canonical link functions},
author = {Reid, Mark and Williamson, Bob},
doi = {10.1145/1553374.1553489},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Reid, Williamson - 2009 - Surrogate Regret Bounds for Proper Losses.pdf:pdf},
isbn = {9781605585161},
keywords = {Learning/Statistics \& Optimisation},
number = {Theorem 3},
pages = {897--904},
title = {{Surrogate Regret Bounds for Proper Losses}},
url = {http://eprints.pascal-network.org/archive/00008977/},
year = {2009}
}
@unpublished{Maeda2014,
abstract = {Dropout is one of the key techniques to prevent the learning from overfitting. It is explained that dropout works as a kind of modified L2 regularization. Here, we shed light on the dropout from Bayesian standpoint. Bayesian interpretation enables us to optimize the dropout rate, which is beneficial for learning of weight parameters and prediction after learning. The experiment result also encourages the optimization of the dropout.},
archivePrefix = {arXiv},
arxivId = {1412.7003},
author = {Maeda, Shin-ichi},
eprint = {1412.7003},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Maeda - 2014 - A Bayesian encourages dropout.pdf:pdf},
month = dec,
pages = {1--9},
title = {{A Bayesian encourages dropout}},
url = {http://arxiv.org/abs/1412.7003},
year = {2014}
}
@article{Shaffer1991,
author = {Shaffer, Juliet Popper},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Shaffer - 1991 - The Gauss-Markov Theorem and Random Regressors.pdf:pdf},
journal = {The American Statistician},
keywords = {best linear unbiased estimators,finite-,linear regression,population sampling,unbiased esti-},
number = {4},
pages = {269--273},
title = {{The Gauss-Markov Theorem and Random Regressors}},
volume = {45},
year = {1991}
}
@article{Niyogi2013,
author = {Niyogi, Partha},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Niyogi - 2013 - Manifold Regularization and Semi-supervised Learning Some Theoretical Analyses.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {graph laplacian,manifold regularization,minimax rates,semi-supervised learning},
pages = {1229--1250},
title = {{Manifold Regularization and Semi-supervised Learning : Some Theoretical Analyses}},
volume = {14},
year = {2013}
}
@article{Chandrasekaran2013a,
archivePrefix = {arXiv},
arxivId = {arXiv:1211.1073v2},
author = {Chandrasekaran, Venkat and Jordan, Michael I.},
eprint = {arXiv:1211.1073v2},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Chandrasekaran, Jordan - 2013 - Computational and statistical tradeoffs via convex relaxation(2).pdf:pdf},
journal = {Proceedings of the National Academy of Sciences},
keywords = {convex geometry,convex relaxation,high-dimensional statistics,massive datasets},
number = {13},
pages = {1181--1190},
title = {{Computational and statistical tradeoffs via convex relaxation}},
url = {http://www.pnas.org/content/110/13/E1181.short},
volume = {110},
year = {2013}
}
@article{Lindner1999,
author = {Lindner, Guido and Studer, Rudi},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lindner, Studer - 1999 - AST Support for algorithm selection with a CBR approach.pdf:pdf},
journal = {Principles of Data Mining and Knowledge Discovery},
title = {{AST: Support for algorithm selection with a CBR approach}},
url = {http://www.springerlink.com/index/QBDF3R2GKVW57LUF.pdf http://link.springer.com/chapter/10.1007/978-3-540-48247-5\_52},
year = {1999}
}
@article{Brodley1995,
author = {Brodley, Carla E.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Brodley - 1995 - Recursive automatic bias selection for classifier construction.pdf:pdf},
journal = {Machine Learning},
keywords = {automatic algorithm selection,decision trees,hybrid classifiers,inductive bias,learning from},
pages = {63--94},
title = {{Recursive automatic bias selection for classifier construction}},
url = {http://link.springer.com/article/10.1023/A:1022686102325},
volume = {94},
year = {1995}
}
@article{Chakraborty2015,
archivePrefix = {arXiv},
arxivId = {1502.03491v1},
author = {Chakraborty, Mithun and Das, Sanmay and Lavoie, Allen},
eprint = {1502.03491v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Chakraborty, Das, Lavoie - 2015 - How to show a probablistic model is better.pdf:pdf},
pages = {1--5},
title = {{How to show a probablistic model is better}},
year = {2015}
}
@article{Goldenberg2009,
author = {Luxburg, Ulrike Von},
doi = {10.1561/2200000008},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Luxburg - 2009 - Clustering Stability An overview.pdf:pdf},
issn = {1935-8237},
journal = {Foundations and Trends® in Machine Learning},
number = {3},
pages = {235--274},
title = {{Clustering Stability: An overview}},
url = {http://www.nowpublishers.com/product.aspx?product=MAL\&doi=2200000008},
volume = {2},
year = {2009}
}
@book{Aubin2000,
author = {Aubin, Jean-Pierre},
publisher = {John Wiley \& Sons},
title = {{Applied functional analysis}},
volume = {47},
year = {2000}
}
@unpublished{Grunwald2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1412.3730v1},
author = {Gr\"{u}nwald, Peter and van Ommen, Thijs},
eprint = {arXiv:1412.3730v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gr\"{u}nwald, Ommen - 2014 - Inconsistency of Bayesian Inference for Misspecified Linear Models , and a Proposal for Repairing It.pdf:pdf},
pages = {1--70},
title = {{Inconsistency of Bayesian Inference for Misspecified Linear Models , and a Proposal for Repairing It}},
year = {2014}
}
@article{Wang2012,
abstract = {MOTIVATION: Epistasis or gene-gene interaction has gained increasing attention in studies of complex diseases. Its presence as an ubiquitous component of genetic architecture of common human diseases has been contemplated. However, the detection of gene-gene interaction is difficult due to combinatorial explosion. RESULTS: We present a novel feature selection method incorporating variable interaction. Three gene expression datasets are analyzed to illustrate our method, although it can also be applied to other types of high-dimensional data. The quality of variables selected is evaluated in two ways: first by classification error rates, then by functional relevance assessed using biological knowledge. We show that the classification error rates can be significantly reduced by considering interactions. Secondly, a sizable portion of genes identified by our method for breast cancer metastasis overlaps with those reported in gene-to-system breast cancer (G2SBC) database as disease associated and some of them have interesting biological implication. In summary, interaction-based methods may lead to substantial gain in biological insights as well as more accurate prediction.},
author = {Wang, Haitian and Lo, Shaw-Hwa and Zheng, Tian and Hu, Inchi},
doi = {10.1093/bioinformatics/bts531},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wang et al. - 2012 - Interaction-based feature selection and classification for high-dimensional biological data.pdf:pdf},
issn = {1367-4811},
journal = {Bioinformatics (Oxford, England)},
month = nov,
number = {21},
pages = {2834--42},
pmid = {22945786},
title = {{Interaction-based feature selection and classification for high-dimensional biological data.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22945786},
volume = {28},
year = {2012}
}
@article{Gomez-Chova2008,
abstract = {This letter presents a semisupervised method based on kernel machines and graph theory for remote sensing image classification. The support vector machine (SVM) is regularized with the unnormalized graph Laplacian, thus leading to the Laplacian SVM (LapSVM). The method is tested in the challenging problems of urban monitoring and cloud screening, in which an adequate exploitation of the wealth of unlabeled samples is critical. Results obtained using different sensors, and with low number of training samples, demonstrate the potential of the proposed LapSVM for remote sensing image classification.},
author = {G\'{o}mez-Chova, Luis and Camps-Valls, Gustavo and Mu\~{n}oz-Mari, Jordi and Calpe, Javier},
doi = {10.1109/LGRS.2008.916070},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/G\'{o}mez-Chova et al. - 2008 - Semisupervised image classification with Laplacian support vector machines.pdf:pdf},
isbn = {1545-598X},
issn = {1545598X},
journal = {IEEE Geoscience and Remote Sensing Letters},
keywords = {Kernel methods,Manifold learning,Regularization,Semisupervised learning (SSL),Support vector machines (SVMs)},
number = {3},
pages = {336--340},
title = {{Semisupervised image classification with Laplacian support vector machines}},
volume = {5},
year = {2008}
}
@inproceedings{Joachims1999,
author = {Joachims, Thorsten},
booktitle = {Proceedings of the 16th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Joachims - 1999 - Transductive inference for text classification using support vector machines.pdf:pdf},
pages = {200--209},
publisher = {Morgan Kaufmann Publishers},
title = {{Transductive inference for text classification using support vector machines}},
year = {1999}
}
@article{Chan1997,
author = {Chan, Philip K. and Stolfo, Salvatore J.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Chan, Stolfo - 1997 - On the accuracy of meta-learning for scalable data mining.pdf:pdf},
journal = {Journal of Intelligent Information Systems},
title = {{On the accuracy of meta-learning for scalable data mining}},
url = {http://www.springerlink.com/index/M27133K052552242.pdf},
year = {1997}
}
@article{Wainwright2008,
author = {Wainwright, Martin J. and Jordan, Michael I.},
doi = {10.1561/2200000001},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wainwright, Jordan - 2008 - Graphical models, exponential families, and variational inference.pdf:pdf},
journal = {Foundations and Trends in Machine Learning},
pages = {1--305},
title = {{Graphical models, exponential families, and variational inference}},
url = {http://dl.acm.org/citation.cfm?id=1498841},
volume = {1},
year = {2008}
}
@article{Wang2009a,
author = {Wang, Fei and Wang, Xin and Li, Tao},
doi = {10.1109/CVPR.2009.5206675},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wang, Wang, Li - 2009 - Beyond the graphs Semi-parametric semi-supervised discriminant analysis.pdf:pdf},
isbn = {978-1-4244-3992-8},
journal = {IEEE Conference on Computer Vision and Pattern Recognition},
month = jun,
pages = {2113--2120},
publisher = {Ieee},
title = {{Beyond the graphs: Semi-parametric semi-supervised discriminant analysis}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5206675},
year = {2009}
}
@article{Gelman2012a,
author = {Gelman, Andrew and Hill, Jennifer and Yajima, Masanao},
doi = {10.1080/19345747.2011.618213},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Gelman, Hill, Yajima - 2012 - Why We (Usually) Don't Have to Worry About Multiple Comparisons.pdf:pdf},
issn = {1934-5747},
journal = {Journal of Research on Educational Effectiveness},
keywords = {bayesian inference,hierarchical modeling,multiple comparisons,statis-,type s error},
month = apr,
number = {2},
pages = {189--211},
title = {{Why We (Usually) Don't Have to Worry About Multiple Comparisons}},
url = {http://www.tandfonline.com/doi/abs/10.1080/19345747.2011.618213},
volume = {5},
year = {2012}
}
@article{Martella2011,
abstract = {In healthy aging research, typically multiple health outcomes are measured, representing health status. The aim of this paper was to develop a model-based clustering approach to identify homogeneous sibling pairs according to their health status. Model-based clustering approaches will be considered on the basis of linear mixed effect model for the mixture components. Class memberships of siblings within pairs are allowed to be correlated, and within a class the correlation between siblings is modeled using random sibling pair effects. We propose an expectation-maximization algorithm for maximum likelihood estimation. Model performance is evaluated via simulations in terms of estimating the correct parameters, degree of agreement, and the ability to detect the correct number of clusters. The performance of our model is compared with the performance of standard model-based clustering approaches. The methods are used to classify sibling pairs from the Leiden Longevity Study according to their health status. Our results suggest that homogeneous healthy sibling pairs are associated with a longer life span. Software is available for fitting the new models.},
author = {Martella, F. and Vermunt, J.K. and Beekman, M. and Westendorp, R.G.J. and Slagboom, P.E. and Houwing-Duistermaat, J.J.},
doi = {10.1002/sim.4365},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Martella et al. - 2011 - A mixture model with random-effects components for classifying sibling pairs.pdf:pdf},
issn = {1097-0258},
journal = {Statistics in medicine},
keywords = {80 and over,Aged,Aging,Aging: physiology,Cluster Analysis,Computer Simulation,Female,Health,Humans,Longevity,Longevity: physiology,Male,Models,Siblings,Statistical},
month = nov,
number = {27},
pages = {3252--64},
pmid = {21905068},
title = {{A mixture model with random-effects components for classifying sibling pairs.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21905068},
volume = {30},
year = {2011}
}
@article{Poon2011,
abstract = {The key limiting factor in graphical model inference and learning is the complexity of the partition function. We thus ask the question: what are the most general conditions under which the partition function is tractable? The answer leads to a new kind of deep architecture, which we call sum-product networks (SPNs) and will present in this abstract.},
archivePrefix = {arXiv},
arxivId = {1202.3732},
author = {Poon, Hoifung and Domingos, Pedro},
doi = {10.1109/ICCVW.2011.6130310},
eprint = {1202.3732},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Poon, Domingos - 2011 - Sum-product networks A new deep architecture.pdf:pdf},
isbn = {9781467300629},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {689--690},
title = {{Sum-product networks: A new deep architecture}},
year = {2011}
}
@article{Brazdil2003a,
author = {Brazdil, Pavel B. and Soares, Carlos and Costa, JP Da},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Brazdil, Soares, Costa - 2003 - Ranking learning algorithms Using IBL and meta-learning on accuracy and time results.pdf:pdf},
journal = {Machine Learning},
keywords = {algorithm recommendation,data characterization,meta-learning,ranking},
pages = {251--277},
title = {{Ranking learning algorithms: Using IBL and meta-learning on accuracy and time results}},
url = {http://link.springer.com/article/10.1023/A:1021713901879},
volume = {50},
year = {2003}
}
@inproceedings{Yu2013,
author = {Yu, Felix X. and Liu, Dong and Kumar, Sanjiv and Jebara, Tony and Chang, Shih-Fu},
booktitle = {International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Yu et al. - 2013 - \$\$backslash\$ propto \$ SVM for Learning with Label Proportions.pdf:pdf},
pages = {504--512},
title = {{\$\$$\backslash$backslash\$ propto \$ SVM for Learning with Label Proportions}},
year = {2013}
}
@article{Zhang2014a,
author = {Zhang, C. and Liu, Y.},
doi = {10.1093/biomet/asu017},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhang, Liu - 2014 - Multicategory angle-based large-margin classification.pdf:pdf},
issn = {0006-3444},
journal = {Biometrika},
month = jul,
number = {3},
pages = {625--640},
title = {{Multicategory angle-based large-margin classification}},
url = {http://biomet.oxfordjournals.org/cgi/doi/10.1093/biomet/asu017},
volume = {101},
year = {2014}
}
@article{Pearson1926,
author = {Pearson, Karl},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Pearson - 1926 - Researches on the Mode of Distribution of the Constants of Samples Taken at Random from a Bivaraite Normal Population.pdf:pdf},
journal = {Proceedings of the Royal Society of London. Series A},
number = {760},
pages = {1--14},
title = {{Researches on the Mode of Distribution of the Constants of Samples Taken at Random from a Bivaraite Normal Population}},
volume = {112},
year = {1926}
}
@article{Tibshirani2001,
author = {Tibshirani, Robert and Walther, Guenther and Hastie, Trevor},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Tibshirani, Walther, Hastie - 2001 - Estimating the number of clusters in a data set via the gap statistic.pdf:pdf},
journal = {Journal of the Royal Statistical Society. Series B, Statistical methodology},
pages = {411--423},
title = {{Estimating the number of clusters in a data set via the gap statistic}},
url = {http://onlinelibrary.wiley.com/doi/10.1111/1467-9868.00293/abstract},
volume = {63},
year = {2001}
}
@article{Nowak2008,
abstract = {When applying hierarchical clustering algorithms to cluster patient samples from microarray data, the clustering patterns generated by most algorithms tend to be dominated by groups of highly differentially expressed genes that have closely related expression patterns. Sometimes, these genes may not be relevant to the biological process under study or their functions may already be known. The problem is that these genes can potentially drown out the effects of other genes that are relevant or have novel functions. We propose a procedure called complementary hierarchical clustering that is designed to uncover the structures arising from these novel genes that are not as highly expressed. Simulation studies show that the procedure is effective when applied to a variety of examples. We also define a concept called relative gene importance that can be used to identify the influential genes in a given clustering. Finally, we analyze a microarray data set from 295 breast cancer patients, using clustering with the correlation-based distance measure. The complementary clustering reveals a grouping of the patients which is uncorrelated with a number of known prognostic signatures and significantly differing distant metastasis-free probabilities.},
author = {Nowak, Gen and Tibshirani, Robert},
doi = {10.1093/biostatistics/kxm046},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Nowak, Tibshirani - 2008 - Complementary hierarchical clustering.pdf:pdf},
issn = {1468-4357},
journal = {Biostatistics (Oxford, England)},
keywords = {Algorithms,Breast Neoplasms,Breast Neoplasms: genetics,Cluster Analysis,Computer Simulation,Female,Fuzzy Logic,Gene Expression,Gene Expression Profiling,Gene Expression Profiling: methods,Gene Expression Profiling: statistics \& numerical,Genetic Markers,Humans,Information Storage and Retrieval,Information Storage and Retrieval: methods,Neoplasm Metastasis,Neoplasm Metastasis: genetics,Oligonucleotide Array Sequence Analysis,Oligonucleotide Array Sequence Analysis: methods,Pattern Recognition, Automated,Principal Component Analysis,Reference Values},
month = jul,
number = {3},
pages = {467--83},
pmid = {18093965},
title = {{Complementary hierarchical clustering.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3294318\&tool=pmcentrez\&rendertype=abstract},
volume = {9},
year = {2008}
}
@article{Byrne1999a,
author = {Byrne, Charles},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Byrne - 1999 - Iterative projection onto convex sets using multipe Bregman distances.pdf:pdf},
journal = {Inverse Problems},
title = {{Iterative projection onto convex sets using multipe Bregman distances}},
year = {1999}
}
@inproceedings{Zhou2007,
author = {Zhou, Zhi-hua and Zhan, De-Chuan and Yang, Qiang},
booktitle = {Proceedings of the 22nd national conference on Artificial intelligence},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhou, Zhan, Yang - 2007 - Semi-supervised learning with very few labeled training examples.pdf:pdf},
title = {{Semi-supervised learning with very few labeled training examples}},
url = {http://www.aaai.org/Papers/AAAI/2007/AAAI07-107.pdf},
year = {2007}
}
@article{Chapelle2007,
abstract = {Most literature on support vector machines (SVMs) concentrates on the dual optimization problem. In this letter, we point out that the primal problem can also be solved efficiently for both linear and nonlinear SVMs and that there is no reason for ignoring this possibility. On the contrary, from the primal point of view, new families of algorithms for large-scale SVM training can be investigated.},
author = {Chapelle, Olivier},
doi = {10.1162/neco.2007.19.5.1155},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Chapelle - 2007 - Training a support vector machine in the primal.pdf:pdf},
issn = {0899-7667},
journal = {Neural computation},
keywords = {Algorithms,Models, Theoretical,Neural Networks (Computer),Nonlinear Dynamics,Pattern Recognition, Automated},
month = may,
number = {5},
pages = {1155--78},
pmid = {17381263},
title = {{Training a support vector machine in the primal.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17381263},
volume = {19},
year = {2007}
}
@article{Loog,
author = {Loog, Marco},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Loog - Unknown - Semi-Supervised Linear Discriminant Analysis through Moment-Constraint Parameter Estimation(2).pdf:pdf},
keywords = {affine invariant,classification,constraints,linear discriminant analysis,moment,semi-supervised learning},
title = {{Semi-Supervised Linear Discriminant Analysis through Moment-Constraint Parameter Estimation}}
}
@article{Kappen2013,
author = {Kappen, Hilbert J. and G\'{o}mez, Vicen\c{c}},
doi = {10.1007/s10994-013-5427-7},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Kappen, G\'{o}mez - 2013 - The Variational Garrote.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {mean-field,sparse regression,spike-and-slab,variational approximation},
month = dec,
number = {January 2012},
title = {{The Variational Garrote}},
url = {http://link.springer.com/10.1007/s10994-013-5427-7},
year = {2013}
}
@inproceedings{Wolpert2002,
author = {Wolpert, David H},
booktitle = {Soft Computing and Industry},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Wolpert - 2002 - The Supervised Learning No-Free-Lunch Theorems.pdf:pdf},
pages = {25--42},
title = {{The Supervised Learning No-Free-Lunch Theorems}},
url = {http://link.springer.com/chapter/10.1007/978-1-4471-0123-9\_3},
year = {2002}
}
@article{Tran2014,
archivePrefix = {arXiv},
arxivId = {1411.7596},
author = {Tran, Dustin},
eprint = {1411.7596},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Tran - 2014 - Convex Techniques for Model Selection.pdf:pdf},
month = nov,
pages = {1--9},
title = {{Convex Techniques for Model Selection}},
url = {http://arxiv.org/abs/1411.7596v1},
year = {2014}
}
@article{Lei2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1401.6978v1},
author = {Lei, Jing and Vu, Vincent Q and Jan, S T},
doi = {10.1214/14-AOS1273},
eprint = {arXiv:1401.6978v1},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Lei, Vu, Jan - 2014 - Sparsistency and Agnostic Inference in Sparse PCA.pdf:pdf},
issn = {0090-5364},
number = {1},
pages = {1--23},
title = {{Sparsistency and Agnostic Inference in Sparse PCA}},
volume = {43},
year = {2014}
}
@inproceedings{Zhu2003,
author = {Zhu, Xiaojin and Ghahramani, Zoubin and Lafferty, John},
booktitle = {Proceedings of the 20th International Conference on Machine Learning},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhu, Ghahramani, Lafferty - 2003 - Semi-supervised learning using gaussian fields and harmonic functions.pdf:pdf},
pages = {912--919},
title = {{Semi-supervised learning using gaussian fields and harmonic functions}},
year = {2003}
}
@article{Zhang2006,
abstract = {With the development of DNA microarray technology, scientists can now measure the expression levels of thousands of genes simultaneously in one single experiment. One current difficulty in interpreting microarray data comes from their innate nature of 'high-dimensional low sample size'. Therefore, robust and accurate gene selection methods are required to identify differentially expressed group of genes across different samples, e.g. between cancerous and normal cells. Successful gene selection will help to classify different cancer types, lead to a better understanding of genetic signatures in cancers and improve treatment strategies. Although gene selection and cancer classification are two closely related problems, most existing approaches handle them separately by selecting genes prior to classification. We provide a unified procedure for simultaneous gene selection and cancer classification, achieving high accuracy in both aspects.},
author = {Zhang, Hao Helen and Ahn, Jeongyoun and Lin, Xiaodong and Park, Cheolwoo},
doi = {10.1093/bioinformatics/bti736},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Zhang et al. - 2006 - Gene selection using support vector machines with non-convex penalty.pdf:pdf},
issn = {1367-4803},
journal = {Bioinformatics (Oxford, England)},
keywords = {Algorithms,Bayes Theorem,Cluster Analysis,Databases, Genetic,Humans,Models, Genetic,Models, Statistical,Neoplasms,Neoplasms: genetics,Neoplasms: metabolism,Oligonucleotide Array Sequence Analysis,Oligonucleotide Array Sequence Analysis: methods,Pattern Recognition, Automated,Sequence Analysis, DNA,Software,Time Factors},
month = jan,
number = {1},
pages = {88--95},
pmid = {16249260},
title = {{Gene selection using support vector machines with non-convex penalty.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16249260},
volume = {22},
year = {2006}
}
@article{Huang1998,
author = {Huang, Jianhua Z.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Huang - 1998 - Projection estimation in multiple regression with application to functional ANOVA models.pdf:pdf},
journal = {The Annals of Statistics},
keywords = {and phrases,anova,curse of dimensionality,finite elements,interaction,least},
number = {1},
pages = {242--272},
title = {{Projection estimation in multiple regression with application to functional ANOVA models}},
url = {http://projecteuclid.org/euclid.aos/1030563984},
volume = {26},
year = {1998}
}
@article{Hartley1968b,
author = {Hartley, H.O. and Rao, J.N.K.},
file = {:Users/jkrijthe/Documents/Mendeley Desktop/Hartley, Rao - 1968 - Classification and Estimation in Analysis of Variance Problems.pdf:pdf},
journal = {Revue de l'Institut International de Statistique},
number = {2},
pages = {141--147},
title = {{Classification and Estimation in Analysis of Variance Problems}},
url = {http://www.jstor.org/stable/10.2307/1401602},
volume = {36},
year = {1968}
}
