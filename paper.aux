\relax 
\citation{Cozman2006}
\@writefile{toc}{\contentsline {title}{Implicitly Constrained Semi-Supervised Least Squares Classification}{1}}
\@writefile{toc}{\authcount {2}}
\@writefile{toc}{\contentsline {author}{Jesse H. Krijthe \and Marco Loog}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\citation{Widrow1960}
\citation{Seeger2001,Singh2008}
\citation{Sokolovska2008}
\citation{Hastie2001,Rifkin2003,Tibshirani1996,Poggio2003}
\citation{Rifkin2003}
\citation{Bottou2010}
\citation{Chapelle2006,Zhu2009}
\citation{Nigam2000}
\citation{Cozman2003,Cozman2006}
\citation{Goldberg2009,Wang2007a}
\citation{Goldberg2009}
\citation{Goldberg2009}
\citation{McLachlan1975}
\citation{Yarowsky1995,Abney2004}
\citation{Nigam2000,Yarowsky1995}
\citation{Cozman2003,Cozman2006}
\citation{Abney2004}
\citation{Zhu2009}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{3}}
\newlabel{section:relatedwork}{{2}{3}}
\citation{Grandvalet2005}
\citation{Joachims1999}
\citation{Bennett1998,Sindhwani2006}
\citation{Collobert2006}
\citation{Sindhwani2006,Wang2007}
\citation{Singh2008}
\citation{Singh2008}
\citation{Wang2007a}
\citation{Li2011}
\citation{Loog2014b,Loog2014a}
\citation{Loog2014b,Loog2014a}
\citation{Poggio, Hastie, Suykens}
\citation{Shaffer1991}
\citation{Fan2008}
\@writefile{toc}{\contentsline {section}{\numberline {3}Implicitly Constrained Least Squares Classification}{5}}
\citation{Hastie2001,Rifkin2003}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A visual representation of implicitly constrained semi-supervised learning. $\mathcal  {F}_{\beta }$ is the space of all linear models. $\mathaccentV {hat}05E{\beta }_{sup}$ denotes the solution given only a small amount of labeled data. $\mathcal  {C}_{\beta }$ is the subset of the space which contains all the solutions we get when applying all possible labelings to the unlabeled data. $\mathaccentV {hat}05E{\beta }_{semi}$ is a projection of $\mathaccentV {hat}05E{\beta }_{sup}$ onto $\mathcal  {C}_{\beta }$. $\mathaccentV {hat}05E{\beta }_{oracle}$ is the supervised solution if we would have the labels for all the objects.}}{6}}
\newlabel{fig:constrainedsubset}{{1}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Multivariate Least Squares Classification}{7}}
\newlabel{section:leastsquares}{{3.1}{7}}
\newlabel{squaredloss}{{1}{7}}
\newlabel{olssolution}{{2}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Implicitly Constrained Least Squares Classification}{7}}
\newlabel{section:icls}{{3.2}{7}}
\newlabel{constrainedregion}{{3}{7}}
\citation{Byrd1995}
\newlabel{icls}{{5}{8}}
\citation{Sokolovska2008}
\@writefile{toc}{\contentsline {section}{\numberline {4}Theoretical Results}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Strong performance result in 1D}{9}}
\newlabel{eq:trueloss}{{8}{9}}
\newlabel{eq:bayesoptimal}{{9}{9}}
\newlabel{eqn:sslsolution}{{12}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces An example where implicitly constrained optimization improves performance. The supervised solution $\mathaccentV {hat}05E{\beta }_{sup}$ which minimizes the supervised loss (shown), is not part of the interval of allowed solutions. The solution that minimizes this supervised loss within the allowed interval is $\mathaccentV {hat}05E{\beta }_{semi}$. This solution is closer to the optimal solution ${\beta }^{\ast }$ than the supervised solution $\mathaccentV {hat}05E{\beta }_{sup}$.}}{10}}
\newlabel{fig:constrainedproblem}{{2}{10}}
\newlabel{supervisedsolution}{{13}{10}}
\newlabel{eq:condition}{{14}{11}}
\newlabel{eq:condition2}{{15}{11}}
\newlabel{eq:condition3}{{16}{11}}
\citation{Berger1985}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Euclidean Projection Estimator for Multivariate ICLS}{12}}
\newlabel{section:proofmultivariate}{{4.2}{12}}
\newlabel{eq:adaptedICLS}{{17}{12}}
\citation{Aubin2000}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Semi-Supervised Projection Estimator for Multivariate ICLS}{13}}
\newlabel{eq:extendedICLS}{{20}{13}}
\newlabel{th:robustness}{{3}{13}}
\citation{Shaffer1991}
\citation{Bache2013}
\citation{Chapelle2006}
\citation{Chapelle2006}
\newlabel{eq:projectiontheorem}{{22}{14}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Empirical Results}{14}}
\newlabel{section:empiricalresults}{{5}{14}}
\citation{Bache2013}
\citation{Bache2013}
\citation{Bache2013}
\citation{Bache2013}
\citation{Bache2013}
\citation{Bache2013}
\citation{Bache2013}
\citation{Bache2013}
\citation{Bache2013}
\citation{Bache2013}
\citation{Bache2013}
\citation{Bache2013}
\citation{Chapelle2006}
\citation{Chapelle2006}
\citation{Chapelle2006}
\citation{Chapelle2006}
\citation{Chapelle2006}
\citation{Chapelle2006}
\citation{Raudys1998,Opper1996}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Description of the datasets used in the experiments. Features indicates the dimensionality of the design matrix after categorical features are expanded into dummy variables.}}{15}}
\newlabel{table:datasets}{{1}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Comparison of Learning Curves}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Mean classification error for $L=\qopname  \relax m{max}(d+5,20)$ and $100$ repeats. The error bounds are $+/-$ the standard error of the mean.}}{16}}
\newlabel{fig:learningcurves1}{{3}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Mean classification error for $L=\qopname  \relax m{max}(d+5,20)$ and $100$ repeats. The error bounds are $+/-$ the standard error of the mean.}}{17}}
\newlabel{fig:learningcurves2}{{4}{17}}
\citation{Chapelle2006}
\citation{Chapelle2006}
\citation{Chapelle2006}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Benchmark performance}{18}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Average 10-fold cross-validation error and standard deviation over 20 repeats. The classifiers that have been compared are supervised Least Squares (LS), Implicitly constrained least squares (ICLS), the extended ICLS presented in section 4.3 (ICLS$_{ext}$), the adapted ICLS procedure from section 4.2 (ICLS$_{adp}$), self-learned least squares (SLLS), updated covariance least squares (UCLS, see text) and the supervised least squares classifier that has access to all the labels (LS$_{oracle}$). Indicated in $\mathbf  {bold}$ is whether a semi-supervised classifier significantly outperform the supervised LS classifier, as measured using a $t$-test with a $0.05$ significance level. \relax $\@@underline {\hbox {Underlined}}\mathsurround \z@ $\relax  indicates whether a semi-supervised classifier is (significantly) best among the three semi-supervised classifiers considered.}}{19}}
\newlabel{table:cvresults}{{2}{19}}
\citation{Cozman2006}
\citation{Seeger2001,Singh2008}
\@writefile{toc}{\contentsline {section}{\numberline {6}Discussion}{20}}
\citation{Shaffer1991}
\citation{Shaffer1991}
\citation{Krijthe2014}
\citation{Joachims1999,Collobert2006}
\citation{Mann2010}
\citation{Culp2008}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{22}}
\bibstyle{splncs}
\bibdata{library}
