\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\citation{Widrow1960}
\citation{Seeger2001}
\citation{Sokolovska2008}
\citation{Hastie2001,Rifkin2003,Tibshirani1996,Poggio2003}
\citation{Rifkin2003}
\citation{Bottou2010}
\citation{Cozman2006}
\citation{Chapelle2006,Zhu2009}
\citation{Nigam2000}
\citation{Cozman2003,Cozman2006}
\citation{Goldberg2009,Wang2007a}
\citation{Goldberg2009}
\citation{Goldberg2009}
\citation{McLachlan1975b}
\citation{Yarowsky1995,Abney2004}
\citation{Nigam2000,Yarowsky1995}
\citation{Cozman2003,Cozman2006}
\citation{Abney2004}
\citation{Zhu2009}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{3}}
\newlabel{section:relatedwork}{{2}{3}}
\citation{Grandvalet2005}
\citation{Joachims1999}
\citation{Bennett1998,Sindhwani2006}
\citation{Collobert2006}
\citation{Sindhwani2006,Wang2007}
\citation{Wang2007a}
\citation{Li2011}
\citation{Loog2010,Loog2013a}
\citation{Loog2010,Loog2013a}
\citation{Shaffer1991}
\citation{Fan2008}
\@writefile{toc}{\contentsline {section}{\numberline {3}Implicitly Constrained Least Squares Classification}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A visual representation of implicitly constrained semi-supervised learning. $\mathcal  {F}_{\beta }$ is the space of all linear models. $\mathaccentV {hat}05E{\beta }_{sup}$ denotes the solution given only a small amount of labeled data. $\mathcal  {C}_{\beta }$ is the subset of the space which contains all the solutions we get when applying all possible labelings to the unlabeled data. $\mathaccentV {hat}05E{\beta }_{semi}$ is a projection of $\mathaccentV {hat}05E{\beta }_{sup}$ onto $\mathcal  {C}_{\beta }$. $\mathaccentV {hat}05E{\beta }_{N_l+N_u}$ is the supervised solution if we would have the labels for all the objects.}}{5}}
\newlabel{fig:constrainedsubset}{{1}{5}}
\citation{Hastie2001,Rifkin2003}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Multivariate Least Squares Classification}{6}}
\newlabel{squaredloss}{{1}{6}}
\newlabel{olssolution}{{2}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Implicitly Constrained Least Squares Classification}{7}}
\newlabel{section:icls}{{3.2}{7}}
\newlabel{constrainedregion}{{3}{7}}
\newlabel{icls}{{5}{7}}
\citation{Byrd1995}
\citation{Sokolovska2008}
\@writefile{toc}{\contentsline {section}{\numberline {4}Theoretical Results}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Strong performance result in 1D}{8}}
\newlabel{eq:trueloss}{{8}{8}}
\newlabel{eq:bayesoptimal}{{9}{8}}
\newlabel{eqn:sslsolution}{{12}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces An example where implicitly constrained optimization always improves performance. The supervised solution $\mathaccentV {hat}05E{\beta }_{sup}$ which minimizes the supervised loss (shown), is not part of the interval of allowed solutions. The solution that minimizes this supervised loss within the allowed interval is $\mathaccentV {hat}05E{\beta }_{semi}$. This solution is closer to the Bayes optimal solution ${\beta }^{bayes}$ than the supervised solution $\mathaccentV {hat}05E{\beta }_{sup}$.}}{9}}
\newlabel{fig:constrainedproblem}{{2}{9}}
\newlabel{supervisedsolution}{{13}{9}}
\newlabel{eq:condition}{{14}{10}}
\newlabel{eq:condition2}{{15}{10}}
\newlabel{eq:condition3}{{16}{10}}
\citation{Berger1985}
\citation{Aubin2000}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Euclidean Projection Estimator for Multivariate ICLS}{11}}
\newlabel{eq:adaptedICLS}{{17}{11}}
\citation{Shaffer1991}
\citation{Bache2013}
\citation{Chapelle2006}
\citation{Chapelle2006}
\citation{Bache2013}
\citation{Bache2013}
\citation{Bache2013}
\citation{Bache2013}
\citation{Bache2013}
\citation{Bache2013}
\citation{Bache2013}
\citation{Bache2013}
\citation{Bache2013}
\citation{Bache2013}
\citation{Chapelle2006}
\citation{Chapelle2006}
\citation{Chapelle2006}
\citation{Chapelle2006}
\citation{Chapelle2006}
\citation{Chapelle2006}
\@writefile{toc}{\contentsline {section}{\numberline {5}Empirical Results}{12}}
\newlabel{section:empiricalresults}{{5}{12}}
\citation{Raudys1998,Opper1996}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Description of the datasets used in the experiments}}{13}}
\newlabel{table:datasets}{{1}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Comparison of Learning Curves}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Mean classification error for $N_l=\qopname  \relax m{max}(m+5,20)$ and $100$ repeats. The error bounds are $+/-$ the standard error of the mean.}}{14}}
\newlabel{fig:learningcurves1}{{3}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Benchmark performance}{14}}
\citation{Chapelle2006}
\citation{Chapelle2006}
\citation{Chapelle2006}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Mean classification error for $N_l=\qopname  \relax m{max}(m+5,20)$ and $100$ repeats. The error bounds are $+/-$ the standard error of the mean.}}{15}}
\newlabel{fig:learningcurves2}{{4}{15}}
\citation{Cozman2006}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Average 10-fold cross-validation error and standard deviation over 10 repeats. The classifiers that have been compared are supervised Least Squares (LS), Implicitly constrained least squares (ICLS), self-learned least squares (SLLS), updated covariance least squares (UCLS, see text) and for comparison a supervised least squares classifier that has access to all the labels (LSoracle). Indicated in $\mathbf  {bold}$ is whether a semi-supervised classifier significantly outperform the supervised LS classifier, as measured using a $t$-test with a $0.05$ significance level. \relax $\@@underline {\hbox {Underlined}}\mathsurround \z@ $\relax  indicates whether a semi-supervised classifier is (significantly) best among the three semi-supervised classifiers considered.}}{16}}
\newlabel{table:cvresults}{{2}{16}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Discussion}{16}}
\citation{Seeger2001}
\citation{Shaffer1991}
\citation{Shaffer1991}
\citation{Culp2008}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{18}}
\bibstyle{splncs}
\bibcite{Widrow1960}{1}
\bibcite{Seeger2001}{2}
\bibcite{Sokolovska2008}{3}
\bibcite{Hastie2001}{4}
\bibcite{Rifkin2003}{5}
\bibcite{Tibshirani1996}{6}
\bibcite{Poggio2003}{7}
\bibcite{Bottou2010}{8}
\bibcite{Cozman2006}{9}
\bibcite{Chapelle2006}{10}
\bibcite{Zhu2009}{11}
\bibcite{Nigam2000}{12}
\bibcite{Cozman2003}{13}
\bibcite{Goldberg2009}{14}
\bibcite{Wang2007a}{15}
\bibcite{McLachlan1975b}{16}
\bibcite{Yarowsky1995}{17}
\bibcite{Abney2004}{18}
\bibcite{Grandvalet2005}{19}
\bibcite{Joachims1999}{20}
\bibcite{Bennett1998}{21}
\bibcite{Sindhwani2006}{22}
\bibcite{Collobert2006}{23}
\bibcite{Wang2007}{24}
\bibcite{Li2011}{25}
\bibcite{Loog2010}{26}
\bibcite{Loog2013a}{27}
\bibcite{Shaffer1991}{28}
\bibcite{Fan2008}{29}
\bibcite{Byrd1995}{30}
\bibcite{Berger1985}{31}
\bibcite{Aubin2000}{32}
\bibcite{Bache2013}{33}
\bibcite{Raudys1998}{34}
\bibcite{Opper1996}{35}
\bibcite{Culp2008}{36}
