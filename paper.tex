\documentclass[smallcondensed]{svjour3} 

\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{mathtools}

% \newcommand{\Xe}{\begin{bmatrix} \mathbf{X}  \\ \mathbf{X}_u \end{bmatrix}}
% \newcommand{\XeT}{\begin{bmatrix} \mathbf{X} \\ \mathbf{X}_u \end{bmatrix}^T}

\newcommand{\Xe}{\mathbf{X}_e  }
\newcommand{\XeT}{\mathbf{X}_e^T}

\newcommand{\ye}{\begin{bmatrix} \mathbf{y}  \\ \mathbf{y}_u \end{bmatrix}}
\newcommand{\G}{\left(\Xe^T \Xe \right)^{-1}}
\newcommand{\missidentity}{\begin{bmatrix} 0 & 0 \\ 0 & \textbf{I }\end{bmatrix}}
\newcommand{\Greg}{\left(\Xe^T \Xe + \lambda \missidentity \right)^{-1}}
\newcommand{\Cb}{\mathcal{C}_{\beta}}


\begin{document}

\title{Implicitly Constrained Semi-Supervised Least Squares Classification}
\subtitle{}
\author{Jesse H. Krijthe \and Marco Loog}

\institute{Leiden University Medical Center\\
\and
Pattern Recognition Laboratory, Delft University of Technology
}

\date{Received: date / Accepted: date}

\maketitle


\begin{abstract}
We introduce a novel semi-supervised version of the least squares classifier. In implicitly constrained least squares (ICLS), we minimize the squared loss on the labeled data among the set of parameters implied by all possible labelings of the unlabeled data. Unlike previous discriminative semi-supervised methods, our approach does not introduce explicit additional assumptions into the objective function, but leverages implicit assumptions already present in the choice of the supervised least squares classifier.  We show this classifier can be formulated as a quadratic programming problem and its solution can be found using a simple gradient descent procedure. In a specific 1-dimensional case without intercept, we prove that this method can never lead to worse performance than supervised least squares classification, while in the multidimensional case we prove improvement of the parameter estimates for an adapted version of the proposed procedure. Experimental results corroborate the theoretical results and indicate desirable properties over alternative semi-supervised least squares approaches. 

\keywords{Semi-supervised learning, Least Squares Classification, Constrained Learning}
\end{abstract}

\section{Introduction}
We consider the problem of semi-supervised learning of binary classification functions. Like in the supervised paradigm, the goal in semi-supervised learning is to construct a classification rule that maps objects in some input space to a target outcome, such that future objects map to correct target outcomes as closely as possible. In the supervised paradigm this mapping is learned using a set of $N_l$ training objects and their corresponding outputs. In the semi-supervised scenario we are given an additional and often large set of $N_u$ unlabeled objects. The challenge of semi-supervised learning is to incorporate this additional information to improve the classification rule.

In this work, we present a novel approach to semi-supervised learning for the least squares classifier. We will refer to this approach as implicitly constrained semi-supervised learning (ICLS). The goal is to leverage the implicit assumptions present in supervised least squares classification to construct a semi-supervised version. That is, we exploit constraints inherent in the choice of supervised classifier, whereas current state-of-the-art typically relies on imposing additional extraneous, and possibly incorrect, assumptions.

In least squares classification, classes are encoded as numerical outputs after which a linear regression model is applied (see also, Section 3). By placing a threshold on the output of this model, we can use the linear function to predict class labels. In a different neural network formulation, this classifier is also known as Adaline \cite{Widrow1960}. There are several reasons why this is a particularly interesting classifier to study. First of all, this is a discriminative classifier. Some have claimed semi-supervised learning without additional assumptions is impossible for discriminative classifiers \cite{Seeger2001}. Our results, like \cite{Sokolovska2008}, show this may not strictly hold. Secondly, as we will show in section \ref{section:icls}, it allows us to formulate our approach as a quadratic programming problem, that can be solved through a simple gradient descent with boundary constraints. Lastly, least squares classification is a useful and adaptable classification technique  allowing for straightforward use of, for instance, regularization, sparsity penalties or kernelization \cite{Hastie2001,Rifkin2003,Tibshirani1996,Poggio2003}. Using these formulations, it has been shown to be competitive with state-of-the-art methods based on loss functions other than the squared loss \cite{Rifkin2003} as well as computationally efficient on large datasets \cite{Bottou2010}.

Ultimately, the goal of our research is to build a semi-supervised least squares classier that has the property that, at least in expectation, its performance is not worse than supervised least squares classification. While it may seem like an obvious requirement for any semi-supervised method, current semi-supervised methods do not have this property. In fact, performance can significantly degrade as more unlabeled data is added, as has been shown in \cite{Cozman2006}. Also, many semi-supervised learning procedures are formulated as non-convex objective functions which are hard to optimize. A more satisfactory state of affairs would therefore be computationally efficient methods that on average do not lead to worse classification performance than their supervised alternatives.

The proposed semi-supervised least squares classifier works by constraining the solution of the supervised least squares classifier based on the unlabeled data. These constraints follow from the choice of the supervised classifier and require only minimal assumptions. A limited though important result, which follows from this formulation, is that in the 1 dimensional case without intercept, this procedure \emph{never} leads to a decrease in generalization performance. In our experiments, we indeed find that this new approach sports many of the properties we deem desirable in a semi-supervised learner, namely that on average performance increases as we increase the amount of unlabeled data and that we efficiently converge to a global optimum.

The main contributions of this paper are

\begin{itemize}
  \item A novel convex formulation for robust semi-supervised learning under squared loss (Equation \ref{icls})
  \item An intuition through a proof of non-degradation for the 1-dimensional case (Theorem 1) and improvement in parameter estimation for an adapted procedure in the multivariate case (Theorem 2).
  \item An empirical evaluation of the properties proposed by these theories (Section 4)
\end{itemize}

The rest of this paper is organized as follows. Section 2 discusses related work on semi-supervised learning. Section 3 introduces our semi-supervised version of the least squares classifier. We then derive a quadratic programming formulation and present a simple way to solve this problem through bounded gradient descent. Section 4 contains a proof that this classifier works under some assumptions about the data. Section 5 presents an empirical evaluation of the proposed approach on benchmark datasets. The final sections discuss the results and conclude.

\section{Related Work} \label{section:relatedwork}
Many diverse semi-supervised learning techniques have been proposed \cite{Chapelle2006,Zhu2009}. While these have proven successful in  particular applications, such as document classification \cite{Nigam2000}, it has also been observed that these techniques may give worse performance than their supervised counterparts \cite{Cozman2003,Cozman2006}. In these cases, disregarding the unlabeled data would lead to better performance. Some \cite{Goldberg2009,Wang2007a} have argued that \emph{agnostic} semi-supervised learning, which \cite{Goldberg2009} define as semi-supervised learning that is at least no worse than supervised learning, can be achieved by cross-validation on the limited labeled data. Agnostic semi-supervised learning follows if we only use semi-supervised methods when their estimated cross-validation error is significantly lower than those of the supervised alternatives. As the results of  \cite{Goldberg2009} indicate, this criterion may be too conservative: given the small amount of labeled data, a semi-supervised method will only be preferred if the difference in performance is very large. If the difference is less distinct, the supervised learner will always be preferred and we potentially ignore useful information from the unlabeled objects. Moreover, this cross-validation approach can be computationally demanding.
 % computational complexity of many of the semi-supervised learning algorithms.

A simple approach to semi-supervised learning is offered by the self-learning procedure \cite{McLachlan1975b}, also known as Yarowsky's algorithm \cite{Yarowsky1995,Abney2004}. Taking any classifier, we first estimate its parameters on only  the labeled data. Using this trained classifier we label the unlabeled points and add the most confident label predictions to the training set. The classifier parameters are re-estimated using these labeled objects to get a new classifier. One iteratively applies this procedure until the predicted labels of the unlabeled data no longer change.

One of the advantages of this procedure is that it can be applied to any supervised classifier. It has also shown practical success in some application domains, particularly document classification \cite{Nigam2000,Yarowsky1995}. Unfortunately, the process of self-training can also lead to severely decreased performance, compared to the supervised solution \cite{Cozman2003,Cozman2006}. One can imagine that once an object is incorrectly labeled and added to the training set, its incorrect label may be reinforced, leading the solution away from the optimum. Self-learning is closely related to the expectation maximization (EM) based approaches \cite{Abney2004}. Indeed, expectation maximization suffers from the same issues as self-learning \cite{Zhu2009}.

More recent work has focused on introducing useful assumptions about the unlabeled data that can help link information about the distribution of the features $P(X)$ to the posterior of the classes $P(Y|X)$. Commonly used assumptions are the smoothness assumption, objects that are close in the feature space likely share the same label; the cluster assumption, objects in the same cluster share a label; and the low density assumption enforcing that the decision boundary should be in a region of low density. 

%  In the case of logistic regression, entropy regularization can be applied by adding an additional term to the objective function that encodes the entropy of the classifier in the unlabeled points. Unfortunately, the new objective is hard to minimize. Some work has been done to do this more efficiently for particular models \cite{Mann2007}.

The low-density assumption is used in entropy regularization \cite{Grandvalet2005} as well as for support vector classification in the transductive support vector machine (TSVM)  \cite{Joachims1999}. When used in the semi-supervised setting this is closely related to the formulation of S$^3$VM \cite{Bennett1998,Sindhwani2006}. Like in entropy regularization, for the semi-supervised versions of SVM, an additional term is added to the objective function to push the decision boundary away from dense regions. The resulting objective function is non-convex, owing to the possible labelings that can be assigned to the unlabeled objects. Several approaches have been put forth to solve this difficult optimization problem, such as a convex concave procedure \cite{Collobert2006} and difference convex programming \cite{Sindhwani2006,Wang2007}.

In the approaches presented above, a parameter controls the importance of the unlabeled points. When the parameter is correctly set, it is clear, as \cite{Wang2007a} claims, that TSVM is always better than supervised SVM. It is, however, non-trivial to choose this parameter, given that semi-supervised learning is most interesting in cases where we have limited labeled objects, making a choice using cross-validation very unstable. In practice, therefore, TSVM may also lead to worse performance than the supervised support vector machine. \cite{Li2011} tries to guard against this deterioration by proposing safe semi-supervised SVM (S4VM). In some way, this method tries to find the safest decision boundary among all low-density decision boundaries identified by S3VM. While the approach is successful in protecting against deterioration when compared to supervised SVM, the price to pay is smaller performance increases on many datasets as well as significantly higher computational cost.

\cite{Loog2010,Loog2013a} attempt to guard against the possibility of deterioration in performance by not introducing additional assumptions, but instead leveraging implicit assumptions already present in supervised classifiers. These assumptions link parameters estimates that depend on labeled data to parameter estimates that rely on all data. By exploiting these links, semi-supervised versions of the nearest mean classifier and the linear discriminant are derived. Because these links are unique to each classifier, the approach does not generalize directly to other classifiers. The method presented here is similar in spirit, but unlike \cite{Loog2010,Loog2013a}, no explicit equations have to be formulated to link parameter estimates using only labeled data to parameter estimates based on all data. This makes the current approach more flexible.

Little work has been done on applying semi-supervised learning to the least squares classifier specifically. For least squares regression \cite{Shaffer1991} studied the value of knowing $\mathbb{E}[\mathbf{X}'\mathbf{X}]$, where $\mathbf{X}$ is the $N_l \times m$ design matrix containing the feature values for each observation. If we assume the number of unlabeled data points is large, this is similar to the semi-supervised situation. It is shown that if the size of the parameters is small compared to the noise, the variance of a procedure that plugs in $\mathbb{E}[\mathbf{X}'\mathbf{X}]$ as the estimate of $\mathbf{X}'\mathbf{X}$ has a lower variance than supervised least squares regression.  As the size of the parameters increases, this effect reverses. In fact, the paper demonstrates that in this semi-supervised setting no best linear unbiased estimator for the regression coefficients exists. In Section \ref{section:empiricalresults}, we compare our approach to using this plug-in estimate by substituting the matrix $\mathbf{X}'\mathbf{X}$ by a version based on both labeled and unlabeled data. 
A similar plug-in procedure has been used by \cite{Fan2008} for the dimensionality reduction technique that often is referred to as linear discriminant analysis. Here the (normalized) total scatter matrix, which plays a similar role to the $\mathbf{X}'\mathbf{X}$ matrix in least squares regression is exchanged with the more accurate estimate of the total scatter based on both labeled and unlabeled data.

\section{Implicitly Constrained Least Squares Classification}

Given a limited set of $N_l$ labeled objects and a potentially large set $N_u$ of unlabeled data, the goal of implicitly constrained least squares classification is to use the latter to improve the solution of the least squares classifier trained on just the labeled data. We start with a sketch of this approach, before discussing the details.

\begin{figure}[h] 
  \centering
      \includegraphics[width=0.5\textwidth]{constrainedspace.pdf}
  \caption{A visual representation of implicitly constrained semi-supervised learning. $\mathcal{F}_{\beta}$ is the space of all linear models. $\hat{\beta}_{sup}$ denotes the solution given only a small amount of labeled data. $\mathcal{C}_{\beta}$ is the subset of the space which contains all the solutions we get when applying all possible labelings to the unlabeled data. $\hat{\beta}_{semi}$ is a projection of $\hat{\beta}_{sup}$ onto $\mathcal{C}_{\beta}$. $\hat{\beta}_{N_l+N_u}$ is the supervised solution if we would have the labels for all the objects.} \label{fig:constrainedsubset}
\end{figure}

Given the supervised least squares classifier, consider the hypothesis space of all possible parameter vectors, which we will denote as $\mathcal{F}_{\beta}$, see Figure \ref{fig:constrainedsubset}. Given a set of labeled objects, we can determine the supervised parameter vector $\hat{\beta}_{sup}$. Suppose we also have a potentially large number $N_u$ of unlabeled objects. Assume that these object have a label, it is merely unknown to us. If these labels were to be revealed, it is clear how the additional objects can improve classification performance: we estimate the least squares classifier using all the data to obtain the parameter vector $\hat{\beta}_{N_l+N_u}$. Since this estimate is based on more objects, we expect the parameter estimate to be better. These real labels are unknown, but we still can consider all possible labelings of unlabeled objects, and estimate corresponding parameters based on these imputed labelings. In this way, we get a set of possible parameters for our classifier, which form the set denoted by $\mathcal{C}_{\beta}$, which is a subset of all possible solutions $\mathcal{F}_{\beta}$. Clearly one of these labelings corresponds to the real, but unknown, labeling, so one of the parameter estimates in this set corresponds to the solution we would obtain using all the correct labels of both the labeled and unlabeled objects. Because these are the only possible classifiers when the true labels would be revealed, we propose to look within this set $\mathcal{C}_{\beta}$ for an improved semi-supervised solution. 

Two issues then remain: how do we choose the best parameters from this set and how do we find these without having to enumerate all possible labelings?

Looking at the first problem, we reiterate that the goal of semi-supervised learning is to find a good classification rule and, therefore, still the obvious way to evaluate this rule is by the loss on the labeled training points. In other words, we choose the classifier from the parameter set that minimizes the loss on the labeled points. Note this approach is rather different from other approaches to semi-supervised learning where the loss is adapted by including a term that depends on the unlabeled data points. In our formulation, the loss function is still the regular, supervised loss of our classification procedure. We can the interpret the minimization of this loss under the constraint that its solution needs to be in $\mathcal{C}_{\beta}$ as a projection of $\hat{\beta}_{sup}$ onto the subset $\mathcal{C}_{\beta}$. We will denote this solution by $\hat{\beta}_{semi}$.

As for the second issue, after relaxing the constraint that we need hard labels for the data points, we will derive the gradient of the loss on the labeled training points with respect to the imputed labels of the unlabeled objects. This will allow us to find the optimal labeling through a simple gradient descent procedure without having to go through all possible labelings of the unlabeled data. 

\subsection{Multivariate Least Squares Classification}

Least squares classification \cite{Hastie2001,Rifkin2003} is the direct application of well-known ordinary least squares regression to a classification problem. In other words, a linear model is assumed and the parameters are minimized under squared loss. Let $\mathbf{X}$ be an $N_l \times (m+1)$ design matrix with $N_l$ rows containing vectors of length equal to the number of features $m$ plus one for the intercept. Vector $\textbf{y}$ denotes an $N_l \times 1$ vector of  class labels. Without loss of generality one class is encoded as $0$ and the other by $1$.  The multivariate version of the empirical risk function for least squares regression is given by

\begin{equation} \label{squaredloss}
\hat{R}(\beta) = \frac{1}{n} \left\|  \mathbf{X} \boldsymbol{\beta}-\mathbf{y} \right\| _2^2
\end{equation}
The well known closed-form solution for this problem is found by setting derivative with respect to $\boldsymbol{\beta}$ equal to $\textbf{0}$ and solving for $\boldsymbol{\beta}$, giving:

\begin{equation} \label{olssolution}
\boldsymbol{\hat{\beta}}=\left(\mathbf{X}^T \mathbf{X}\right)^{-1} \mathbf{X}^T \mathbf{y}
\end{equation}

In case $\textbf{X}^T \textbf{X}$ is not invertible (for instance when $n<(m+1)$), a pseudo-inverse is applied. As we will see, the convexity and subsequent closed form solution to this problem will enable us to formulate our semi-supervised learning approach in terms of a standard quadratic programming problem..

\subsection{Implicitly Constrained Least Squares Classification} \label{section:icls}

In the semi-supervised setting, apart from a design matrix $\textbf{X}$ and target vector $\textbf{y}$, an additional set of measurements $\textbf{X}_u$ of size $N_u \times (m+1)$ \emph{without} a corresponding target vector $\textbf{y}_u$ is given. In what follows, $\mathbf{X}_e=\begin{bmatrix} \mathbf{X}^T  & \mathbf{X}_u^T \end{bmatrix}^T$ denotes the extended design matrix which is simply the concatenation of the design matrices of the labeled and unlabeled objects.

In the implicitly constrained approach, we propose that a sensible solution to incorporate this additional information is to search within the set of classifiers that can be obtained by all possible labelings $\textbf{y}_u$, for the one classifier that minimizes the \emph{supervised} empirical risk function \eqref{squaredloss}. This set, $\mathcal{C}_{\boldsymbol{\beta}}$, is formed by the $\boldsymbol{\beta}$'s that would follow from training supervised classifiers on all (labeled and unlabeled) objects going through all possible soft labelings for the unlabeled samples, i.e., using all $\textbf{y}_u \in [0,1]^{N_u}$. Since these supervised solutions have a closed form, this can be written as:

\begin{equation} \label{constrainedregion}
\mathcal{C}_{\boldsymbol{\beta}} := \left\{   \boldsymbol{\beta} = \left( {\XeT} {\Xe} \right)^{-1} {\XeT} \ye: \mathbf{y}_u \in [0,1]^{N_u} \right\}
\end{equation}
Note that soft labelings are allowed. This is both a relaxation for computational reasons as well as a strategy to deal with label uncertainty. We can interpret these fractions as ``responsibilities'', a type of class posterior for the unlabeled objects. 

This constrained region $\mathcal{C}_{\boldsymbol{\beta}}$, combined with the supervised loss that we want to optimize \eqref{squaredloss}, gives the following definition for implicitly constrained semi-supervised least squares classification:

\begin{center}
\begin{equation}
\begin{aligned}
&\operatorname*{argmin}_{\boldsymbol{\beta} \in \mathbb{R}^{m+1}} & \frac{1}{n}  ||\mathbf{X} \boldsymbol{\beta}-\mathbf{y}||^2  \\
& \text{subject to} & \boldsymbol{\beta} \in \mathcal{C}_{\boldsymbol{\beta}}  \\
\end{aligned}
\end{equation}
\end{center}
Since $\boldsymbol{\beta}$ is fixed for a particular choice of $\textbf{y}_u$ and has a closed form solution (2), we can rewrite the minimization problem in terms of $\textbf{y}_u$ instead of $\boldsymbol{\beta}$:
\begin{equation} \label{icls}
\begin{aligned}
& \operatorname*{argmin}_{\mathbf{y}_u} & \frac{1}{n}  \left\|  X \left(\XeT \Xe \right)^{-1} \XeT \ye - \mathbf{y} \right\|_2^2 \\ 
& \text{subject to} & \mathbf{y}_u \in [0,1]^{N_u} \\
\end{aligned}
\end{equation}
Solving this optimization problem provides an optimal $\mathbf{y}_u$. The corresponding solution for $\boldsymbol{\beta}$ then follows from equation \eqref{olssolution} by using this imputed labeling as the labels for the unlabeled data. The problem defined in \eqref{icls}, is a standard quadratic programming problem of the form:
\begin{equation}
\begin{aligned}
& \quad \min_{\mathbf{y}_u} \frac{1}{2} \textbf{y}_u^T  \textbf{Q}  \textbf{y}_u + \textbf{c}^T \textbf{y}_u   & \\
& \text{subject to:}  \quad \begin{bmatrix} \textbf{I}_{N_u}  \\ -\textbf{I}_{N_u} \end{bmatrix}  \textbf{y}_u \leq \begin{bmatrix} \textbf{1}_{N_u}  \\ \textbf{0}_{N_u} \end{bmatrix} & \\
\end{aligned}
\end{equation}
where
\begin{equation}
\begin{aligned}
\textbf{Q} = & \frac{1}{n}  \textbf{X}_u \G \textbf{X}^T \textbf{X} \G \textbf{X}_u^T &\\
\end{aligned} \nonumber
\end{equation}
and
\begin{equation}
\begin{aligned}
\textbf{c} = & \frac{1}{n}  \textbf{X}_u \G \textbf{X}^T  \textbf{y} & \\
& + \frac{1}{2 n} \textbf{X}_u \G \textbf{X}^T \textbf{X} \G \textbf{X}^T \textbf{y}. &\\
\end{aligned} \nonumber
\end{equation}
Where $\textbf{I}_{N_l}$ denotes the $N_u \times N_u$ identity matrix and $\textbf{1}_{N_u}$ and $\textbf{0}_{N_u}$ denote column vectors of respectively ones and zeros.

Since the matrix \textbf{Q} is a product of a matrix and its transpose, it is guaranteed to be positive semi-definite. The problem is typically not positive definite because there are different labelings that will lead to one and the same minimum objective. 

The quadratic problem defined above can be solved using, for instance, an interior point method. We have found a gradient descent approach to be easier to implement. Ignoring the constraint $\textbf{y}_u \in [0,1]^{N_u}$ in \eqref{icls}, taking the derivative to $\textbf{y}_u$ and rearranging the terms we find: 
\begin{equation}
\begin{aligned}
&\frac { \partial \textbf{L} }{ \partial \textbf{y}_u } = & & \frac{2}{n}  \textbf{X}_u \G \textbf{X}^T \textbf{X} \G \textbf{X}^T \textbf{y}  \\
& & + & \frac{2}{n}  \textbf{X}_u \G \textbf{X}^T  \textbf{X}  \G  \textbf{X}_u^T \textbf{y}_u  \\
& & + & \frac{2}{n}  \textbf{X}_u \G \textbf{X}^T  \textbf{y}
\end{aligned}
\end{equation}
Because of its convexity, this problem can be solved efficiently using a quasi-Newton approach that allows for the simple [0,1] box bounds, such as L-BFGS-B \cite{Byrd1995}. Finally, the optimal labeling $\textbf{y}_u$ (as defined by the supervised loss function) gives us the semi-supervised estimate of $\boldsymbol{\beta}$.


\section{Theoretical Results}

\subsection{Strong performance result in 1D}
% We will examine this procedure by considering it in a simple, yet illustrative setting. In this case we will, in fact, prove this procedure will \emph{never} give worse performance than the supervised solution.
Consider the case where we have just one feature $x$, a limited set of labeled instances and assume we know the probability density function of this feature $f_X(x)$ exactly. This last assumption is similar to having unlimited unlabeled data and is also considered, for instance, in \cite{Sokolovska2008}. We consider a linear model with no intercept: $y = x \beta$ where $y$, without loss of generality, is set as $0$ for one class and $1$ for the other. For new data points, estimates $\hat{y}$ can be used to determine the predicted label of an object by using a threshold set at, for instance, $0.5$.

The expected squared loss, or risk, for this model is given by:

\begin{equation} \label{eq:trueloss}
R^*(\beta) = \sum_{y=\{0,1\}}{ \int_{-\infty}^{\infty}(x \beta - y)^2  f_{X,Y}(x,y)  \mathrm{d}x}
\end{equation}
Where  $f_{X,Y}$ is the joint density of X and Y. The Bayes optimal solution $\beta^*$ is given by the $\beta$ that minimizes this loss:

\begin{equation} \label{eq:bayesoptimal}
\beta^* = \operatorname*{argmin}_{\beta \in \mathbb{R}} R^*(\beta)
\end{equation}
Setting the derivative with respect to $\beta$ to $0$ and rearranging we get:

\begin{eqnarray}
&\beta & = \left( \int_{-\infty}^{\infty} { x^2 f_X(x) \mathrm{d}x} \right)^{-1} \sum_{y=\{0,1\}} \int_{-\infty}^{\infty} { x y f_{X,Y}(x,y) \mathrm{d}x } \\
& & =    \left( \int_{-\infty}^{\infty} { x^2 f_X(x) \mathrm{d}x} \right)^{-1}  \int_{-\infty}^{\infty} { x f_X(x) \sum_{y=\{0,1\}} y P(y|x) \mathrm{d}x} \\
& & =   \left( \int_{-\infty}^{\infty} { x^2 f_X(x) \mathrm{d}x} \right)^{-1}  \int_{-\infty}^{\infty} { x f_X(x) \mathbb{E}[y|x] \mathrm{d}x} \label{eqn:sslsolution}
\end{eqnarray}

In this last equation, since the assume $f_X(x)$ as given, the only unknown is the function $\mathbb{E}[y|x]$, the expectation of the label $y$, given $x$. Now suppose we consider every possible labeling of the unlimited number of unlabeled objects including fractional labels, that is, every possible function where $\mathbb{E}[y|x] \in [0,1]$. Given this restriction on $\mathbb{E}[y|x]$, the latter integral becomes a re-weighted version of the expectation operation $\mathbb{E}[x]$. By changing the choice of $\mathbb{E}[y|x]$ one can vary the value of this integral, but it will always be bounded on an interval on $\mathbb{R}$. It follows that all possible $\beta$'s also form an interval on $\mathbb{R}$, which is the constrained set $\mathcal{C}_{\boldsymbol{\beta}}$. The Bayes optimal solution has to be in this interval, since it corresponds to a particular but unknown labeling $\mathbb{E}[y|x]$.

\begin{figure}[!ht] 
  \centering
      \includegraphics[width=0.8\textwidth]{ConstrainedProblem-1D.pdf}
  \caption{An example where implicitly constrained optimization always improves performance. The supervised solution $\hat{\beta}_{sup}$ which minimizes the supervised loss (shown), is not part of the interval of allowed solutions. The solution that minimizes this supervised loss within the allowed interval is $\hat{\beta}_{semi}$. This solution is closer to the Bayes optimal solution ${\beta}^{bayes}$ than the supervised solution $\hat{\beta}_{sup}$.} \label{fig:constrainedproblem}
\end{figure}

Using the set of labeled data, we can construct a supervised solution $\hat{\beta}_{sup}$ that minimizes the loss on the training set of $N_l$ labeled objects, see Figure \ref{fig:constrainedproblem}:

\begin{equation} \label{supervisedsolution}
\hat{\beta}_{sup} = \operatorname*{argmin}_{\beta \in \mathbb{R}} \sum_{i=1}^{N_l} (x_i \beta - y_i)^2
\end{equation}

Now, either this solution falls within the constrained region, $\hat{\beta}_{sup} \in \mathcal{C}_{\boldsymbol{\beta}}$ or not, $\hat{\beta}_{sup} \notin \mathcal{C}_{\boldsymbol{\beta}}$, with different consequences:

\begin{enumerate}
  \item If $\hat{\beta}_{sup} \in \mathcal{C}_{\boldsymbol{\beta}}$ there is a labeling of the unlabeled points that gives us the same value for $\beta$. Therefore, the solution falls within the allowed region and there is no reason to update our estimate. Therefore $\hat{\beta}_{semi}=\beta_{sup}$.
  \item Alternatively, if $\hat{\beta}_{sup} \notin  \mathcal{C}_{\boldsymbol{\beta}}$, the solution is outside of the constrained region (as shown in Figure \ref{fig:constrainedproblem}): there is no possible labeling of the unlabeled data that will give the same solution for $\beta$. We then update the $\beta$ to be the $\beta$ within the constrained region that minimizes the loss on the supervised training set. As can be seen from Figure \ref{fig:constrainedproblem}, this will always be a point on the boundary of the interval. Note that $\hat{\beta}_{semi}$ is now closer to $\beta^{*}$ than $\hat{\beta}_{sup}$. Since the true loss function $R^*(\beta)$ is convex  and achieves its minimum in the Bayes optimal solution, the true loss of our semi-supervised solution will always be equal to or lower than the loss of the supervised solution.
\end{enumerate}

Thus, the proposed update either improves the estimate of the parameter $\beta$ or it does not change the supervised estimate. In no case will the semi-supervised solution be worse than the supervised solution, in terms of the expected squared loss. We summarize this result in the following theorem:

\begin{theorem} \mbox{}
Given a linear model without intercept, $y = x\beta$, and $f_X(x)$ known, the estimate obtained through implicitly constrained least squares performs at least as good as the regular least squares solution: $R^\ast (\hat{\beta}_{semi}) \le R^\ast (\hat{\beta}_{sup})$.

In particular, if $f_{X,Y}$ is continuous and $f_{X,Y}(0,1) > 0$, then \\ $\mathbb{E}[R^*(\hat{\beta}_{semi})] < \mathbb{E}[R^*(\hat{\beta}_{sup})]$.
\end{theorem}

The last part of this theorem gives a general condition when, in expectation, our semi-supervised approach will outperform the supervised learner. Because $\hat{\beta}_{semi}$ will never be worse than $\hat{\beta}_{sup}$, to prove this we only need to show that for some observation of a labeled point with positive $f_{X,Y}(x,y)>0$, the estimated $\hat{\beta}^{SL}$ is outside of the interval $\mathcal{C}_{\boldsymbol{\beta}}$, in which case $R^*(\hat{\beta}_{semi}) < R^*(\hat{\beta}_{sup})$. 

If we observe an object labeled $1$ with feature value $x$, the corresponding estimate $\hat{\beta}_{sup}=\tfrac{1}{x}$. Since the improvement in loss will only result if this estimate is not in the constrained region, we need to show that:

\begin{equation} \label{eq:condition}
P(\tfrac{1}{x} \notin \mathcal{C}_{\boldsymbol{\beta}},y=1)>0
\end{equation}

To do this, consider the bounds of the interval $\mathcal{C}_{\boldsymbol{\beta}}$. These most extreme values are obtained whenever all negative values of $x$ are assigned label $0$ while the positive $x$ get labels $1$, or the other way around. From \eqref{eqn:sslsolution} and writing $\mathbb{E}(X^2)=\int_{-\infty}^{\infty} { x^2 f_X(x) \mathrm{d}x}$ we find the interval is given by:  

\begin{equation} \label{eq:condition2}
\mathcal{C}_{\boldsymbol{\beta}}=\left[ \frac{\int_{-\infty}^{0}{x f_X(x) \mathrm{d}x }}{\mathbb{E}(X^2)},\frac{\int_{0}^{\infty}{x f_X(x)  \mathrm{d}x }}{\mathbb{E}(X^2)} \right]
\end{equation}
Using this and rearranging \eqref{eq:condition2}, we get the following condition:

\begin{equation} \label{eq:condition3}
P \left( \frac{\mathbb{E}(X^2)}{\int_{-\infty}^{0}{x f_X(x)  \mathrm{d}x }} < x < 0 \vee 0 < x < \frac{\mathbb{E}(X^2)}{\int_{0}^{\infty}{x f_X(x)  \mathrm{d}x }},y=1 \right) > 0
\end{equation}
Since $f_{X,Y}$ is assumed to be continuous, $\mathbb{E}[X^2]>0$ and the lower bound in this equation is always smaller than $0$, while the upper bound is always larger than $0$.  Therefore, \eqref{eq:condition2} holds whenever $f_{X,Y}(0,1)>0$. The assumption of the continuity of $f_{X,Y}$ ensures that  \eqref{eq:condition3} holds.  The property $f_{X,Y}(0,1)>0$ is satisfied by many distributions of the data. The result, therefore, indicates, that improvement is not only possible, but will occur in many cases. 

%The strong result of Theorem 1, that performance \emph{never} gets worse,  holds in the 1D case with unlimited unlabeled data and no intercept in the model. A slightly weaker result, that performance does not degrade \emph{on average} may still hold without these assumptions. The set of classifiers induced by the possible labelings of a large number of unlabeled objects is  likely to contain a better solution than the supervised solution based on limited labeled data alone. By finding the solution within this set that minimizes the loss on the labeled data, we may still, on average, always improve performance.

\subsection{Euclidean Projection Estimator for Multivariate ICLS}

In the multivariate case (with intercept), we will prove that the solution vector obtained through an adapted version of the implicitly constrained least squares classifier is always as close or closer to the real coefficients $\beta$ than the supervised solution, when using Euclidean distance as a measure of closeness. While not directly equivalent to the goal of classification error, this different notion of statistical risk is commonly used in many areas of statistics \cite{Berger1985}. We can use this risk as an alternative way to study in what sense the constrained parameter space may offer an improved estimator over the supervised estimator.

In the adapted ICLS procedure considered, we minimize a slightly different risk function than the squared loss on the labeled objects used in ICLS:

\begin{equation} \label{eq:adaptedICLS}
\boldsymbol{\hat{\beta}}_{adapted} = \operatorname*{argmin}_{\boldsymbol{\beta} \in \mathcal{C}_{\boldsymbol{\beta}}}  ||\boldsymbol{\beta}-\boldsymbol{\hat{\beta}}_{sup}||^2 
\end{equation}

This can be interpreted as finding the projection of the supervised solution $\boldsymbol{\hat{\beta}}_{sup}$ onto the constrained space $\mathcal{C}_{\boldsymbol{\beta}}$ defined in \ref{constrainedregion}, where the projection is the minimum distance to this region, measured by the Euclidean distance. Note that the difference between this and the regular ICLS procedure is the measure used to calculate this distance. $\boldsymbol{\hat{\beta}}_{adapted}$ uses Euclidean distance, while $\boldsymbol{\hat{\beta}}_{semi}$ uses the loss on the labeled objects to determine the best element in $\mathcal{C}_{\boldsymbol{\beta}}$. The subset of the parameter space onto which we project is still the same. For this adapted ICLS we can prove the following:

\begin{theorem}
Given a multivariate linear model, $y = \boldsymbol{X} \boldsymbol{{\beta}}$, and $f_X(\boldsymbol{x})$ known, we have for $\boldsymbol{\hat{\beta}}_{adapted}$ as defined in Equation \eqref{eq:adaptedICLS}, that  
\begin{equation}
||\boldsymbol{\hat{\beta}}_{adapted}-\boldsymbol{{\beta}}|| \leq ||\boldsymbol{\hat{\beta}}_{sup}-\boldsymbol{{\beta}}||.
\end{equation}
\end{theorem}

To prove this result, we first show that the constrained region $\mathcal{C}_{\boldsymbol{\beta}}$ is convex. Note that the constrained space in terms of $\mathbf{y}_u$, $\mathcal{C}_{\mathbf{y}_u}=[0,1]^{N_u}$ is convex. Now, for every pair $b_1, b_2 \in \mathcal{C}_{\boldsymbol{\beta}}$, their corresponding labelings $\mathbf{y}_{u_1}, \mathbf{y}_{u_2}$ and all $c \in [0,1]$, we have that: 

\begin{equation}
\begin{aligned}
&c b_1 + (1-c) b_2 \\
=&c \left( {\XeT} {\Xe} \right)^{-1} {\XeT} \begin{bmatrix} \mathbf{y}  \\ \mathbf{y}_{u_1} \end{bmatrix} + (1-c) \left( {\XeT} {\Xe} \right)^{-1} {\XeT} \begin{bmatrix} \mathbf{y}  \\ \mathbf{y}_{u_2} \end{bmatrix} \\
=&\left( {\XeT} {\Xe} \right)^{-1} {\XeT} \begin{bmatrix} \mathbf{y}  \\ c \mathbf{y}_{u_1} + (1-c) \mathbf{y}_{u_2}  \end{bmatrix} \in \mathcal{C}_{\beta}
\end{aligned}
\end{equation}
Where the conclusion that this estimate is an element of $\mathcal{C}_{\beta}$ follows from the fact that the space of labelings $\mathcal{C}_{y_u}=[0,1]^{N_u}$ is convex. 

We now note the following result which can be found in, for instance, Proposition 1.4.1ii in \cite[p.17]{Aubin2000}: For any two elements $a,b$ in a Hilbert space $H$ we have that $||P_C(a)-P_C(b)|| \leq ||a-b||$, where $P_C$ is the best approximation projector onto a closed convex subset $C \subset H$.

Now, take $a$ to be $\boldsymbol{\hat{\beta}_{sup}}$, and $b$ to be $\boldsymbol{\beta}$, the optimal parameter vector. Since $\boldsymbol{\beta} \in \mathcal{C}_{\beta}$, its projection is $P_{\mathcal{C}_{\beta}}(\boldsymbol{\beta})=\boldsymbol{\beta}$. The projection of $\boldsymbol{\hat{\beta}}_{sup}$ is $\boldsymbol{\hat{\beta}}_{adapted}$. By the theorem above, we have $||\boldsymbol{\hat{\beta}}_{adapted}-\boldsymbol{\beta}|| \leq ||\boldsymbol{\hat{\beta}}_{sup}-\boldsymbol{\beta}||$ which proves the result.

Note that in 1D the Euclidean projection of Equation (\refeq{eq:adaptedICLS}) and the regular ICLS projection are the same, which can also be seen in Figure \ref{fig:constrainedproblem}. In the multivariate case, the two projections are not the same, meaning a better estimator in terms of Euclidean distance does not necessarily imply a better estimator in terms of squared loss. While the projection of regular ICLS onto the subset is slightly different from the Euclidean projection, the result in Theorem 2 indicates some form of improvement is possible by projecting onto the subset $\mathcal{C}_{\boldsymbol{\beta}}$. 

\section{Empirical Results} \label{section:empiricalresults}

The empirical properties of our proposed approach are evaluated in two ways. Firstly, we study the behavior of the error rates of our semi-supervised approach for increasing amounts of unlabeled data. The goal is to study whether the ICLS procedure exhibits the improvement in expectation that motivated this method. Secondly, we compare the classification performance of these approaches in a cross-validation setting. This setting is in line with how these procedures are used in practice and allows us to study whether the procedure offers expected improvements there as well.
% several state-of-the-art semi-supervised methods described in Section \ref{section:relatedwork} on various benchmark datasets.

Since we extended the least squares classifier to the semi-supervised setting, we compare how, for different sizes of the unlabeled sample,  our semi-supervised least squares approach fares against supervised least squares classification (LS) without the constraints. For comparison we included two alternative semi-supervised strategies, namely self-learning applied to the least squares classifier (SLLS) and a procedure were the matrix $\mathbf{X}^T \mathbf{X}$ is replaced by an appropriately scaled matrix $\XeT \Xe$ similar to the estimator in \cite{Shaffer1991}. We will refer to the latter as updated covariance least squares (UCLS) classification. We also included the performance of the least squares classifier if all unlabeled object were to be labeled (LSoracle). This serves as the unattainable upper bound on the performance of any semi-supervised learner. 

A description of the datasets used for our experiments is given in Table \ref{table:datasets}. We use datasets from both the UCI repository \cite{Bache2013} and six of the benchmark datasets proposed by \cite{Chapelle2006}. While the benchmark datasets proposed in \cite{Chapelle2006} are useful, in our experience, the results on these datasets are very homogeneous because of the similarity in the dimensionality and their low Bayes errors. The UCI datasets are more diverse both in terms of the number of objects and features as well as the nature of the underlying problems. Taken together, this collection allows us to investigate the properties of our approach for a wide range of problems.

All the code used to run the experiments is available from the authors' website.

\begin{table}[ht] 
\caption{Description of the datasets used in the experiments}
\begin{center}
\begin{tabular}{rrrr}
  \hline
 Name & \# Objects & \#Features & Source \\ 
  \hline
  Haberman & 305 &   4 & \cite{Bache2013} \\ 
  Ionosphere & 351 &  33 & \cite{Bache2013} \\ 
  Parkinsons & 195 &  20 & \cite{Bache2013} \\ 
  Pima & 768 &   9 & \cite{Bache2013} \\ 
  Sonar & 208 &  61 & \cite{Bache2013} \\ 
  SPECT & 265 &  23 & \cite{Bache2013} \\ 
  SPECTF & 265 &  45 & \cite{Bache2013} \\ 
  Transfusion & 748 &   4 & \cite{Bache2013} \\ 
  WDBC & 568 &  30 & \cite{Bache2013} \\
  Mammographic & 960 & 4 & \cite{Bache2013} \\
  Digit1 & 1500 & 242 & \cite{Chapelle2006} \\ 
  USPS & 1500 & 242 & \cite{Chapelle2006}  \\ 
  COIL2 & 1500 & 242 & \cite{Chapelle2006} \\ 
  BCI & 400 & 118 & \cite{Chapelle2006} \\ 
  g241c & 1500 & 242 & \cite{Chapelle2006} \\ 
  g241n & 1500 & 242 & \cite{Chapelle2006} \\ 
   \hline
\end{tabular}
\end{center}

\label{table:datasets}
\end{table}

\subsection{Comparison of Learning Curves}
We study the behavior of the expected classification error of the ICLS procedure for different sizes for the unlabeled set. As we noted in the introduction, this statistic has two desired properties. First of all it should never be higher than the expected classification error of the supervised solution. Secondly, the expected classification error should not increase as we add more unlabeled data. 

Experiments were conducted as follows. For each dataset, $N_l$ labeled points were randomly chosen, where we make sure it contains at least 1 object from each of the two classes.  With fewer than $m$ samples, the least squares classifier is known to deteriorate in performance as more data is added, a behavior known as peaking \cite{Raudys1998,Opper1996}. Since this is not the topic of this work, we will only consider the situation in which the labeled design matrix is of full rank, which is ensured by setting $N_l=m+5$, the dimensionality and intercept of the dataset plus five observations. For all datasets we ensure a minimum of $N_l=20$ labeled objects.

Next, we create unlabeled subsets of increasing size $N_u=[2,4,8,...,1024]$ by randomly selecting points from the original dataset without replacement. The classifiers are trained using these subsets and the classification performance is evaluated on the remaining objects. Since the test set decreases in size as the number of unlabeled objects increases, the standard error is slightly increases with the number of unlabeled objects.

This procedure of sampling labeled and unlabeled points is repeated $100$ times. We report the mean classification error as well as the standard error of this mean. As can be seen from the tight confidence bands, this offers an accurate estimate of the expected classification error.

% This process of sampling the labeled objects and creating the unlabeled subsets is repeated 100 times and the average classification error and loss on the test set is determined. The latter is done to evaluate whether the approach is effective in increasing generalization performance in terms of the loss used in estimating the classifier. Even though in applications the ultimate goal may typically be classification performance, this allows us to see whether problems occur because of the optimization itself, or because of the link surrogate loss and classification error.

\begin{figure}[ht!] 
  \centering
      \includegraphics[width=1.0\textwidth]{LearningCurves-1.pdf}
  \caption{Mean classification error for $N_l=\max(m+5,20)$ and $100$ repeats. The error bounds are $+/-$ the standard error of the mean.} \label{fig:learningcurves1}
\end{figure}

\begin{figure}[ht!] 
  \centering
      \includegraphics[width=1.0\textwidth]{LearningCurves-2.pdf}
  \caption{Mean classification error for $N_l=\max(m+5,20)$ and $100$ repeats. The error bounds are $+/-$ the standard error of the mean.} \label{fig:learningcurves2}
\end{figure}

The results of these experiments are shown in Figures \ref{fig:learningcurves1} and \ref{fig:learningcurves2}. Note that the error-axis in the figures does not extend to $0$ in order to show the differences between the various methods more clearly. UCLS is left out of this analysis, since its performance was often on a very different scale than that of the other procedures.

We find that the ICLS procedure has monotonically decreasing error curves as the number of unlabeled samples increases. Unlike self-learning, there is no deterioration in performance. This is especially apparent, for instance, on the Pima dataset. For the datasets presented in Figure \ref{fig:learningcurves2}, it is interesting to see how the ICLS curves resemble a scaled version of the LS classifier were we assume all data points are labeled. Self-learning does not share this property.

% Also note the difference between the error curves and the loss. It is quite surprising that the loss on the test set decreases in almost every case. Another interesting fact is that the self-learner has similar performance in terms of loss minimization, while its error curves are markedly different.

\subsection{Benchmark performance}
We now consider the cross-validation setting for these datasets. This experiment is set up as follows. For each dataset, the objects are randomly divided into $10$ folds. We iteratively go through the folds using $1$ fold as validation set, and the other $9$ as the training set. From this training set, we then randomly select $N_l=max(m+5,20)$ labeled objects, as in the previous experiment, and use the rest as unlabeled data. After predicting labels for the validation set for each fold, the classification error is then determined by comparing the predicted labels to the real labels. This is repeated $10$ times, while randomly assigning objects to folds in each iteration.

The cross-validation procedure used here is slightly different from that prescribed in \cite{Chapelle2006}, to make it more closely relate to the cross-validation procedure that is usually employed. More specifically, this procedures ensures the validation sets are independent (non-overlapping), such that, after going over all the folds, each object is in the validation set only once. This is different from the procedure in \cite{Chapelle2006}, were the authors ensure the \emph{labeled} sets or non-overlapping. We have not found a qualitative difference in the error rates, however, when using the procedure proposed in \cite{Chapelle2006}. The advantage of the procedure employed here is that every object gets a single predicted label, allowing for the direct comparison of predictions of different classifiers.

% \input{cvtable.tex}

\begin{table}
\caption{Average 10-fold cross-validation error and standard deviation over 10 repeats. The classifiers that have been compared are supervised Least Squares (LS), Implicitly constrained least squares (ICLS), self-learned least squares (SLLS), updated covariance least squares (UCLS, see text) and for comparison a supervised least squares classifier that has access to all the labels (LSoracle). Indicated in $\mathbf{bold}$ is whether a semi-supervised classifier significantly outperform the supervised LS classifier, as measured using a $t$-test with a $0.05$ significance level. \underline{Underlined} indicates whether a semi-supervised classifier is (significantly) best among the three semi-supervised classifiers considered.} \label{table:cvresults}
\begin{tabular}{l|lllll}
Dataset & LS & ICLS & SLLS & UCLS & LSoracle \\ 
\hline
Haberman & $0.28 \pm 0.02$& $0.28 \pm 0.02$& $0.28 \pm 0.01$& $0.39 \pm 0.02$& $0.26 \pm 0.01$\\ 
Ionosphere & $0.28 \pm 0.02$& $\mathbf{\underline{0.19 \pm 0.02}} $& $\mathbf{0.24 \pm 0.01} $& $0.35 \pm 0.05$& $0.14 \pm 0.01$\\ 
Parkinsons & $0.27 \pm 0.02$& $\mathbf{\underline{0.24 \pm 0.02}} $& $0.25 \pm 0.04$& $0.40 \pm 0.03$& $0.16 \pm 0.01$\\ 
Pima & $0.32 \pm 0.02$& $0.30 \pm 0.01$& $0.35 \pm 0.01$& $0.40 \pm 0.01$& $0.23 \pm 0.00$\\ 
Sonar & $0.44 \pm 0.02$& $\mathbf{\underline{0.35 \pm 0.03}} $& $\mathbf{0.39 \pm 0.02} $& $0.43 \pm 0.03$& $0.25 \pm 0.01$\\ 
SPECT & $0.42 \pm 0.05$& $\mathbf{\underline{0.33 \pm 0.03}} $& $0.42 \pm 0.03$& $0.45 \pm 0.04$& $0.18 \pm 0.01$\\ 
SPECTF & $0.44 \pm 0.03$& $\mathbf{\underline{0.37 \pm 0.03}} $& $\mathbf{0.39 \pm 0.02} $& $0.46 \pm 0.02$& $0.23 \pm 0.01$\\ 
Transfusion & $0.26 \pm 0.01$& $0.25 \pm 0.01$& $0.28 \pm 0.03$& $0.34 \pm 0.02$& $0.23 \pm 0.00$\\ 
WDBC & $0.10 \pm 0.02$& $\mathbf{\underline{0.08 \pm 0.01}} $& $0.12 \pm 0.02$& $0.29 \pm 0.02$& $0.05 \pm 0.00$\\ 
Mammographic & $0.28 \pm 0.02$& $0.28 \pm 0.02$& $0.28 \pm 0.02$& $0.41 \pm 0.04$& $0.20 \pm 0.00$\\ 
Digit1 & $0.42 \pm 0.02$& $\mathbf{\underline{0.20 \pm 0.01}} $& $\mathbf{0.34 \pm 0.02} $& $\mathbf{0.39 \pm 0.02} $& $0.06 \pm 0.00$\\ 
USPS & $0.42 \pm 0.01$& $\mathbf{\underline{0.20 \pm 0.01}} $& $\mathbf{0.34 \pm 0.02} $& $\mathbf{0.38 \pm 0.02} $& $0.09 \pm 0.00$\\ 
COIL2 & $0.38 \pm 0.01$& $\mathbf{\underline{0.20 \pm 0.01}} $& $\mathbf{0.26 \pm 0.01} $& $0.40 \pm 0.01$& $0.10 \pm 0.00$\\ 
BCI & $0.41 \pm 0.03$& $\mathbf{\underline{0.28 \pm 0.03}} $& $\mathbf{0.36 \pm 0.04} $& $0.41 \pm 0.02$& $0.16 \pm 0.02$\\ 
g241c & $0.45 \pm 0.01$& $\mathbf{\underline{0.28 \pm 0.01}} $& $\mathbf{0.39 \pm 0.01} $& $\mathbf{0.42 \pm 0.01} $& $0.14 \pm 0.00$\\ 
g241n & $0.45 \pm 0.02$& $\mathbf{\underline{0.28 \pm 0.01}} $& $\mathbf{0.39 \pm 0.01} $& $\mathbf{0.43 \pm 0.02} $& $0.13 \pm 0.00$\\ 
\end{tabular}
\end{table}


The results shown in Table \ref{table:cvresults} tell a similar story to those in the previous section. Most importantly for the purposes of this paper, ICLS, in general, offers solutions that give at least no higher expected classification error than the supervised procedure. Exceptions are the Transfusion dataset were both ICLS and the self-learning approach show (non statistically significant) deterioration in performance, and Pima where ICLS shows non-statistically significant deterioration while self-learning does show statistically significant deterioration. Particularly on the last six datasets, ICLS offers large improvement in classification accuracy over the supervised solution. The differences in performance between ICLS and self-learning can also be quite substantial, where ICLS outperforms self-learning on most of the datasets. 

Self-learning also performs quite well in the experiments, especially considering its low computational cost. Unlike ICLS, it does lead to significantly higher classification error than supervised learning on the 'Pima' and 'WDBC' datasets.

UCLS often shows major deterioration in performance. It does show some minor improvements on some of the SSL benchmark datasets, in particular USPS, g241c and g241n. In the latter cases, improvements are small. It is unclear whether these effects are real, given the small deterioration in performance on related datasets.

% Similar to \cite{Chapelle2006}, we choose to compare our linear method to state-of-the-art linear semi-supervised classifiers. In this case, this means logistic regression and entropy regularization proposed by \cite{Grandvalet2005}, the linear support vector machine and its Transductive SVM as implemented by \cite{Sindhwani2006}, and self-learning applied to these two supervised classifiers.

% To evaluate the performance of the different methods, the datasets were divided in $\tfrac{n}{(m+2)}$ folds, following the same argument as in the first experiment. Sequentially, each fold was used for training, while the rest of the data were used as both unlabeled data and as the test set. This is therefore a transductive learning setting. The setup is similar to \cite{Chapelle2006}, but with more folds used. The averaged classification errors over all folds can be found in Table \ref{table:cvtable}.

% latex table generated in R 2.15.2 by xtable 1.7-0 package
% Mon Apr 22 21:43:10 2013
% \begin{table}[ht] \label{table:results}
% \caption{The results comparing the proposed technique to alternatives on several benchmark datasets. Least Squares Classifier (LS), Implicitly Constrained Least Squares Classifier (ICLS), Self Learning Least Squares Classifier (SLLS),  Logistic Regression(LR), Entropy Regularized Logistic Regression (ERLR), Self Learning Logistic Regression (SLLR), Linear Support Vector Machine (SVM), Transductive Support Vector Machine (TVM), Self Learning Support Vector Machine (SLSVM). Bold results indicate the lowest error on a dataset for compared to other versions of the same classifier (LS, LR or SVM). Note ICLS is always best among the LS classifiers, while this does not hold for the semi-supervised versions of the LR or SVM.}


% \begin{center}
% \begin{tabular}{l|lll|lll|lll}
%   \hline
%  & LS & ICLS & SLLS & LR & ERLR & SLLR & SVM & TSVM & SLSVM \\ 
%   \hline
% Haberman & 0.37 & \textbf{0.35} & \textbf{0.35} & \textbf{0.28} & 0.35 & \textbf{0.28} & 0.35 & 0.39 & \textbf{0.34} \\ 
%   Ionosphere & 0.34 & \textbf{0.20} & 0.28 & 0.29 & \textbf{0.26} & 0.33 & \textbf{0.21} & 0.26 & \textbf{0.21} \\ 
%   Parkinsons & 0.28 & \textbf{0.27} & 0.30 & \textbf{0.25} & 0.30 & \textbf{0.25} & \textbf{0.22} & 0.23 & \textbf{0.22} \\ 
%   Pima & 0.41 & \textbf{0.34} & 0.41 & 0.42 & \textbf{0.41} & 0.43 & \textbf{0.34} & 0.45 & \textbf{0.34} \\ 
%   Sonar & 0.41 & \textbf{0.33} & 0.39 & 0.50 & \textbf{0.31} & 0.50 & \textbf{0.27} & 0.42 & \textbf{0.27} \\ 
%   SPECT & 0.46 & \textbf{0.34} & 0.44 & \textbf{0.21} & 0.25 & \textbf{0.21} & 0.28 & \textbf{0.23} & 0.28 \\ 
%   SPECTF & 0.45 & \textbf{0.39} & 0.42 & \textbf{0.21} & 0.79 & \textbf{0.21} & \textbf{0.25} & 0.42 & \textbf{0.25} \\ 
%   Transfusion & 0.36 & \textbf{0.32} & 0.36 & \textbf{0.26} & 0.34 & \textbf{0.26} & 0.32 & 0.37 & \textbf{0.31} \\ 
%   WDBC & 0.38 & \textbf{0.37} & \textbf{0.37} & 0.48 & \textbf{0.44} & 0.48 & \textbf{0.08} & 0.15 & \textbf{0.08} \\ 
%   Digit1 & 0.39 & \textbf{0.20} & 0.34 & 0.50 & \textbf{0.07} & 0.50 & 0.08 & \textbf{0.08} & \textbf{0.08} \\ 
%   USPS & 0.38 & \textbf{0.20} & 0.33 & 0.20 & \textbf{0.11} & 0.20 & \textbf{0.12} & 0.23 & \textbf{0.12} \\ 
%   COIL2 & 0.36 & \textbf{0.21} & 0.27 & \textbf{0.47} & 0.49 & 0.50 & 0.17 & \textbf{0.15} & 0.17 \\ 
%   BCI & 0.47 & \textbf{0.32} & 0.47 & 0.41 & \textbf{0.33} & 0.40 & \textbf{0.30} & 0.35 & \textbf{0.30} \\ 
%   g241c & 0.45 & \textbf{0.29} & 0.40 & 0.17 & \textbf{0.16} & 0.43 & 0.23 & \textbf{0.17} & 0.23 \\ 
%   g241n & 0.47 & \textbf{0.28} & 0.42 & 0.22 & \textbf{0.17} & 0.50 & 0.23 & \textbf{0.18} & 0.23 \\ 
%    \hline
% \end{tabular}

% \end{center}
% \end{table}



% Note that the \emph{supervised} linear support vector machine outperforms most other methods on many of the datasets. The semi-supervised SVM, however, is much less stable and on some datasets, has worse performance. This is not the case for our approach, which is always the preferred approach among the least squares classifiers.

\section{Discussion}
The theoretical results in Section 4 show that projecting onto a constrained subset $\Cb$ leads to improvement in terms of squared loss (Theorem 1) and in terms of Euclidean distance of the parameter estimate (Theorem 2). These results are encouraging in the light of negative theoretical performance results in the semi-supervised literature \cite{Cozman2006}. The empirical results in the previous section indicate that in terms of the expected classification error, ICLS never significantly deteriorates with increasing amounts of unlabeled data on our collection of datasets. These empirical observations are all the more interesting considering that the loss evaluated in Section 5 is misclassification error and not the squared loss that was considered in Theorem 1 or the Euclidean parameter distance of Theorem 2. Furthermore the experiments were carried out on limited unlabeled data, not the unlimited setting considered in the theorems. This indicates that projecting onto the subset $\Cb$, leads to a semi-supervised learner with desirable behavior, both theoretically in terms of various measures of risk and empirically in terms of classification error.

Some have argued that, for discriminative classifiers, semi-supervised learning is impossible without additional assumptions about the link between labeled and unlabeled objects \cite{Seeger2001}. ICLS, however, is both a discriminative classifier and no explicit additional assumptions about this link are made. Any assumptions that are present follow, implicitly, from the choice of squared loss as the loss function. Furthermore, no additional parameters need to be correctly set for the results in Sections 4 and 5. There is, for instance, no weight to be chosen for the importance of the unlabeled data. Therefore, implicitly constrained semi-supervised learning  isa very different approach to semi-supervised learning than the methods discussed in Section 2.

The quadratic programming formulation of ICLS presented in Section 3 allows one to use the standard and constantly improving tools from convex optimization to find the ICLS estimator. Unfortunately one has to go from a convex problem with $m$ variables in the supervised case to a constrained convex problem with $N_u$ variables for ICLS. For very large $N_u$, this may not currently be computationally feasible. Improvements in quadratic programming solvers may change this. Additionally, instead of finding the exact optimal labeling $\mathbf{y}_u$, as was employed in our experiments, approximations to these optima may take less time to compute without having a large effect on the final estimate of $\boldsymbol{\hat{\beta}}_{semi}$.

Compared to ICLS, self-learning is much more favorable in terms of computational cost. Self-learning usually converges in a few iterations, where each iteration has the cost of one supervised least squares estimation. As we noted in Section 5 the self-learning approach can increase performance, but large amounts of unlabeled data can also have a detrimental effect. Also, the performance of ICLS is significantly better on many of the datasets considered in our experiments. Hence the price one pays for the low computational cost is in terms of classification error. Note that the solution provided by self-learning is, by construction, also in the constrained subset $\Cb$. The difference with ICLS is that in ICLS the choice of estimate from $\Cb$ is based on information of the labeled objects only, while SLLS also uses the imputed labels on the unlabeled objects. This may lead to self-deception: if the imputed labels are wrong, a good fit for these wrongly imputed labels does not necessarily lead to a good estimate of $\boldsymbol{\beta}$.

The plug-in version of the LS, UCLS, while fast and intuitive, does not perform well. We found that it only offers some improvement on datasets with low Bayes error. This does not correspond to the observations of \cite{Shaffer1991} that the covariance update only decreases the parameter value in high noise settings. While we do not currently fully understand this behavior, it may be related to the finite sample estimate of $\mathbb{E}[\mathbf{X}'\mathbf{X}]$ that we consider or the differences in modeling assumptions when going from the regression setting considered in \cite{Shaffer1991} to the classification setting considered here.

In Figure 1, we illustrate that projecting onto the subset $\Cb$ causes improvement as long as a better solution $\hat{\beta}_{N_l+N_u}$ than the supervised solution is within $\Cb$. A smaller $\Cb$ will give a larger improvement, since the projection is going to be closer to $\hat{\beta}_{N_l+N_u}$. In the extreme case where only $\hat{\beta}_{N_l+N_u}$ forms the subset, this clearly gives a great improvement over supervised learning. It therefore makes sense to think about reducing the size of $\Cb$. In the approach presented in this work, however, to ensure a better solution $\hat{\beta}_{N_l+N_u}$ than the supervised solution is always within the constrained set with probability $P(\hat{\beta}_{N_l+N_u} \in \Cb)=1$, our choice of $\Cb$ is conservatively large. It contains elements corresponding to all labelings of the unlabeled points,  even extremely unlikely ones. By excluding unlikely labelings from the subset, the size of the $\Cb$ may shrink, while the probability that it includes $\hat{\beta}_{N_l+N_u}$ remains high. For instance, one might exclude labelings with class priors that are very unlikely to occur, given the class priors that are observed in the labeled data. Changes to $\Cb$ may, therefore, allow for larger improvements in terms of the risk or classification error, while introducing a small chance of deterioration in performance.

While the results presented in this work are promising for squared loss, a worthwhile extension would be to other loss functions. In this work, we were able to derive a quadratic programming formulation for ICLS because there is a closed-form solution of the supervised least squares problem. For many loss functions, closed-form solutions do not exist, which complicates the derivation of their semi-supervised counterparts following the same lines expounded in this work. One of the main difficulties is that, even if the loss considered is differentiable, one cannot straightaway apply techniques like gradient descent to the parameters as this typically leads to solutions that are outside of the set $\Cb$. Besides these issues, there is the other open question of what loss functions could benefit from constraining the solution to a subset like $\Cb$ in the first place.

Finally, a rather interesting application of the idea presented here is to semi-supervised regression. \cite{Culp2008} argues that in least squares regression, unlabeled data may not help. However, one of the things we exploit in the classification setting is that the labels have bounded outputs. Using bounds on the outputs may therefore also lead to improvement in the semi-supervised regression setting. In some regression tasks, bounded outputs could be a natural assumption.

% While the results presented in this work promising for squared loss, in its current form, it can be shown that the approach does not work for some other losses.  For instance, for monotonically decreasing loss function, such as the hinge or logistic loss used, respectively, in support vector classification and logistic regression, the approach does not seem to offer any improvement. For these types of losses, we can always find a labeling such that the loss is minimized at the supervised solution. In other words, the supervised solution is always within $\Cb$. For these types of classifiers, we need to additionally shrink the constrained space to allow for improvements. Fortunately, a lot of interesting classifiers still allow for the implicitly constrained semi-supervised learning approach to be applied. For instance, many classifiers based on a maximum likelihood formulation. In this way, the procedure offers an interesting alternative to Expectation Maximization based approaches like self-learning, given its convex loss function and robustness to misspecification.

% In many cases the improvements were substantial. It is possible that alternative semi-supervised techniques may perform even better on some of these datasets. As we noted in Section 2, however, these are not based on least squares loss and hence are not a semi-supervised version of least squares classification. Moreover, they do not offer the same improvement in expectation that we have attempted to ensure here. Regardless of the size of the improvement, perhaps the most surprising conclusion is that this novel type of semi-supervised method works at all. This is surprising, first of all because we 

% Alternatively, one could argue it is unsurprising ICLS works, since the model does not fit the data and we do not use any form of regularization on the parameter values. Most models used in practice do not directly minimize the ultimate goal of minimizing misclassification. For computational reasons, often convex surrogate losses, such as the one employed here are minimized. We have chosen to restrict ourselves to a particular convex loss and attempted whether we could ensure improvement in terms of this chosen loss function. As for regularization, the proposed method can easily be extended to include a regularization term. This would, however, introduce an additional parameter to be set by the user. On of the advantages of ICLS is that it is a type of data dependent regularization, and no additional parameter value needs to be selected. In our experience, ICLS combined with regularization still outperforms regularized supervised learning, although the differences become smaller.







% There are several simple ways in which the speed of ICLS could be improved. Firstly, whereas we initialize the gradient descent by soft labeling all objects as $0.5$, better initializations are likely possible. For instance, we may use the self-learning solution or a labeling given by the supervised solution. Secondly, it may not be necessary to solve the minimization exactly. While this gives precise values for the imputed labels, early termination of the gradient descent may not have much effect on the estimation of $\mathbf{\beta}$.



% Another interesting observation is the occurrence of the peaking phenomenon \cite{Raudys1998,Opper1996} in this semi-supervised setting, see for example, the learning curves on the COIL2 and BCI datasets. While this has been observed for learning curves of the number of labeled samples, here we find it also occurs when we change the number of unlabeled samples. It also seems that for some datasets ICLS is more sensitive to this problem than self-learning. As yet, we do not have any explanation for this behavior.  Further improvements to the current approach may start by trying to understand this occurrence of peaking.




% These results are all the more interesting once we compare the total squared loss on the test set. Here, ICLS and self-learning seem to offer similar performance. This is quite unexpected and unlike the results in, for instance \cite{Loog2010,Loog2012a}, where the self-learner  often performed much worse in terms of the loss then an approach based on constraining the solution. The difference may be explained by the fact that the discriminative classifier used here is less susceptible to this problem than the generative approaches in \cite{Loog2010,Loog2012a}.

% The learning curves of the loss in Figure \ref{fig:learningcurvesloss} also show that the the multivariate ICLS behaves much like the univariate result derived in Theorem 1. Interestingly, for very small sample sizes $2$ and $4$, performance may still degrade. A better understanding of why this occurs would both lead to an improved method and can perhaps inspire more theoretical results for the multivariate model.



%The robustness of the method comes from the fact that we do not accept solutions that do not work on the labeled data. The goal of semi-supervised learning is to improve supervised techniques using the additional information inherent in the additional unlabeled objects. Previous approaches have done this by changing the loss function that is being optimized by introducing an extra term corresponding to assumptions about the unlabeled data. The loss function then becomes a mixture between the supervised objective and an unsupervised objective. If the goal is supervised learning, we propose that the loss function should remain the supervised loss function. The unlabeled objects are used to introduce constraints on the possible solutions to this loss function, but does not change its shape.  

% Compared to other approaches in Table \ref{table:cvtable}, the supervised support vector machine works surprisingly well.  Semi-supervised variants of the support vector machine, however, at times degrade the performance, while our approach \emph{never} gives worse performance than the supervised least squares classifier. Clearly, an approach that combines the robustness of our method with the effectiveness of the support vector machine on these datasets  would be beneficial. In principle, our approach can be extended to create an implicitly constrained support vector machine. Unlike the least squares classifier, however, the support vector machine does not have a direct closed form solution. The key challenge will therefore be how to efficiently solve or approximate the resulting procedure. 

%In those cases, our approach is clearly helpful. In our experience, the performance increase of unlabeled data is reduced as we set the regularization parameter. In the small sample case, we may not be able to accurately set this parameter, so the unlabeled data may still be preferable.

%Extension to other methods. The challenge here is to solve the resulting optimization problem.  Given the desirable properties of our approach, an interesting extension would be an implicitly constrained variant of the support vector machine, or logistic regression. Because of the closed work solution of the supervised linear least squares model \eqref{}, in our formulation we could plug in this solution in the semi-supervised problem. In general, this is not possible. Another issue is whether there are any implicit constraints that can be leveraged by the unlabeled objects. For instance, when we allow fractional labelings for the unlabeled data in logistic regression, we can always find a labeling that corresponds to the supervised solution. Things chance if we do not allow for fractional labelings.

\section{Conclusion}
This contribution introduced a new semi-supervised approach to least squares classification. By considering all possible labelings of the unlabeled objects and choosing the one that best matches the labeled observations, we derived a robust classifier through a simple quadratic programming formulation. For this procedure, in the univariate setting with a linear model without intercept, we can prove it never degrades performance in terms of squared loss (Theorem 1). An additional theoretical result shows that in the multivariate case, an adapted procedure never degrades in terms of the Euclidean distance of the parameter estimates (Theorem 2). Experimental results indicate that in expectation this robustness also holds in terms of classification error on real datasets. Hence, semi-supervised learning for least squares classification without additional assumptions can lead to improvements over supervised least squares classification both in theory and in practice.

\begin{acknowledgements}
Part of this work was funded by project P24 of the Dutch public-private research community COMMIT.
\end{acknowledgements}

\bibliographystyle{splncs}
\begin{thebibliography}{10}

\bibitem{Widrow1960}
Widrow, B., Hoff, M.E.:
\newblock {Adaptive switching circuits.}
\newblock In: IRE WESCON Convention Record 4. (1960)  96--104

\bibitem{Seeger2001}
Seeger, M.:
\newblock {Learning with labeled and unlabeled data}.
\newblock Technical report (2001)

\bibitem{Sokolovska2008}
Sokolovska, N., Capp\'{e}, O., Yvon, F.:
\newblock {The asymptotics of semi-supervised learning in discriminative
  probabilistic models}.
\newblock In Cohen, W.W., McCallum, A., Roweis, S.T., eds.: Proceedings of the
  25th International Conference on Machine Learning, Helsinki, Finland, ACM
  Press (2008)  984--991

\bibitem{Hastie2001}
Hastie, T., Tibshirani, R., Friedman, J.H.:
\newblock {The elements of statistical learning}.
\newblock Spinger (2001)

\bibitem{Rifkin2003}
Rifkin, R., Yeo, G., Poggio, T.:
\newblock {Regularized least-squares classification}.
\newblock Nato Science Series Sub Series III Computer and Systems Sciences 190
  (2003)

\bibitem{Tibshirani1996}
Tibshirani, R.:
\newblock {Regression shrinkage and selection via the lasso}.
\newblock Journal of the Royal Statistical Society. Series B \textbf{58}(1)
  (1996)  267--288

\bibitem{Poggio2003}
Poggio, T., Smale, S.:
\newblock {The Mathematics of Learning: Dealing with Data}.
\newblock Notices of the AMS (2003)  537--544

\bibitem{Bottou2010}
Bottou, L.:
\newblock {Large-scale machine learning with stochastic gradient descent}.
\newblock In: Proceedings of COMPSTAT'2010, Springer (2010)  177--186

\bibitem{Cozman2006}
Cozman, F., Cohen, I.:
\newblock {Risks of Semi-Supervised Learning}.
\newblock In Chapelle, O., Sch\"{o}lkopf, B., Zien, A., eds.: Semi-Supervised
  Learning.
\newblock MIT press (2006)  56--72

\bibitem{Chapelle2006}
Chapelle, O., Sch\"{o}lkopf, B., Zien, A.:
\newblock {Semi-supervised learning}.
\newblock MIT press (2006)

\bibitem{Zhu2009}
Zhu, X., Goldberg, A.B.:
\newblock {Introduction to Semi-Supervised Learning}. Volume~3.
\newblock Morgan \& Claypool (January 2009)

\bibitem{Nigam2000}
Nigam, K., McCallum, A.K., Thrun, S., Mitchell, T.:
\newblock {Text classification from labeled and unlabeled documents using EM}.
\newblock Machine learning \textbf{34} (2000)  1--34

\bibitem{Cozman2003}
Cozman, F.G., Cohen, I., Cirelo, M.C.:
\newblock {Semi-Supervised Learning of Mixture Models}.
\newblock Proceedings of the Twentieth International Conference on Machine
  Learning (2003)

\bibitem{Goldberg2009}
Goldberg, A.B., Zhu, X.:
\newblock {Keepin'it real: semi-supervised learning with realistic tuning}.
\newblock NAACL HLT 2009 Workshop on Semi-supervised Learning for Natural
  Language Processing (2009)

\bibitem{Wang2007a}
Wang, J., Shen, X., Pan, W.:
\newblock {On transductive support vector machines}.
\newblock Contemporary Mathematics \textbf{443} (2007)  7--19

\bibitem{McLachlan1975b}
McLachlan, G.J.:
\newblock {Iterative Reclassification Procedure for Constructing an
  Asymptotically Optimal Rule of Allocation in Discriminant Analysis}.
\newblock \textbf{70}(350) (1975)  365--369

\bibitem{Yarowsky1995}
Yarowsky, D.:
\newblock {Unsupervised word sense disambiguation rivaling supervised methods}.
\newblock Proceedings of the 33rd annual meeting on Association for
  Computational Linguistics - (1995)  189--196

\bibitem{Abney2004}
Abney, S.:
\newblock {Understanding the yarowsky algorithm}.
\newblock Computational Linguistics \textbf{30}(3) (2004)  365--395

\bibitem{Grandvalet2005}
Grandvalet, Y., Bengio, Y.:
\newblock {Semi-supervised learning by entropy minimization}.
\newblock In Saul, L.K., Weiss, Y., Bottou, L., eds.: Advances in Neural
  Information Processing Systems 17, Cambridge, MA, MIT Press (2005)

\bibitem{Joachims1999}
Joachims, T.:
\newblock {Transductive inference for text classification using support vector
  machines}.
\newblock In: Proceedings of the 16th International Conference on Machine
  Learning, Morgan Kaufmann Publishers (1999)  200--209

\bibitem{Bennett1998}
Bennett, K.P., Demiriz, A.:
\newblock {Semi-supervised support vector machines}.
\newblock In: Advances in Neural Information Processing Systems 11. (1998)

\bibitem{Sindhwani2006}
Sindhwani, V., Keerthi, S.S.:
\newblock {Large scale semi-supervised linear SVMs}.
\newblock In: Proceedings of the 29th annual international ACM SIGIR conference
  on Research and development in information retrieval, New York, New York,
  USA, ACM Press (2006)  477

\bibitem{Collobert2006}
Collobert, R., Sinz, F., Weston, J., Bottou, L.:
\newblock {Large scale transductive SVMs}.
\newblock Journal of Machine Learning Research \textbf{7} (2006)  1687--1712

\bibitem{Wang2007}
Wang, J., Shen, X.:
\newblock {Large margin semi-supervised learning}.
\newblock Journal of Machine Learning Research \textbf{8} (2007)  1867--1891

\bibitem{Li2011}
Li, Y.F., Zhou, Z.h.:
\newblock {Towards making unlabeled data never hurt}.
\newblock In: Proceedings of the 28th International Conference on Machine
  Learning. (2011)

\bibitem{Loog2010}
Loog, M.:
\newblock {Constrained Parameter Estimation for Semi-Supervised Learning: The
  Case of the Nearest Mean Classifier}.
\newblock In: Proceedings of the 2010 European Conference on Machine learning
  and Knowledge Discovery in Databases. (2010)  291--304

\bibitem{Loog2013a}
Loog, M.:
\newblock {Semi-supervised linear discriminant analysis through
  moment-constraint parameter estimation}.
\newblock Pattern Recognition Letters \textbf{In press} (March 2013)

\bibitem{Shaffer1991}
Shaffer, J.P.:
\newblock {The Gauss-Markov Theorem and Random Regressors}.
\newblock The American Statistician \textbf{45}(4) (1991)  269--273

\bibitem{Fan2008}
Fan, B., Lei, Z., Li, S.Z.:
\newblock {Normalized LDA for semi-supervised learning}.
\newblock In: 8th IEEE International Conference on Automatic Face \& Gesture
  Recognition, Ieee (September 2008)  1--6

\bibitem{Byrd1995}
Byrd, R.H., Lu, P., Nocedal, J., Zhu, C.:
\newblock {A limited memory algorithm for bound constrained optimization}.
\newblock SIAM Journal on Scientific Computing (1995)

\bibitem{Berger1985}
Berger, J.O.:
\newblock {Statistical decision theory and Bayesian analysis}.
\newblock Springer (1985)

\bibitem{Aubin2000}
Aubin, J.P.:
\newblock {Applied functional analysis}. Volume~47.
\newblock John Wiley \& Sons (2000)

\bibitem{Bache2013}
Bache, K., Lichman, M.:
\newblock {\{UCI\} Machine Learning Repository} (2013)

\bibitem{Raudys1998}
Raudys, S., Duin, R.P.:
\newblock {Expected classification error of the Fisher linear classifier with
  pseudo-inverse covariance matrix}.
\newblock Pattern Recognition Letters \textbf{19}(5-6) (April 1998)  385--392

\bibitem{Opper1996}
Opper, M., Kinzel, W.:
\newblock {Statistical Mechanics of Generalization}.
\newblock In Domany, E., Hemmen, J.L., Schulten, K., eds.: Models of Neural
  Networks III.
\newblock Springer, New York (1996)  151--209

\bibitem{Culp2008}
Culp, M., Michailidis, G.:
\newblock {An iterative algorithm for extending learners to a semi-supervised
  setting}.
\newblock Journal of Computational and Graphical Statistics \textbf{17}(3)
  (2008)  545--571

\end{thebibliography}


\end{document}
